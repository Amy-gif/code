{
 "cells": [
  {
   "cell_type": "code",
   "id": "cb89b207",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:33:52.914709Z",
     "iopub.status.busy": "2023-09-11T21:33:52.913934Z",
     "iopub.status.idle": "2023-09-11T21:34:08.433091Z",
     "shell.execute_reply": "2023-09-11T21:34:08.431692Z"
    },
    "papermill": {
     "duration": 15.557542,
     "end_time": "2023-09-11T21:34:08.436046",
     "exception": false,
     "start_time": "2023-09-11T21:33:52.878504",
     "status": "completed"
    },
    "tags": []
   },
   "source": "!pip install geopandas matplotlib folium",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e56f3e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:08.569025Z",
     "iopub.status.busy": "2023-09-11T21:34:08.568634Z",
     "iopub.status.idle": "2023-09-11T21:34:11.489573Z",
     "shell.execute_reply": "2023-09-11T21:34:11.488520Z"
    },
    "papermill": {
     "duration": 2.9594,
     "end_time": "2023-09-11T21:34:11.492222",
     "exception": false,
     "start_time": "2023-09-11T21:34:08.532822",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML as html_print\n",
    "from termcolor import colored\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# # 处理完成后释放内存\n",
    "# del df1  # 删除 DataFrame\n",
    "# del df2  # 删除 DataFrame\n",
    "# del df3 # 删除 DataFrame\n",
    "# del df4  # 删除 DataFrame\n",
    "# gc.collect()  # 强制垃圾回收"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "728c33a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:11.631653Z",
     "iopub.status.busy": "2023-09-11T21:34:11.630708Z",
     "iopub.status.idle": "2023-09-11T21:34:11.637335Z",
     "shell.execute_reply": "2023-09-11T21:34:11.636326Z"
    },
    "papermill": {
     "duration": 0.045731,
     "end_time": "2023-09-11T21:34:11.639962",
     "exception": false,
     "start_time": "2023-09-11T21:34:11.594231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "# 定义一个函数来读取并预处理数据\n",
    "# 读取CSV文件修改重复字段\n",
    "#df1 = pd.read_csv(\"D:/code/junma/600000/0424/zndspeed.csv\")\n",
    "df1 = pd.read_csv(\"D:/CSV/zbdata/zbhisshift.csv\")\n",
    "# 批量修改列名：在所有以\"dw\"开头的列名前加\"S_\"\n",
    "df1.columns = ['V_' + col if col.startswith('dw') else col for col in df1.columns]\n",
    "\n",
    "# 强制所有表格截断到10000行\n",
    "common_rows = 240000\n",
    "df1 = df1.head(common_rows).reset_index(drop=True)\n",
    "\n",
    "# 保存修改后的文件\n",
    "df1.to_csv('D:/CSV/zbdata/hisshift.csv', index=False)\n",
    "\n"
   ],
   "id": "cd011fe7df85ab7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#df2 = pd.read_csv(\"D:/code/junma/600000/0424/zndfpercent.csv\")\n",
    "df2 = pd.read_csv(\"D:/CSV/zbdata/zbparam.csv\")\n",
    "# 批量修改列名：在所有以\"dw\"开头的列名前加\"P_\"\n",
    "df2.columns = ['P_' + col if col.startswith('dw') else col for col in df2.columns]\n",
    "\n",
    "# 强制所有表格截断到10000行\n",
    "common_rows = 240000\n",
    "df2 = df2.head(common_rows).reset_index(drop=True)\n",
    "\n",
    "# 保存修改后的文件\n",
    "df2.to_csv('D:/CSV/zbdata/param.csv', index=False)\n"
   ],
   "id": "ef2945007c7962b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df3 = pd.read_csv(\"D:/CSV/zbdata/zbshift.csv\")\n",
    "# 批量修改列名：在所有以\"dw\"开头的列名前加\"D_\"\n",
    "df3.columns = ['D_' + col if col.startswith('dw') else col for col in df3.columns]\n",
    "\n",
    "# 强制所有表格截断到10000行\n",
    "common_rows = 240000\n",
    "df3 = df3.head(common_rows).reset_index(drop=True)\n",
    "\n",
    "# 保存修改后的文件\n",
    "df3.to_csv('D:/CSV/zbdata/shift.csv.csv', index=False)"
   ],
   "id": "dbeca7ea38b10db5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df5 = pd.read_csv(\"D:/code/junma/600000/0424/znmstatus.csv\")\n",
    "# 批量修改列名：在所有以\"dw\"开头的列名前加\"D_\"\n",
    "df5.columns = ['M_' + col if col.startswith('dw') else col for col in df5.columns]\n",
    "\n",
    "# 强制所有表格截断到10000行\n",
    "common_rows = 240000\n",
    "df5 = df5.head(common_rows).reset_index(drop=True)\n",
    "\n",
    "# 保存修改后的文件\n",
    "df5.to_csv('D:/code/junma/600000/0424/mstatus.csv', index=False)"
   ],
   "id": "a9509654ad1d525f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df6 = pd.read_csv(\"D:/code/junma/600000/0424/znproduct.csv\")\n",
    "# 批量修改列名：在所有以\"dw\"开头的列名前加\"D_\"\n",
    "df6.columns = ['C_' + col if col.startswith('dw') else col for col in df6.columns]\n",
    "\n",
    "# 强制所有表格截断到10000行\n",
    "common_rows = 240000\n",
    "df6 = df6.head(common_rows).reset_index(drop=True)\n",
    "\n",
    "# 保存修改后的文件\n",
    "df6.to_csv('D:/code/junma/600000/0424/product.csv', index=False)\n"
   ],
   "id": "948ea2e8ba550621",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df7 = pd.read_csv(\"D:/code/junma/600000/0424/znenergy.csv\")\n",
    "# 批量修改列名：在所有以\"dw\"开头的列名前加\"D_\"\n",
    "df7.columns = ['E_' + col if col.startswith('dw') else col for col in df7.columns]\n",
    "\n",
    "# 强制所有表格截断到10000行\n",
    "common_rows = 240000\n",
    "df7 = df7.head(common_rows).reset_index(drop=True)\n",
    "\n",
    "# 保存修改后的文件\n",
    "df7.to_csv('D:/code/junma/600000/0424/energy.csv', index=False)"
   ],
   "id": "7dd87860e9662ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 计算基础能耗指标\n",
    "df7.to_csv('D:/code/junma/600000/0424/energy.csv', index=False)\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "# 去除列名前后空格\n",
    "df7.columns = df7.columns.str.strip()\n",
    "# 查看数据基本信息\n",
    "print(df7.info())\n",
    "# 查看列名\n",
    "print(df7.columns)\n",
    "energy_metrics = df7[['E_dw13', 'E_dw14']].describe()\n",
    "\n",
    "print(\"\\n基础能耗指标分析:\")\n",
    "print(energy_metrics)\n",
    "\n",
    "# 可视化功率变化\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df7.index, df7['E_dw13'], label='总功率')\n",
    "plt.title('总功率随时间变化趋势')\n",
    "plt.xlabel('时间')\n",
    "plt.ylabel('功率(kW)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "602e78e9a5543628",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df4 = pd.read_csv(\"D:/code/junma/600000/0424/znrstatus.csv\")\n",
    "# 批量修改列名：在所有以\"dw\"开头的列名前加\"D_\"\n",
    "df4.columns = ['R_' + col if col.startswith('dw') else col for col in df4.columns]\n",
    "\n",
    "# 强制所有表格截断到10000行\n",
    "common_rows = 240000\n",
    "df4 = df4.head(common_rows).reset_index(drop=True)\n",
    "\n",
    "# 保存修改后的文件\n",
    "df4.to_csv('D:/code/junma/600000/0424/rstatus.csv', index=False)"
   ],
   "id": "59934b20c71402e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "03d45761",
   "metadata": {
    "papermill": {
     "duration": 0.033824,
     "end_time": "2023-09-11T21:34:11.710147",
     "exception": false,
     "start_time": "2023-09-11T21:34:11.676323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>3 |</span></b> <b>Loading The Data Set</b></div>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#7按照name对齐\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def read_and_preprocess(file_path):\n",
    "    try:\n",
    "        # 尝试多种编码方式读取\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='GBK')\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"处理文件 {file_path} 时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 保留关键字段\n",
    "    key_columns = ['name', 'created_at']\n",
    "    for col in key_columns:\n",
    "        if col in df.columns:\n",
    "            if col == 'created_at':\n",
    "                # 修改时间解析方式，自动处理不同格式\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce', format='mixed', dayfirst=False)\n",
    "                # 移除无效的时间数据\n",
    "                df = df.dropna(subset=[col])\n",
    "            df[col] = df[col].astype(str)  # 统一转为字符串类型便于合并\n",
    "\n",
    "    # 去除不需要的列（保留关键字段）\n",
    "    columns_to_drop = ['id', 'ip', 'status', 'subsytem']\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "# 文件路径列表\n",
    "file_paths = [\n",
    "    'D:/code/junma/600000/0424/energy.csv',\n",
    "    'D:/code/junma/600000/0424/product.csv',\n",
    "    'D:/code/junma/600000/0424/speed.csv',\n",
    "    'D:/code/junma/600000/0424/percent.csv',\n",
    "    'D:/code/junma/600000/0424/mstatus.csv',\n",
    "    'D:/code/junma/600000/0424/dstatus.csv',\n",
    "    'D:/code/junma/600000/0424/rstatus.csv'\n",
    "]\n",
    "\n",
    "# 读取并预处理所有文件\n",
    "dfs = []\n",
    "for file_path in file_paths:\n",
    "    if os.path.exists(file_path):\n",
    "        df = read_and_preprocess(file_path)\n",
    "        if df is not None:\n",
    "            dfs.append(df)\n",
    "            print(f\"已加载文件: {file_path}, 行数: {len(df)}, 列数: {len(df.columns)}\")\n",
    "    else:\n",
    "        print(f\"文件不存在：{file_path}\")\n",
    "\n",
    "if not dfs:\n",
    "    print(\"没有可合并的数据\")\n",
    "else:\n",
    "    # 1. 检查所有文件是否都包含关键字段\n",
    "    required_columns = ['name', 'created_at']\n",
    "    for i, df in enumerate(dfs):\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"文件 {file_paths[i]} 缺少关键字段: {missing_cols}\")\n",
    "\n",
    "    # 2. 找出所有文件中共同的name（即energy.csv中的NX48至NX78）\n",
    "    common_names = set(dfs[0]['name'].unique())  # 以energy.csv为基础\n",
    "    for df in dfs[1:]:\n",
    "        common_names.intersection_update(df['name'].unique())\n",
    "\n",
    "    print(f\"共有的name数量: {len(common_names)}\")\n",
    "\n",
    "    # 3. 找出所有表中共同的时间范围\n",
    "    # 获取每个表的时间范围\n",
    "    time_ranges = []\n",
    "    for df in dfs:\n",
    "        if 'created_at' in df.columns:\n",
    "            # 转换为datetime类型进行比较\n",
    "            df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "            min_time = df['created_at'].min()\n",
    "            max_time = df['created_at'].max()\n",
    "            time_ranges.append((min_time, max_time))\n",
    "            print(f\"表时间范围: {min_time} 至 {max_time}\")\n",
    "\n",
    "    # 计算所有表的共同时间范围\n",
    "    common_start = max([tr[0] for tr in time_ranges])\n",
    "    common_end = min([tr[1] for tr in time_ranges])\n",
    "\n",
    "    print(f\"\\n所有表共同的时间范围: {common_start} 至 {common_end}\")\n",
    "\n",
    "    # 4. 合并所有数据框，只保留共有的name和时间范围内的数据\n",
    "    merged_df = dfs[0][(dfs[0]['name'].isin(common_names)) &\n",
    "                       (pd.to_datetime(dfs[0]['created_at']) >= common_start) &\n",
    "                       (pd.to_datetime(dfs[0]['created_at']) <= common_end)].copy()\n",
    "\n",
    "    for i in range(1, len(dfs)):\n",
    "        current_df = dfs[i][(dfs[i]['name'].isin(common_names)) &\n",
    "                           (pd.to_datetime(dfs[i]['created_at']) >= common_start) &\n",
    "                           (pd.to_datetime(dfs[i]['created_at']) <= common_end)].copy()\n",
    "\n",
    "        # 按时间和设备信息合并\n",
    "        how_to_merge = 'outer'  # 可根据需要改为 'inner' 或 'left'\n",
    "        merged_df = pd.merge(\n",
    "            merged_df,\n",
    "            current_df,\n",
    "            on=['name', 'created_at'],\n",
    "            how=how_to_merge,\n",
    "            suffixes=('', f'_{i}')  # 为重复列添加后缀\n",
    "        )\n",
    "\n",
    "        print(f\"合并后形状: {merged_df.shape}\")\n",
    "\n",
    "    # 5. 处理合并后的数据\n",
    "    # 按时间和设备排序\n",
    "    merged_df['created_at'] = pd.to_datetime(merged_df['created_at'])\n",
    "    merged_df = merged_df.sort_values(['name', 'created_at'])\n",
    "\n",
    "    # 保存结果\n",
    "    output_path = 'D:/code/junma/600000/0424/merged_data2.csv'\n",
    "    merged_df.to_csv(output_path, index=False, encoding='GBK')\n",
    "    print(f\"\\n最终合并完成，形状: {merged_df.shape}\")\n",
    "    print(f\"结果已保存到: {output_path}\")"
   ],
   "id": "1f115f70003fd821",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#7\n",
    "import pandas as pd\n",
    "\n",
    "def expand_and_rename_energy(input_path, output_path):\n",
    "    # 读取原始文件（自动尝试GBK和utf-8编码）\n",
    "    try:\n",
    "        energy_df = pd.read_csv(input_path, encoding='GBK')\n",
    "    except UnicodeDecodeError:\n",
    "        energy_df = pd.read_csv(input_path, encoding='utf-8')\n",
    "\n",
    "    # 确保有必要的列\n",
    "    if 'name' not in energy_df.columns:\n",
    "        raise ValueError(\"输入文件必须包含'name'列\")\n",
    "\n",
    "    # 创建空DataFrame存储结果\n",
    "    expanded_df = pd.DataFrame()\n",
    "\n",
    "    # 对每个设备进行处理\n",
    "    for _, row in energy_df.iterrows():\n",
    "        original_name = row['name']\n",
    "\n",
    "        # 创建左面数据\n",
    "        left_row = row.copy()\n",
    "        left_name = f\"{original_name}-L102\"\n",
    "        left_row['name'] = left_name\n",
    "        left_row['subsystem'] = left_name  # 同时更新subsystem\n",
    "\n",
    "        # 创建右面数据\n",
    "        right_row = row.copy()\n",
    "        right_name = f\"{original_name}-R103\"\n",
    "        right_row['name'] = right_name\n",
    "        right_row['subsystem'] = right_name  # 同时更新subsystem\n",
    "\n",
    "        # 添加到结果DataFrame\n",
    "        expanded_df = pd.concat([expanded_df, pd.DataFrame([left_row, right_row])])\n",
    "\n",
    "    # 重置索引\n",
    "    expanded_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 保存结果\n",
    "    expanded_df.to_csv(output_path, index=False, encoding='GBK')\n",
    "    print(f\"扩展后的数据已保存到 {output_path}，共 {len(expanded_df)} 行\")\n",
    "\n",
    "# 使用示例\n",
    "input_file = 'D:/code/junma/600000/0424/energy.csv'\n",
    "output_file = 'D:/code/junma/600000/0424/energy2.csv'\n",
    "\n",
    "expand_and_rename_energy(input_file, output_file)"
   ],
   "id": "a4c6c869e7f8247",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#7\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def merge_by_subsystem_and_time():\n",
    "    # 文件路径\n",
    "    energy_path = 'D:/code/junma/600000/0424/energy2.csv'\n",
    "    merged_path = 'D:/code/junma/600000/0424/merged_data.csv'\n",
    "    output_path = 'D:/code/junma/600000/0424/merged_data2.csv'\n",
    "\n",
    "    # 读取文件（自动处理编码）\n",
    "    def read_csv_with_encoding(file_path):\n",
    "        try:\n",
    "            return pd.read_csv(file_path, encoding='GBK')\n",
    "        except UnicodeDecodeError:\n",
    "            return pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "    # 检查文件是否存在\n",
    "    if not os.path.exists(energy_path):\n",
    "        print(f\"文件不存在: {energy_path}\")\n",
    "        return\n",
    "    if not os.path.exists(merged_path):\n",
    "        print(f\"文件不存在: {merged_path}\")\n",
    "        return\n",
    "\n",
    "    # 读取数据\n",
    "    energy_df = read_csv_with_encoding(energy_path)\n",
    "    merged_df = read_csv_with_encoding(merged_path)\n",
    "\n",
    "    # 确保有必要的列\n",
    "    required_cols = ['subsystem', 'created_at']\n",
    "    for df, name in [(energy_df, 'energy2'), (merged_df, 'merged_data')]:\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        if missing:\n",
    "            print(f\"{name} 缺少必要列: {missing}\")\n",
    "            return\n",
    "\n",
    "    # 转换时间格式\n",
    "    energy_df['created_at'] = pd.to_datetime(energy_df['created_at'])\n",
    "    merged_df['created_at'] = pd.to_datetime(merged_df['created_at'])\n",
    "\n",
    "    # 按subsystem和created_at合并\n",
    "    final_df = pd.merge(\n",
    "        energy_df,\n",
    "        merged_df,\n",
    "        on=['subsystem', 'created_at'],\n",
    "        how='inner',  # 只保留两者都有的数据\n",
    "        suffixes=('_energy', '_merged')\n",
    "    )\n",
    "\n",
    "    # 定义需要删除的多余列\n",
    "    columns_to_drop = [\n",
    "        'id', 'ip', 'status',\n",
    "        'name_energy', 'name_merged',\n",
    "        'subsystem_energy', 'subsystem_merged'  # 防止重复的subsystem列\n",
    "    ]\n",
    "\n",
    "    # 只删除实际存在的列\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in final_df.columns]\n",
    "\n",
    "    # 删除多余列\n",
    "    if columns_to_drop:\n",
    "        final_df = final_df.drop(columns=columns_to_drop)\n",
    "        print(f\"已删除多余列: {columns_to_drop}\")\n",
    "\n",
    "    # 保存结果\n",
    "    final_df.to_csv(output_path, index=False, encoding='GBK')\n",
    "    print(f\"合并完成，结果已保存到: {output_path}\")\n",
    "    print(f\"合并后数据形状: {final_df.shape}\")\n",
    "    print(f\"保留的列: {list(final_df.columns)}\")\n",
    "\n",
    "# 执行合并\n",
    "merge_by_subsystem_and_time()"
   ],
   "id": "ad999ca70b0b4b83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#7\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def merge_by_subsystem_and_time():\n",
    "    # 文件路径\n",
    "    energy_path = 'D:/code/junma/600000/0424/energy2.csv'\n",
    "    merged_path = 'D:/code/junma/600000/0424/merged_data.csv'\n",
    "    output_path = 'D:/code/junma/600000/0424/merged_data2.csv'\n",
    "\n",
    "    # 读取文件（自动处理编码）\n",
    "    def read_csv_with_encoding(file_path):\n",
    "        try:\n",
    "            return pd.read_csv(file_path, encoding='GBK')\n",
    "        except UnicodeDecodeError:\n",
    "            return pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "    # 检查文件是否存在\n",
    "    if not os.path.exists(energy_path):\n",
    "        print(f\"文件不存在: {energy_path}\")\n",
    "        return\n",
    "    if not os.path.exists(merged_path):\n",
    "        print(f\"文件不存在: {merged_path}\")\n",
    "        return\n",
    "\n",
    "    # 读取数据\n",
    "    energy_df = read_csv_with_encoding(energy_path)\n",
    "    merged_df = read_csv_with_encoding(merged_path)\n",
    "\n",
    "    # 确保有必要的列\n",
    "    required_cols = ['subsystem', 'created_at']\n",
    "    for df, name in [(energy_df, 'energy2'), (merged_df, 'merged_data')]:\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        if missing:\n",
    "            print(f\"{name} 缺少必要列: {missing}\")\n",
    "            return\n",
    "\n",
    "    # 转换时间格式\n",
    "    energy_df['created_at'] = pd.to_datetime(energy_df['created_at'])\n",
    "    merged_df['created_at'] = pd.to_datetime(merged_df['created_at'])\n",
    "\n",
    "    # 获取两个表的时间范围\n",
    "    energy_start = energy_df['created_at'].min()\n",
    "    energy_end = energy_df['created_at'].max()\n",
    "    merged_start = merged_df['created_at'].min()\n",
    "    merged_end = merged_df['created_at'].max()\n",
    "\n",
    "    # 确定共同的时间范围\n",
    "    common_start = max(energy_start, merged_start)\n",
    "    common_end = min(energy_end, merged_end)\n",
    "\n",
    "    print(f\"energy2时间范围: {energy_start} 至 {energy_end}\")\n",
    "    print(f\"merged_data时间范围: {merged_start} 至 {merged_end}\")\n",
    "    print(f\"共同时间范围: {common_start} 至 {common_end}\")\n",
    "\n",
    "    # 获取两个表的共同设备（subsystem）\n",
    "    energy_subsystems = set(energy_df['subsystem'].unique())\n",
    "    merged_subsystems = set(merged_df['subsystem'].unique())\n",
    "    common_subsystems = energy_subsystems & merged_subsystems\n",
    "\n",
    "    print(f\"\\nenergy2设备数量: {len(energy_subsystems)}\")\n",
    "    print(f\"merged_data设备数量: {len(merged_subsystems)}\")\n",
    "    print(f\"共同设备数量: {len(common_subsystems)}\")\n",
    "\n",
    "    # 筛选两个表在共同时间范围内且属于共同设备的数据\n",
    "    energy_df = energy_df[(energy_df['created_at'] >= common_start) &\n",
    "                          (energy_df['created_at'] <= common_end) &\n",
    "                          (energy_df['subsystem'].isin(common_subsystems))]\n",
    "\n",
    "    merged_df = merged_df[(merged_df['created_at'] >= common_start) &\n",
    "                          (merged_df['created_at'] <= common_end) &\n",
    "                          (merged_df['subsystem'].isin(common_subsystems))]\n",
    "\n",
    "    # 按subsystem和created_at合并\n",
    "    final_df = pd.merge(\n",
    "        energy_df,\n",
    "        merged_df,\n",
    "        on=['subsystem', 'created_at'],\n",
    "        how='outer',  # 保留所有时间点，不匹配的为空\n",
    "        suffixes=('_energy', '_merged')\n",
    "    )\n",
    "\n",
    "    # 定义需要删除的多余列\n",
    "    columns_to_drop = [\n",
    "        'id', 'ip', 'status',\n",
    "        'name_energy', 'name_merged',\n",
    "        'subsystem_energy', 'subsystem_merged'\n",
    "    ]\n",
    "\n",
    "    # 只删除实际存在的列\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in final_df.columns]\n",
    "\n",
    "    # 删除多余列\n",
    "    if columns_to_drop:\n",
    "        final_df = final_df.drop(columns=columns_to_drop)\n",
    "        print(f\"\\n已删除多余列: {columns_to_drop}\")\n",
    "\n",
    "    # 保存结果\n",
    "    final_df.to_csv(output_path, index=False, encoding='GBK')\n",
    "    print(f\"\\n合并完成，结果已保存到: {output_path}\")\n",
    "    print(f\"合并后数据形状: {final_df.shape}\")\n",
    "    print(f\"保留的列: {list(final_df.columns)}\")\n",
    "    print(f\"保留的设备数量: {len(final_df['subsystem'].unique())}\")\n",
    "\n",
    "# 执行合并\n",
    "merge_by_subsystem_and_time()"
   ],
   "id": "1c22b2509b50650e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#6 按照subsystem对齐\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 修改后的预处理函数，增加时间格式处理\n",
    "def read_and_preprocess(file_path, is_first_file=False):\n",
    "    try:\n",
    "        # 读取 CSV 文件，指定编码为 'GBK'\n",
    "        df = pd.read_csv(file_path, encoding='GBK')\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            # 如果 GBK 失败，尝试 utf-8\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        except Exception as e:\n",
    "            print(f\"处理文件 {file_path} 时出错: {e}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"处理文件 {file_path} 时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 定义要去除的列 - 不再去除 subsystem\n",
    "    columns_to_drop = ['id', 'ip', 'status']  # 移除了 subsystem\n",
    "\n",
    "    # 如果是第一个文件，保留 name, subsystem 和 created_at\n",
    "    if is_first_file:\n",
    "        columns_to_drop = [col for col in columns_to_drop if col not in ['name', 'subsystem', 'created_at']]\n",
    "\n",
    "    # 去除不需要的列\n",
    "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    # 确保 name, subsystem 和 created_at 存在\n",
    "    required_columns = ['name', 'subsystem', 'created_at']\n",
    "    missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"文件 {file_path} 缺少以下列: {missing_cols}\")\n",
    "        return None\n",
    "\n",
    "    # 统一处理时间格式\n",
    "    if 'created_at' in df.columns:\n",
    "        try:\n",
    "            # 尝试多种常见时间格式\n",
    "            df['created_at'] = pd.to_datetime(df['created_at'],\n",
    "                                             format='mixed',\n",
    "                                             errors='coerce')\n",
    "\n",
    "            # 检查是否有无效时间\n",
    "            if df['created_at'].isnull().any():\n",
    "                print(f\"警告: 文件 {file_path} 中有无法解析的时间格式\")\n",
    "                # 尝试其他可能的格式\n",
    "                df['created_at'] = pd.to_datetime(df['created_at'],\n",
    "                                                format='%Y-%m-%d %H:%M:%S',\n",
    "                                                errors='coerce')\n",
    "\n",
    "            # 删除无效时间的行\n",
    "            df = df.dropna(subset=['created_at'])\n",
    "\n",
    "            # 按时间排序\n",
    "            df = df.sort_values('created_at').reset_index(drop=True)\n",
    "        except Exception as e:\n",
    "            print(f\"处理文件 {file_path} 的时间列时出错: {e}\")\n",
    "            return None\n",
    "\n",
    "    return df\n",
    "\n",
    "# 定义要处理的 .csv 文件列表\n",
    "file_paths = [\n",
    "    'D:/code/junma/600000/0424/product.csv',\n",
    "    'D:/code/junma/600000/0424/speed.csv',\n",
    "    'D:/code/junma/600000/0424/percent.csv',\n",
    "    'D:/code/junma/600000/0424/mstatus.csv',\n",
    "    'D:/code/junma/600000/0424/dstatus.csv',\n",
    "    'D:/code/junma/600000/0424/rstatus.csv'\n",
    "]\n",
    "\n",
    "# 读取并预处理所有文件\n",
    "dfs = []\n",
    "for i, file_path in enumerate(file_paths):\n",
    "    try:\n",
    "        # 更可靠的文件存在性检查\n",
    "        import os\n",
    "        if os.path.exists(file_path):\n",
    "            df = read_and_preprocess(file_path, is_first_file=(i==0))\n",
    "            if df is not None:\n",
    "                dfs.append(df)\n",
    "                print(f\"已加载文件: {file_path}, 行数: {len(df)}\")\n",
    "                print(f\"时间范围: {df['created_at'].min()} 到 {df['created_at'].max()}\")\n",
    "            else:\n",
    "                print(f\"文件 {file_path} 预处理失败\")\n",
    "        else:\n",
    "            print(f\"文件不存在：{file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"加载文件 {file_path} 时出错: {e}\")\n",
    "\n",
    "# 检查 dfs 是否为空\n",
    "if not dfs:\n",
    "    print(\"错误: 没有可合并的数据 - 可能原因:\")\n",
    "    print(\"1. 所有文件路径不正确\")\n",
    "    print(\"2. 所有文件缺少必要的列(name, subsystem, created_at)\")\n",
    "    print(\"3. 所有文件读取失败\")\n",
    "    merged_df = None\n",
    "else:\n",
    "    try:\n",
    "        # 按第一个数据框中的 name, subsystem 和 created_at 进行对齐和拼接\n",
    "        merged_df = dfs[0]\n",
    "\n",
    "        for i in range(1, len(dfs)):\n",
    "            try:\n",
    "                temp_df = dfs[i]\n",
    "                # 按 name, subsystem 和 created_at 进行左连接\n",
    "                merged_df = pd.merge(\n",
    "                    merged_df,\n",
    "                    temp_df,\n",
    "                    on=['name', 'subsystem', 'created_at'],\n",
    "                    how='outer',\n",
    "                    suffixes=('', f'_{i}')\n",
    "                )\n",
    "                print(f\"成功合并文件: {file_paths[i]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"合并文件 {file_paths[i]} 时出错: {e}\")\n",
    "\n",
    "        # 检查是否有重复列名\n",
    "        duplicate_cols = merged_df.columns[merged_df.columns.duplicated()]\n",
    "        if len(duplicate_cols) > 0:\n",
    "            print(f\"警告: 存在重复列名: {duplicate_cols}\")\n",
    "\n",
    "        # 去除多余重复的 name, subsystem 和 created_at 列\n",
    "        cols_to_keep = []\n",
    "        for col in merged_df.columns:\n",
    "            if col.startswith('name') or col.startswith('subsystem') or col.startswith('created_at'):\n",
    "                if col in cols_to_keep:\n",
    "                    continue\n",
    "                cols_to_keep.append(col)\n",
    "            else:\n",
    "                cols_to_keep.append(col)\n",
    "        merged_df = merged_df[cols_to_keep]\n",
    "\n",
    "        # 修改后的时间筛选部分 - 更精确的时间对齐\n",
    "        if 'created_at' in merged_df.columns and len(dfs) > 0:\n",
    "            # 找出所有表中共同的时间范围\n",
    "            min_time = max(df['created_at'].min() for df in dfs)\n",
    "            max_time = min(df['created_at'].max() for df in dfs)\n",
    "\n",
    "            print(f\"所有表共有的时间范围: {min_time} 到 {max_time}\")\n",
    "\n",
    "            if min_time >= max_time:\n",
    "                print(\"错误: 没有共同的时间范围\")\n",
    "                merged_df = None\n",
    "            else:\n",
    "                # 筛选数据 - 精确到秒级的时间匹配\n",
    "                merged_df = merged_df[(merged_df['created_at'] >= min_time) &\n",
    "                                     (merged_df['created_at'] <= max_time)]\n",
    "\n",
    "                # 将时间列四舍五入到秒，确保精确匹配\n",
    "                merged_df['created_at'] = merged_df['created_at'].dt.round('S')\n",
    "\n",
    "                print(f\"已筛选所有表共有的时间范围数据，剩余行数: {len(merged_df)}\")\n",
    "\n",
    "    except IndexError:\n",
    "        print(\"严重错误: 数据框列表索引越界，请检查数据加载情况\")\n",
    "        merged_df = None\n",
    "\n",
    "# 保存结果\n",
    "if merged_df is not None:\n",
    "    output_path = 'D:/code/junma/600000/0424/merged_data.csv'\n",
    "    try:\n",
    "        # 确保时间格式统一为 ISO 8601 格式\n",
    "        merged_df['created_at'] = merged_df['created_at'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        merged_df.to_csv(output_path, index=False, encoding='GBK')\n",
    "        print(f\"合并完成，已保存到: {output_path}\")\n",
    "        print(f\"合并后的数据形状: {merged_df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"保存数据时出错: {e}\")\n",
    "else:\n",
    "    print(\"合并失败，请检查上述错误信息\")"
   ],
   "id": "8e6981d3a65fd005",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#7行\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 读取数据\n",
    "merged_df = pd.read_csv('D:/code/junma/600000/0424/merged_data2.csv', encoding='GBK')\n",
    "\n",
    "# 2. 时间解析（修复异常时间）\n",
    "merged_df['created_at'] = merged_df['created_at'].str.replace('25:00:00', '23:59:59')  # 示例修正\n",
    "merged_df['created_at'] = pd.to_datetime(merged_df['created_at'], errors='coerce')\n",
    "\n",
    "# 3. 检查 NX78 的时间解析情况\n",
    "print(\"NX78 的无效时间记录:\", merged_df[merged_df['subsystem'] == 'NX78']['created_at'].isna().sum())\n",
    "\n",
    "# 4. 获取所有设备名\n",
    "all_devices = merged_df['subsystem'].unique()\n",
    "print(\"\\n所有设备名:\", all_devices)\n",
    "\n",
    "# 5. 按设备分组 + 重采样（修改这部分以保留每分钟所有记录）\n",
    "merged_df.set_index('created_at', inplace=True)\n",
    "\n",
    "# 创建时间分组键（按分钟）\n",
    "merged_df['time_group'] = merged_df.index.floor('1min')\n",
    "\n",
    "resampled_dfs = []\n",
    "for device in all_devices:\n",
    "    device_data = merged_df[merged_df['subsystem'] == device]\n",
    "\n",
    "    # 按分钟分组，保留每组所有记录\n",
    "    device_grouped = device_data.groupby('time_group')\n",
    "\n",
    "    # 对每分钟的数据进行处理（这里直接保留所有记录）\n",
    "    device_resampled = device_grouped.apply(lambda x: x)\n",
    "\n",
    "    # 如果有多条记录，可以在这里添加处理逻辑（如排序等）\n",
    "    resampled_dfs.append(device_resampled)\n",
    "\n",
    "# 合并所有设备数据\n",
    "resampled = pd.concat(resampled_dfs)\n",
    "\n",
    "# 6. 按设备名分组填充缺失值（确保填充不跨设备）\n",
    "resampled_filled = resampled.groupby('subsystem').apply(lambda x: x.ffill().bfill())\n",
    "\n",
    "# 7. 删除全0列（保留班次列）\n",
    "# 定义班次列名\n",
    "shift_columns = ['C_dw3', 'C_dw4', 'C_dw5', 'C_dw6']\n",
    "\n",
    "# 删除所有值都为0的列（排除班次列）\n",
    "mask = ((resampled_filled != 0).any(axis=0)) | resampled_filled.columns.isin(shift_columns)\n",
    "df = resampled_filled.loc[:, mask]\n",
    "\n",
    "# 8. 检查 NX78 是否存在\n",
    "print(\"\\n最终数据中的设备列表:\", df['subsystem'].unique())\n",
    "print(\"NX78 的数据量:\", len(df[df['subsystem'] == 'NX78']))\n",
    "\n",
    "# 9. 保存\n",
    "df.to_csv('D:/code/junma/600000/0424/processed_time_series_filled2.csv', index=True, encoding='GBK')"
   ],
   "id": "657dd5f12e3fcfb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#6行\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 读取数据\n",
    "merged_df = pd.read_csv('D:/code/junma/600000/0424/merged_data.csv', encoding='GBK')\n",
    "\n",
    "# 2. 时间解析（修复异常时间）\n",
    "merged_df['created_at'] = merged_df['created_at'].str.replace('25:00:00', '23:59:59')  # 示例修正\n",
    "merged_df['created_at'] = pd.to_datetime(merged_df['created_at'], errors='coerce')\n",
    "\n",
    "# 3. 检查 NX78 的时间解析情况\n",
    "print(\"NX78 的无效时间记录:\", merged_df[merged_df['subsystem'].str.startswith('NX78-')]['created_at'].isna().sum())\n",
    "\n",
    "# 4. 获取所有设备名\n",
    "all_devices = merged_df['subsystem'].unique()\n",
    "print(\"\\n所有设备名:\", all_devices)\n",
    "\n",
    "# 5. 提取设备基础名和面标识\n",
    "# 根据数据结构，设备两面信息在subsystem列中，如\"NX78-R103\"或\"NX78-L102\"\n",
    "merged_df['base_name'] = merged_df['subsystem'].str.extract(r'^(.*?)-[LR]')[0]  # 提取基础名(如NX78)\n",
    "merged_df['side'] = merged_df['subsystem'].str.extract(r'-([LR]\\d+)$')[0]  # 提取面标识(如R103)\n",
    "\n",
    "# 6. 按设备基础名和时间分组 + 重采样\n",
    "merged_df.set_index('created_at', inplace=True)\n",
    "resampled_dfs = []\n",
    "\n",
    "for base_name in merged_df['base_name'].unique():\n",
    "    # 获取该基础设备的所有数据\n",
    "    base_data = merged_df[merged_df['base_name'] == base_name]\n",
    "\n",
    "    # 按分钟和面标识分组，保留每分钟每面的第一条记录\n",
    "    resampled_base = base_data.groupby([pd.Grouper(freq='1min'), 'side']).first().reset_index()\n",
    "\n",
    "    resampled_dfs.append(resampled_base)\n",
    "\n",
    "# 合并所有设备数据\n",
    "resampled = pd.concat(resampled_dfs)\n",
    "\n",
    "# 7. 恢复原始设备名（使用subsystem列）\n",
    "resampled['subsystem'] = resampled['base_name'] + '-' + resampled['side']\n",
    "resampled.drop(['base_name', 'side'], axis=1, inplace=True)\n",
    "resampled.set_index('created_at', inplace=True)\n",
    "\n",
    "# 8. 按设备名分组填充缺失值（确保填充不跨设备）\n",
    "resampled_filled = resampled.groupby('subsystem').apply(lambda x: x.ffill().bfill())\n",
    "\n",
    "# 9. 删除全0列（保留班次列）\n",
    "shift_columns = ['C_dw3', 'C_dw4', 'C_dw5', 'C_dw6']\n",
    "mask = ((resampled_filled != 0).any(axis=0)) | resampled_filled.columns.isin(shift_columns)\n",
    "df = resampled_filled.loc[:, mask]\n",
    "\n",
    "# 10. 检查 NX78 是否存在\n",
    "print(\"\\n最终数据中的设备列表:\", df['subsystem'].unique())\n",
    "print(\"NX78 的数据量:\", len(df[df['subsystem'].str.startswith('NX78-')]))\n",
    "\n",
    "# 11. 保存\n",
    "df.to_csv('D:/code/junma/600000/0424/processed_time_series_filled.csv', index=True, encoding='GBK')"
   ],
   "id": "2b63829ae071aedc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#7\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/processed_time_series_filled2.csv\")\n",
    "\n",
    "# 确保时间列是datetime类型并按时间排序\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "df = df.sort_values(['subsystem', 'created_at'])\n",
    "\n",
    "# --- 核心修改：统一设备数据行数 ---\n",
    "def uniform_sample_by_device(df):\n",
    "    # 获取每个设备的数据行数\n",
    "    device_counts = df['subsystem'].value_counts()\n",
    "    min_rows = device_counts.min()  # 最小行数\n",
    "\n",
    "    print(colored(f\"所有设备的最小行数: {min_rows}\", 'green'))\n",
    "    print(colored(\"原始设备行数分布:\", 'blue'))\n",
    "    print(device_counts)\n",
    "\n",
    "    # 对每个设备均匀采样\n",
    "    sampled_dfs = []\n",
    "    for device in df['subsystem'].unique():\n",
    "        device_df = df[df['subsystem'] == device]\n",
    "        if len(device_df) > min_rows:\n",
    "            # 按时间均匀采样（保留min_rows条）\n",
    "            device_df = device_df.iloc[\n",
    "                np.linspace(0, len(device_df)-1, num=min_rows, dtype=int)\n",
    "            ]\n",
    "        sampled_dfs.append(device_df)\n",
    "\n",
    "    # 合并结果\n",
    "    return pd.concat(sampled_dfs).sort_values(['subsystem', 'created_at'])\n",
    "\n",
    "# 执行均匀采样\n",
    "df_uniform = uniform_sample_by_device(df)\n",
    "df.to_csv('D:/code/junma/600000/0424/final2.csv', index=True, encoding='GBK')\n",
    "# 验证结果\n",
    "print(colored(\"\\n处理后设备行数分布:\", 'green'))\n",
    "print(df_uniform['subsystem'].value_counts())"
   ],
   "id": "c668bb2fb303d8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#6\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/processed_time_series_filled.csv\")\n",
    "\n",
    "# 确保时间列是datetime类型并按时间排序\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "df = df.sort_values(['name', 'created_at'])\n",
    "\n",
    "# --- 核心修改：统一设备数据行数 ---\n",
    "def uniform_sample_by_device(df):\n",
    "    # 获取每个设备的数据行数\n",
    "    device_counts = df['name'].value_counts()\n",
    "    min_rows = device_counts.min()  # 最小行数\n",
    "\n",
    "    print(colored(f\"所有设备的最小行数: {min_rows}\", 'green'))\n",
    "    print(colored(\"原始设备行数分布:\", 'blue'))\n",
    "    print(device_counts)\n",
    "\n",
    "    # 对每个设备均匀采样\n",
    "    sampled_dfs = []\n",
    "    for device in df['name'].unique():\n",
    "        device_df = df[df['name'] == device]\n",
    "        if len(device_df) > min_rows:\n",
    "            # 按时间均匀采样（保留min_rows条）\n",
    "            device_df = device_df.iloc[\n",
    "                np.linspace(0, len(device_df)-1, num=min_rows, dtype=int)\n",
    "            ]\n",
    "        sampled_dfs.append(device_df)\n",
    "\n",
    "    # 合并结果\n",
    "    return pd.concat(sampled_dfs).sort_values(['name', 'created_at'])\n",
    "\n",
    "# 执行均匀采样\n",
    "df_uniform = uniform_sample_by_device(df)\n",
    "df_uniform.to_csv('D:/code/junma/600000/0424/final.csv', index=True, encoding='GBK')\n",
    "# 验证结果\n",
    "print(colored(\"\\n处理后设备行数分布:\", 'green'))\n",
    "print(df_uniform['name'].value_counts())"
   ],
   "id": "683c6f2aa08917f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cfaa2ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:11.780244Z",
     "iopub.status.busy": "2023-09-11T21:34:11.779850Z",
     "iopub.status.idle": "2023-09-11T21:34:12.533340Z",
     "shell.execute_reply": "2023-09-11T21:34:12.531986Z"
    },
    "papermill": {
     "duration": 0.792042,
     "end_time": "2023-09-11T21:34:12.536393",
     "exception": false,
     "start_time": "2023-09-11T21:34:11.744351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "!pip install openpyxl\n",
    "#df = pd.read_excel(\"D:/code/agstar-livestock-ad-database-combined.xlsx\")\n",
    "!pip install termcolor\n",
    "from termcolor import colored\n",
    "#df = pd.read_csv(\"D:/code/data/merged_data.csv\")\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/final.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed2aa3ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:12.607418Z",
     "iopub.status.busy": "2023-09-11T21:34:12.606822Z",
     "iopub.status.idle": "2023-09-11T21:34:12.618790Z",
     "shell.execute_reply": "2023-09-11T21:34:12.617763Z"
    },
    "papermill": {
     "duration": 0.050368,
     "end_time": "2023-09-11T21:34:12.621083",
     "exception": false,
     "start_time": "2023-09-11T21:34:12.570715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def print_section_title(title):\n",
    "   print(colored(title, 'blue', attrs=['bold', 'underline']))\n",
    "\n",
    "def display_head_and_tail(dataframe, head=5):\n",
    "    display(dataframe.head(head).style.set_caption(\"Head\"))\n",
    "    display(dataframe.tail(head).style.set_caption(\"Tail\"))\n",
    "\n",
    "def display_na(dataframe):\n",
    "    na_df = dataframe.isnull().sum().reset_index()\n",
    "    na_df.columns = ['Column', 'Number of NA']\n",
    "    display(na_df.style.set_caption(\"Number of NA Values\"))\n",
    "\n",
    "def display_quantiles(dataframe):\n",
    "    quantiles_df = dataframe.describe([0, 0.05, 0.50, 0.95, 0.99, 1]).T\n",
    "    display(quantiles_df.style.format(\"{:.2f}\").set_caption(\"Quantiles\"))\n",
    "\n",
    "def check_df(dataframe, head=5):\n",
    "    print_section_title('Shape')\n",
    "    print(dataframe.shape)\n",
    "    print_section_title('Types')\n",
    "    print(dataframe.dtypes.to_frame('Data Type').style.set_caption(\"Data Types\"))\n",
    "    print_section_title('Info')\n",
    "    print(dataframe.info())\n",
    "    print_section_title('Head & Tail')\n",
    "    display_head_and_tail(dataframe, head)\n",
    "    print_section_title('NA Values')\n",
    "    display_na(dataframe)\n",
    "    print_section_title('Quantiles')\n",
    "    display_quantiles(dataframe)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#添加异常值检测的数值验证\n",
    "from termcolor import colored\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "def print_section_title(title):\n",
    "    print(colored(title, 'blue', attrs=['bold', 'underline']))\n",
    "\n",
    "def display_head_and_tail(dataframe, head=5):\n",
    "    display(dataframe.head(head).style.set_caption(\"Head\"))\n",
    "    display(dataframe.tail(head).style.set_caption(\"Tail\"))\n",
    "\n",
    "def display_na(dataframe):\n",
    "    na_df = dataframe.isnull().sum().reset_index()\n",
    "    na_df.columns = ['Column', 'Number of NA']\n",
    "    display(na_df.style.set_caption(\"Number of NA Values\"))\n",
    "\n",
    "def display_quantiles(dataframe):\n",
    "    quantiles_df = dataframe.describe([0, 0.05, 0.50, 0.95, 0.99, 1]).T\n",
    "    display(quantiles_df.style.format(\"{:.2f}\").set_caption(\"Quantiles\"))\n",
    "\n",
    "def detect_outliers(dataframe, method='iqr', threshold=1.5):\n",
    "    \"\"\"\n",
    "    检测数值型列的异常值，返回异常值统计和样本\n",
    "\n",
    "    参数:\n",
    "        dataframe: pd.DataFrame\n",
    "        method: 'iqr' (四分位距) 或 'zscore' (标准差)\n",
    "        threshold: 判定异常值的阈值（IQR默认为1.5，Z-Score默认为3）\n",
    "    \"\"\"\n",
    "    numeric_cols = dataframe.select_dtypes(include=[np.number]).columns\n",
    "    outliers_summary = []\n",
    "    outlier_samples = pd.DataFrame()\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if method == 'iqr':\n",
    "            Q1 = dataframe[col].quantile(0.25)\n",
    "            Q3 = dataframe[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "            outliers = dataframe[(dataframe[col] < lower_bound) | (dataframe[col] > upper_bound)]\n",
    "        elif method == 'zscore':\n",
    "            z_scores = np.abs((dataframe[col] - dataframe[col].mean()) / dataframe[col].std())\n",
    "            outliers = dataframe[z_scores > threshold]\n",
    "\n",
    "        n_outliers = len(outliers)\n",
    "        if n_outliers > 0:\n",
    "            outliers_summary.append({\n",
    "                'Column': col,\n",
    "                'Method': method.upper(),\n",
    "                'Threshold': threshold,\n",
    "                'Outliers Count': n_outliers,\n",
    "                'Outliers Ratio': f\"{(n_outliers / len(dataframe)) * 100:.2f}%\"\n",
    "            })\n",
    "            outlier_samples = pd.concat([outlier_samples, outliers])\n",
    "\n",
    "    if outliers_summary:\n",
    "        summary_df = pd.DataFrame(outliers_summary)\n",
    "        display(summary_df.style.set_caption(\n",
    "            f\"Outliers Detection Summary (Method: {method.upper()}, Threshold: {threshold})\"\n",
    "        ))\n",
    "        if not outlier_samples.empty:\n",
    "            print_section_title('Sample Outliers (First 5)')\n",
    "            display(outlier_samples.head().style.set_caption(\"Sample Outliers\"))\n",
    "    else:\n",
    "        print(colored(\"No outliers detected with current method and threshold.\", 'green'))\n",
    "\n",
    "def check_df(dataframe, head=5, enable_outlier_detection=True, outlier_method='iqr', outlier_threshold=1.5):\n",
    "    print_section_title('Shape')\n",
    "    print(dataframe.shape)\n",
    "    print_section_title('Types')\n",
    "    print(dataframe.dtypes.to_frame('Data Type').style.set_caption(\"Data Types\"))\n",
    "    print_section_title('Info')\n",
    "    dataframe.info()\n",
    "    print_section_title('Head & Tail')\n",
    "    display_head_and_tail(dataframe, head)\n",
    "    print_section_title('NA Values')\n",
    "    display_na(dataframe)\n",
    "    print_section_title('Quantiles')\n",
    "    display_quantiles(dataframe)\n",
    "\n",
    "    if enable_outlier_detection:  # 修改参数名\n",
    "        print_section_title('Outliers Detection')\n",
    "        detect_outliers(dataframe, method=outlier_method, threshold=outlier_threshold)  # 调用函数"
   ],
   "id": "8c4abf98d41b16fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "309d6c92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:12.692315Z",
     "iopub.status.busy": "2023-09-11T21:34:12.691685Z",
     "iopub.status.idle": "2023-09-11T21:34:12.849697Z",
     "shell.execute_reply": "2023-09-11T21:34:12.848573Z"
    },
    "papermill": {
     "duration": 0.196904,
     "end_time": "2023-09-11T21:34:12.852332",
     "exception": false,
     "start_time": "2023-09-11T21:34:12.655428",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "check_df(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "be1c3632",
   "metadata": {
    "papermill": {
     "duration": 0.037753,
     "end_time": "2023-09-11T21:34:12.926571",
     "exception": false,
     "start_time": "2023-09-11T21:34:12.888818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>4 |</span></b> <b>Feature Engineering</b></div>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/final.csv\")\n",
    "\n",
    "# 数据预处理\n",
    "def preprocess_data(df):\n",
    "    # 确保created_at是datetime类型\n",
    "    if 'created_at' in df.columns:\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "    # 提取设备基础名称（去掉-L/-R部分）\n",
    "    df['base_name'] = df['subsystem'].str.extract(r'^(NX\\d+)')\n",
    "\n",
    "    # 提取设备面（L或R）\n",
    "    df['side'] = df['subsystem'].str.extract(r'^(NX\\d+)-([LR])')[1]\n",
    "\n",
    "    return df\n",
    "\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# 1. 锭位状态分析（按设备面）\n",
    "def analyze_spindle_status_by_side(df):\n",
    "    # 计算各状态锭位占比\n",
    "    total_spindles = 100  # 每台设备有100个锭子\n",
    "\n",
    "    # 按设备面分组分析\n",
    "    grouped = df.groupby(['subsystem', 'base_name', 'side'])\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for (subsystem, base_name, side), group in grouped:\n",
    "        # 计算各状态锭位数的累计值\n",
    "        stop_spindles = group['M_dw2'].sum() + group['M_dw5'].sum() + group['M_dw6'].sum()\n",
    "        running_spindles = group['M_dw3'].sum()\n",
    "        full_spindles = group['M_dw4'].sum()\n",
    "        broken_spindles = group['M_dw5'].sum()\n",
    "        overheated_spindles = group['M_dw6'].sum()\n",
    "\n",
    "        # 计算平均占比（按记录次数平均）\n",
    "        num_records = len(group)\n",
    "        avg_stop = stop_spindles / (num_records * total_spindles) * 100\n",
    "        avg_running = running_spindles / (num_records * total_spindles) * 100\n",
    "        avg_full = full_spindles / (num_records * total_spindles) * 100\n",
    "        avg_broken = broken_spindles / (num_records * total_spindles) * 100\n",
    "        avg_overheated = overheated_spindles / (num_records * total_spindles) * 100\n",
    "\n",
    "        results.append({\n",
    "            '设备名称': subsystem,\n",
    "            '基础名称': base_name,\n",
    "            '面': side,\n",
    "            '停止锭位率(%)': avg_stop,\n",
    "            '运行锭位率(%)': avg_running,\n",
    "            '满管锭位率(%)': avg_full,\n",
    "            '断纱锭位率(%)': avg_broken,\n",
    "            '过热锭位率(%)': avg_overheated,\n",
    "            '断纱锭位总数': broken_spindles,\n",
    "            '过热锭位总数': overheated_spindles,\n",
    "            '记录次数': num_records\n",
    "        })\n",
    "\n",
    "    # 创建结果DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"\\n各设备面锭位状态占比分析：\")\n",
    "    print(result_df.round(2))\n",
    "\n",
    "    # 绘制整体锭位状态饼图（按累计值）\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    overall_stop = df['M_dw2'].sum() + df['M_dw5'].sum() + df['M_dw6'].sum()\n",
    "    overall_running = df['M_dw3'].sum()\n",
    "    overall_full = df['M_dw4'].sum()\n",
    "    total = overall_stop + overall_running + overall_full\n",
    "\n",
    "    plt.pie([overall_stop/total*100, overall_running/total*100, overall_full/total*100],\n",
    "            labels=['停止锭位(含断纱/过热)', '运行锭位', '满管锭位'],\n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    plt.title(\"整体锭位状态占比(累计)\")\n",
    "    plt.show()\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# 2. 机器运行状态分析（按设备面）\n",
    "def analyze_machine_status_by_side(df):\n",
    "    if 'M_dw1' not in df.columns:\n",
    "        print(\"\\n警告：缺少M_dw1列，无法分析机器运行状态\")\n",
    "        return None\n",
    "\n",
    "    # 按设备面分组分析\n",
    "    grouped = df.groupby(['subsystem', 'base_name', 'side'])\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for (subsystem, base_name, side), group in grouped:\n",
    "        machine_stop_ratio = (group['M_dw1'] == 0).mean() * 100\n",
    "        machine_running_ratio = (group['M_dw1'] == 2).mean() * 100\n",
    "\n",
    "        results.append({\n",
    "            '设备名称': subsystem,\n",
    "            '基础名称': base_name,\n",
    "            '面': side,\n",
    "            '停机率(%)': machine_stop_ratio,\n",
    "            '运行率(%)': machine_running_ratio\n",
    "        })\n",
    "\n",
    "    # 创建结果DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"\\n各设备面机器运行状态分析：\")\n",
    "    print(result_df.round(2))\n",
    "\n",
    "    # 绘制整体机器状态饼图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    overall_stop = (df['M_dw1'] == 0).mean() * 100\n",
    "    overall_running = (df['M_dw1'] == 2).mean() * 100\n",
    "\n",
    "    plt.pie([overall_stop, overall_running],\n",
    "            labels=['停机', '运行'],\n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    plt.title(\"整体机器运行状态\")\n",
    "    plt.show()\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# 3. 停机设备分析（按设备面）\n",
    "def analyze_downtime_by_side(df):\n",
    "    required_cols = ['M_dw1', 'created_at', 'subsystem', 'base_name', 'side']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "\n",
    "    if missing_cols:\n",
    "        print(f\"\\n警告：缺少必要列 {missing_cols}，无法进行停机时长分析\")\n",
    "        return None\n",
    "\n",
    "    # 按设备面和时间排序\n",
    "    grouped = df.sort_values(['subsystem', 'created_at']).groupby(['subsystem', 'base_name', 'side'])\n",
    "\n",
    "    print(\"\\n各设备面停机时长分析：\")\n",
    "    all_downtimes = []\n",
    "\n",
    "    for (subsystem, base_name, side), group in grouped:\n",
    "        # 找出状态变化点 (0表示停机，2表示运行)\n",
    "        group['status_change'] = group['M_dw1'].diff().ne(0)\n",
    "        status_changes = group[group['status_change']]\n",
    "\n",
    "        # 初始化变量\n",
    "        downtime_start = None\n",
    "        downtime_durations = []\n",
    "\n",
    "        # 遍历状态变化点\n",
    "        for _, row in status_changes.iterrows():\n",
    "            if row['M_dw1'] == 0:  # 开始停机\n",
    "                downtime_start = row['created_at']\n",
    "            elif row['M_dw1'] == 2 and downtime_start is not None:  # 结束停机\n",
    "                downtime_end = row['created_at']\n",
    "                duration = (downtime_end - downtime_start).total_seconds() / 60  # 转换为分钟\n",
    "                downtime_durations.append(duration)\n",
    "                downtime_start = None\n",
    "\n",
    "        # 如果最后一次状态是停机，计算到最后一个时间点的时长\n",
    "        if downtime_start is not None and not group.empty:\n",
    "            last_time = group['created_at'].iloc[-1]\n",
    "            duration = (last_time - downtime_start).total_seconds() / 60\n",
    "            downtime_durations.append(duration)\n",
    "\n",
    "        # 计算总停机时长\n",
    "        total_downtime = sum(downtime_durations)\n",
    "\n",
    "        if total_downtime > 0 or downtime_durations:\n",
    "            print(f\"设备: {subsystem}\")\n",
    "            print(f\"  总停机时长: {total_downtime:.2f} 分钟\")\n",
    "            print(f\"  停机次数: {len(downtime_durations)}\")\n",
    "            if downtime_durations:\n",
    "                print(f\"  最长单次停机: {max(downtime_durations):.2f} 分钟\")\n",
    "                print(f\"  平均停机时长: {sum(downtime_durations)/len(downtime_durations):.2f} 分钟\")\n",
    "            print(\"  -------------------\")\n",
    "\n",
    "            # 保存信息用于后续分析\n",
    "            all_downtimes.append({\n",
    "                '设备名称': subsystem,\n",
    "                '基础名称': base_name,\n",
    "                '面': side,\n",
    "                '总停机时长(分钟)': total_downtime,\n",
    "                '停机次数': len(downtime_durations),\n",
    "                '最长单次停机(分钟)': max(downtime_durations) if downtime_durations else 0,\n",
    "                '平均停机时长(分钟)': sum(downtime_durations)/len(downtime_durations) if downtime_durations else 0\n",
    "            })\n",
    "\n",
    "    if all_downtimes:\n",
    "        downtime_df = pd.DataFrame(all_downtimes)\n",
    "        print(\"\\n停机设备汇总:\")\n",
    "        print(downtime_df)\n",
    "\n",
    "        # 保存到CSV\n",
    "        downtime_df.to_csv(\"D:/code/junma/600000/0424/downtime_analysis_by_side.csv\", index=False)\n",
    "        print(\"\\n已保存停机分析结果到: D:/code/junma/600000/0424/downtime_analysis_by_side.csv\")\n",
    "\n",
    "        # 绘制停机时长分布图\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        downtime_df.sort_values('总停机时长(分钟)', ascending=False).plot(\n",
    "            x='设备名称', y='总停机时长(分钟)', kind='bar', legend=False)\n",
    "        plt.title(\"各设备面总停机时长对比\")\n",
    "        plt.ylabel(\"分钟\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return downtime_df\n",
    "    else:\n",
    "        print(\"\\n没有检测到任何设备的停机记录\")\n",
    "        return None\n",
    "\n",
    "# 4. 分析断纱次数最多的设备面\n",
    "def analyze_most_broken_spindles_by_side(df):\n",
    "    if 'M_dw5' not in df.columns or 'subsystem' not in df.columns:\n",
    "        print(\"\\n警告：缺少必要列，无法分析断纱次数最多的设备\")\n",
    "        return None\n",
    "\n",
    "    # 计算每个设备面的断纱锭位总数\n",
    "    broken_counts = df.groupby(['subsystem', 'base_name', 'side'])['M_dw5'].sum().reset_index()\n",
    "    broken_counts.columns = ['设备名称', '基础名称', '面', '断纱锭位总数']\n",
    "\n",
    "    # 找出断纱最多的设备面\n",
    "    most_broken = broken_counts.loc[broken_counts['断纱锭位总数'].idxmax()]\n",
    "    print(f\"\\n断纱次数最多的设备面: {most_broken['设备名称']} (断纱锭位总数: {most_broken['断纱锭位总数']})\")\n",
    "\n",
    "    # 分析该设备面的锭位状态\n",
    "    device_data = df[df['subsystem'] == most_broken['设备名称']]\n",
    "    print(f\"\\n设备面 {most_broken['设备名称']} 的锭位状态分析:\")\n",
    "\n",
    "    # 计算该设备面的锭位状态累计值\n",
    "    total_spindles = 100 * len(device_data)  # 总锭位记录次数\n",
    "    stop_spindles = device_data['M_dw2'].sum() + device_data['M_dw5'].sum() + device_data['M_dw6'].sum()\n",
    "    running_spindles = device_data['M_dw3'].sum()\n",
    "    full_spindles = device_data['M_dw4'].sum()\n",
    "    broken_spindles = device_data['M_dw5'].sum()\n",
    "    overheated_spindles = device_data['M_dw6'].sum()\n",
    "\n",
    "    # 计算占比\n",
    "    stop_ratio = stop_spindles / total_spindles * 100\n",
    "    running_ratio = running_spindles / total_spindles * 100\n",
    "    full_ratio = full_spindles / total_spindles * 100\n",
    "    broken_ratio = broken_spindles / total_spindles * 100\n",
    "    overheated_ratio = overheated_spindles / total_spindles * 100\n",
    "\n",
    "    # 打印详细状态\n",
    "    print(f\"停止锭位率: {stop_ratio:.2f}%\")\n",
    "    print(f\"运行锭位率: {running_ratio:.2f}%\")\n",
    "    print(f\"满管锭位率: {full_ratio:.2f}%\")\n",
    "    print(f\"断纱锭位率: {broken_ratio:.2f}%\")\n",
    "    print(f\"过热锭位率: {overheated_ratio:.2f}%\")\n",
    "\n",
    "    # 绘制该设备面的锭位状态饼图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.pie([stop_ratio, running_ratio, full_ratio],\n",
    "            labels=['停止锭位(含断纱/过热)', '运行锭位', '满管锭位'],\n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    plt.title(f\"设备面 {most_broken['设备名称']}\\n锭位状态占比\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.pie([broken_ratio, overheated_ratio, (stop_ratio - broken_ratio - overheated_ratio)],\n",
    "            labels=['断纱锭位', '过热锭位', '其他停止锭位'],\n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    plt.title(f\"设备面 {most_broken['设备名称']}\\n停止锭位细分\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return most_broken\n",
    "\n",
    "# 执行分析\n",
    "print(\"=\"*50)\n",
    "print(\"锭位状态分析（按设备面）\")\n",
    "print(\"=\"*50)\n",
    "spindle_status_by_side = analyze_spindle_status_by_side(df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"机器运行状态分析（按设备面）\")\n",
    "print(\"=\"*50)\n",
    "machine_status_by_side = analyze_machine_status_by_side(df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"停机设备分析（按设备面）\")\n",
    "print(\"=\"*50)\n",
    "downtime_analysis_by_side = analyze_downtime_by_side(df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"断纱次数最多的设备面分析\")\n",
    "print(\"=\"*50)\n",
    "most_broken_device_by_side = analyze_most_broken_spindles_by_side(df)\n",
    "\n",
    "# 删除停机设备（C_dw1=0的记录）\n",
    "if 'C_dw1' in df.columns:\n",
    "    df = df[df['C_dw1'] != 0]\n",
    "    df.to_csv(\"D:/code/junma/600000/0424/second_final.csv\", index=False)"
   ],
   "id": "807084919cd5ea46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#斜线产量变化\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "# 班次名称映射\n",
    "shift_names = {\n",
    "    'C_dw3': '第一班产量',\n",
    "    'C_dw4': '第二班产量'\n",
    "}\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "\n",
    "# 检查必要列是否存在\n",
    "required_cols = ['subsystem', 'created_at', 'C_dw1', 'C_dw2', 'C_dw3', 'C_dw4']\n",
    "missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"数据中缺少必要的列: {missing_cols}\")\n",
    "\n",
    "# 预处理数据\n",
    "def preprocess_data(df):\n",
    "    # 转换时间列为datetime格式\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "    # 提取设备基础名称（去掉L/R和后面的数字）\n",
    "    df['device_base'] = df['subsystem'].str.extract(r'(NX\\d+)-[LR]')\n",
    "\n",
    "    # 提取设备面（L或R）\n",
    "    df['device_side'] = df['subsystem'].str.extract(r'-(L|R)')\n",
    "\n",
    "    # 过滤掉无效数据\n",
    "    if 'C_dw1' in df.columns:\n",
    "        # 标识设备状态为0的设备\n",
    "        down_devices = df[df['C_dw1'] == 0]['subsystem'].unique()\n",
    "        down_devices = list(set(down_devices) - set(df[df['subsystem'].isin(down_devices)][df[['C_dw3', 'C_dw4']].sum(axis=1) != 0]['subsystem'].unique()))\n",
    "\n",
    "        if down_devices:\n",
    "            print(f\"以下设备由于 C_dw1 为 0 被标记为停机设备: {down_devices}\")\n",
    "            df = df[~df['subsystem'].isin(down_devices)]\n",
    "            print(f\"已删除 {len(down_devices)} 个停机设备: {down_devices}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# 分析每台设备的产量变化\n",
    "def analyze_device_production(df):\n",
    "    # 获取所有设备基础名称（不区分L/R）\n",
    "    device_bases = df['device_base'].unique()\n",
    "\n",
    "    for device in device_bases:\n",
    "        # 获取该设备的两面数据\n",
    "        device_data = df[df['device_base'] == device]\n",
    "\n",
    "        if len(device_data) == 0:\n",
    "            continue\n",
    "\n",
    "        # 按时间排序\n",
    "        device_data = device_data.sort_values('created_at')\n",
    "\n",
    "        # 创建图形\n",
    "        plt.figure(figsize=(15, 8))\n",
    "\n",
    "        # 绘制每面的产量变化\n",
    "        for side in ['L', 'R']:\n",
    "            side_data = device_data[device_data['device_side'] == side]\n",
    "            if len(side_data) > 0:\n",
    "                plt.plot(side_data['created_at'], side_data['C_dw3'],\n",
    "                         label=f'{device}-{side} 第一班产量', linestyle='-', marker='o')\n",
    "                plt.plot(side_data['created_at'], side_data['C_dw4'],\n",
    "                         label=f'{device}-{side} 第二班产量', linestyle='--', marker='x')\n",
    "\n",
    "        plt.title(f\"设备 {device} 两面产量变化\")\n",
    "        plt.ylabel(\"产量\")\n",
    "        plt.xlabel(\"时间\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 打印统计数据\n",
    "        print(f\"\\n设备 {device} 产量统计:\")\n",
    "        for side in ['L', 'R']:\n",
    "            side_data = device_data[device_data['device_side'] == side]\n",
    "            if len(side_data) > 0:\n",
    "                print(f\"\\n{device}-{side} 面:\")\n",
    "                print(f\"第一班平均产量: {side_data['C_dw3'].mean():.2f}\")\n",
    "                print(f\"第二班平均产量: {side_data['C_dw4'].mean():.2f}\")\n",
    "                print(f\"总平均产量: {(side_data['C_dw3'].mean() + side_data['C_dw4'].mean()):.2f}\")\n",
    "\n",
    "                # 打印时间范围内的产量变化\n",
    "                print(\"\\n时间序列产量数据:\")\n",
    "                print(side_data[['created_at', 'C_dw3', 'C_dw4']].rename(columns={\n",
    "                    'C_dw3': '第一班产量',\n",
    "                    'C_dw4': '第二班产量'\n",
    "                }))\n",
    "\n",
    "# 执行分析\n",
    "analyze_device_production(df)\n",
    "\n",
    "# 保存处理后的数据\n",
    "df.to_csv('D:/code/junma/600000/0424/second_final_processed.csv', index=False)\n",
    "print(f\"\\n处理后的数据已保存到 second_final_processed.csv\")"
   ],
   "id": "fd7930f062ac50f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#提取断纱前速度，卷装程度等相关数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Spindle status mapping\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Running\",\n",
    "    2: \"Full\",\n",
    "    3: \"Broken\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Overspeed\",\n",
    "    7: \"Offline\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# Process parameter columns\n",
    "process_params = [\n",
    "    'R_dw21', 'R_dw22', 'R_dw23', 'R_dw24', 'R_dw25', 'R_dw26',\n",
    "    'R_dw29', 'R_dw30', 'R_dw31', 'R_dw32', 'R_dw33'\n",
    "]\n",
    "\n",
    "def find_broken_spindle_records(df):\n",
    "    # 1. Find top 10 devices with most broken yarn occurrences\n",
    "    device_broken_counts = defaultdict(int)\n",
    "\n",
    "    # Count broken yarn occurrences for each device\n",
    "    for i in range(1, 101):\n",
    "        status_col = f\"D_dw{i}\"\n",
    "        if status_col in df.columns:\n",
    "            broken_records = df[df[status_col] == 3]\n",
    "            for _, row in broken_records.iterrows():\n",
    "                device_broken_counts[row['subsystem']] += 1\n",
    "\n",
    "    if not device_broken_counts:\n",
    "        print(\"No broken yarn records found in the data\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Get top 10 devices with most broken yarns\n",
    "    top_devices = sorted(device_broken_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    top_device_names = [device[0] for device in top_devices]\n",
    "    print(\"Top 10 devices with most broken yarn occurrences:\")\n",
    "    for device, count in top_devices:\n",
    "        print(f\"{device}: {count} times\")\n",
    "\n",
    "    # 2. Extract data around broken yarn events for each device\n",
    "    result_data = []\n",
    "\n",
    "    for device_name in top_device_names:\n",
    "        device_data = df[df['subsystem'] == device_name].copy()\n",
    "        device_data['created_at'] = pd.to_datetime(device_data['created_at'])\n",
    "        device_data = device_data.sort_values('created_at')\n",
    "\n",
    "        # Track processed spindles for this device\n",
    "        processed_spindles = set()\n",
    "\n",
    "        # Find broken yarn events for each spindle\n",
    "        for spindle in range(1, 101):\n",
    "            status_col = f\"D_dw{spindle}\"\n",
    "            speed_col = f\"V_dw{spindle}\"\n",
    "            fullness_col = f\"P_dw{spindle}\"\n",
    "\n",
    "            if status_col not in device_data.columns:\n",
    "                continue\n",
    "\n",
    "            # Find all broken yarn records\n",
    "            broken_records = device_data[device_data[status_col] == 3]\n",
    "            if len(broken_records) == 0:\n",
    "                continue\n",
    "\n",
    "            # Process each broken yarn event\n",
    "            for _, broken_record in broken_records.iterrows():\n",
    "                current_time = broken_record['created_at']\n",
    "                current_idx = broken_record.name\n",
    "\n",
    "                # Get previous record\n",
    "                prev_records = device_data[device_data['created_at'] < current_time]\n",
    "                if len(prev_records) == 0:\n",
    "                    continue\n",
    "\n",
    "                prev_record = prev_records.iloc[-1]\n",
    "\n",
    "                # Validate previous record (speed not 0, status not broken)\n",
    "                if (prev_record[speed_col] == 0 or\n",
    "                    prev_record[status_col] == 3 or\n",
    "                    prev_record[fullness_col] == 0):\n",
    "                    continue\n",
    "\n",
    "                # Get next record\n",
    "                next_records = device_data[device_data['created_at'] > current_time]\n",
    "                if len(next_records) == 0:\n",
    "                    continue\n",
    "\n",
    "                next_record = next_records.iloc[0]\n",
    "\n",
    "                # Validate next record (same device)\n",
    "                if next_record['subsystem'] != device_name:\n",
    "                    continue\n",
    "\n",
    "                # Calculate time difference (minutes)\n",
    "                time_diff = (next_record['created_at'] - prev_record['created_at']).total_seconds() / 60\n",
    "\n",
    "                # Prepare process parameters\n",
    "                process_data = {}\n",
    "                for param in process_params:\n",
    "                    if param in prev_record:\n",
    "                        process_data[param] = prev_record[param]\n",
    "                    else:\n",
    "                        process_data[param] = np.nan\n",
    "\n",
    "                # Build result record\n",
    "                record = {\n",
    "                    'device_name': device_name,\n",
    "                    'spindle': f\"dw{spindle}\",\n",
    "                    'broken_start_time': current_time,\n",
    "                    'broken_duration_min': time_diff,\n",
    "\n",
    "                    # Before broken data\n",
    "                    'prev_time': prev_record['created_at'],\n",
    "                    'prev_status': status_mapping.get(prev_record[status_col], \"Unknown\"),\n",
    "                    'prev_speed': prev_record[speed_col],\n",
    "                    'prev_fullness': prev_record[fullness_col],\n",
    "\n",
    "                    # At broken time data\n",
    "                    'broken_status': status_mapping.get(broken_record[status_col], \"Unknown\"),\n",
    "                    'broken_speed': broken_record[speed_col],\n",
    "                    'broken_fullness': broken_record[fullness_col],\n",
    "\n",
    "                    # After broken data\n",
    "                    'next_time': next_record['created_at'],\n",
    "                    'next_status': status_mapping.get(next_record[status_col], \"Unknown\"),\n",
    "                    'next_speed': next_record[speed_col],\n",
    "                    'next_fullness': next_record[fullness_col],\n",
    "                }\n",
    "\n",
    "                # Add process parameters\n",
    "                record.update(process_data)\n",
    "\n",
    "                result_data.append(record)\n",
    "                processed_spindles.add(spindle)\n",
    "\n",
    "                # Take only one broken yarn record per spindle\n",
    "                break\n",
    "\n",
    "            # Collect at least 3 spindles per device\n",
    "            if len(processed_spindles) >= 3:\n",
    "                break\n",
    "\n",
    "    if not result_data:\n",
    "        print(\"No qualified broken yarn records found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 3. Convert to DataFrame and save\n",
    "    result_df = pd.DataFrame(result_data)\n",
    "\n",
    "    # Reorder columns\n",
    "    cols_order = [\n",
    "        'device_name', 'spindle', 'broken_start_time', 'broken_duration_min',\n",
    "        'prev_time', 'prev_status', 'prev_speed', 'prev_fullness',\n",
    "        'broken_status', 'broken_speed', 'broken_fullness',\n",
    "        'next_time', 'next_status', 'next_speed', 'next_fullness'\n",
    "    ] + process_params\n",
    "\n",
    "    result_df = result_df[cols_order]\n",
    "\n",
    "    # Save to CSV\n",
    "    result_df.to_csv(\"D:/code/junma/600000/0424/1.csv\", index=False)\n",
    "    print(\"\\nData saved to 1.csv\")\n",
    "\n",
    "    # Display sample results\n",
    "    print(\"\\nSample extracted data:\")\n",
    "    print(result_df.head())\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Example usage\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "result = find_broken_spindle_records(df)"
   ],
   "id": "ab65c9b56217b8be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#根据提取到的数据做特征分析\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 定义状态映射\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Running\",\n",
    "    2: \"Full\",\n",
    "    3: \"Broken\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Overspeed\",\n",
    "    7: \"Offline\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 定义工艺参数及其含义\n",
    "process_params = {\n",
    "    'R_dw21': '纱线品种',\n",
    "    'R_dw22': '锭速',\n",
    "    'R_dw23': '捻度',\n",
    "    'R_dw24': '超喂比',\n",
    "    'R_dw25': '内纱张力1',\n",
    "    'R_dw26': '外纱张力1',\n",
    "    'R_dw29': '防叠角度1',\n",
    "    'R_dw30': '防叠角度2',\n",
    "    'R_dw31': '防叠长度1',\n",
    "    'R_dw32': '防叠长度2',\n",
    "    'R_dw33': '锭长设定'\n",
    "}\n",
    "\n",
    "def analyze_broken_yarn_causes(df):\n",
    "    \"\"\"\n",
    "    分析断纱主要原因的函数\n",
    "\n",
    "    参数:\n",
    "    df: 包含断纱数据的DataFrame\n",
    "\n",
    "    返回:\n",
    "    特征重要性DataFrame和可视化图表\n",
    "    \"\"\"\n",
    "    # 1. 数据预处理\n",
    "    # 确保时间列是datetime类型\n",
    "    time_cols = ['broken_start_time', 'prev_time', 'next_time']\n",
    "    for col in time_cols:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "    # 计算断纱持续时间(分钟)\n",
    "    # df['broken_duration_min'] = (df['next_time'] - df['broken_start_time']).dt.total_seconds() / 60\n",
    "\n",
    "    # 2. 准备特征(X)和目标变量(Y)\n",
    "    # X变量: 工艺参数和运行状态\n",
    "    X_columns = list(process_params.keys()) + [\n",
    "        'prev_speed', 'prev_fullness',\n",
    "        'broken_speed', 'broken_fullness'\n",
    "    ]\n",
    "\n",
    "    # Y变量: 断纱持续时间(作为连续变量)或断纱严重程度(作为分类变量)\n",
    "    # 这里我们创建两个Y变量:\n",
    "\n",
    "    # Y1: 断纱持续时间(连续变量，用于回归分析)\n",
    "    df['broken_duration_min'] = df['broken_duration_min'].clip(upper=120)  # 设置上限为2小时\n",
    "\n",
    "    # Y2: 断纱严重程度(分类变量)\n",
    "    conditions = [\n",
    "        (df['broken_duration_min'] < 5),\n",
    "        (df['broken_duration_min'] >= 5) & (df['broken_duration_min'] < 30),\n",
    "        (df['broken_duration_min'] >= 30)\n",
    "    ]\n",
    "    choices = ['轻微', '中等', '严重']\n",
    "    df['severity'] = np.select(conditions, choices)\n",
    "\n",
    "    # 3. 特征工程\n",
    "    # 添加设备类型特征(从device_name中提取)\n",
    "    df['device_type'] = df['device_name'].str.extract(r'([A-Z]+)')[0]\n",
    "\n",
    "    # 添加工艺参数变化特征\n",
    "    for param in ['R_dw22', 'R_dw23', 'R_dw24', 'R_dw25', 'R_dw26']:\n",
    "        df[f'{param}_change'] = df[param].pct_change() * 100  # 百分比变化\n",
    "\n",
    "    # 4. 数据分析\n",
    "    # 按设备类型和严重程度分组统计\n",
    "    severity_stats = df.groupby(['device_type', 'severity']).size().unstack()\n",
    "\n",
    "    # 工艺参数与断纱持续时间的相关性\n",
    "    corr_matrix = df[list(process_params.keys()) + ['broken_duration_min']].corr()\n",
    "\n",
    "    # 5. 机器学习建模 - 预测断纱严重程度\n",
    "    # 准备数据\n",
    "    X = df[X_columns].fillna(0)\n",
    "    y = df['severity']\n",
    "\n",
    "    # 划分训练集和测试集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # 训练随机森林模型\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # 评估模型\n",
    "    y_pred = rf.predict(X_test)\n",
    "    print(\"\\n模型评估报告:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # 获取特征重要性\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_columns,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    # 6. 可视化分析\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # 特征重要性图\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance.head(15))\n",
    "    plt.title('Top 15 影响断纱的特征重要性')\n",
    "\n",
    "    # 断纱严重程度分布\n",
    "    plt.subplot(2, 2, 2)\n",
    "    severity_stats.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "    plt.title('不同设备类型的断纱严重程度分布')\n",
    "    plt.ylabel('计数')\n",
    "\n",
    "    # 工艺参数与断纱持续时间的相关性热图\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "    plt.title('工艺参数与断纱持续时间的相关性')\n",
    "\n",
    "    # 关键参数与断纱持续时间的关系\n",
    "    plt.subplot(2, 2, 4)\n",
    "    for param in ['R_dw22', 'R_dw23', 'R_dw25', 'R_dw26']:\n",
    "        sns.kdeplot(data=df, x=param, hue='severity', common_norm=False)\n",
    "    plt.title('关键工艺参数在不同断纱严重程度下的分布')\n",
    "    plt.legend(title='严重程度')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('broken_yarn_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "    return feature_importance, df\n",
    "\n",
    "# 示例使用\n",
    "if __name__ == \"__main__\":\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(\"D:/code/junma/600000/0424/1.csv\")\n",
    "\n",
    "    # 分析断纱原因\n",
    "    feature_importance, analyzed_df = analyze_broken_yarn_causes(df)\n",
    "\n",
    "    # 保存分析结果\n",
    "    analyzed_df.to_csv(\"broken_yarn_analysis_results.csv\", index=False)\n",
    "    feature_importance.to_csv(\"feature_importance.csv\", index=False)\n",
    "\n",
    "    print(\"\\n分析完成! 结果已保存到文件。\")\n",
    "    print(\"\\n最重要的断纱影响因素:\")\n",
    "    print(feature_importance.head(10))\n"
   ],
   "id": "e1788e3f4de13cb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#提取断纱前速度，卷装程度等相关数据，数据包括的是断纱状态以及断纱次数\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Spindle status mapping\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Running\",\n",
    "    2: \"Full\",\n",
    "    3: \"Broken\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Overspeed\",\n",
    "    7: \"Offline\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# Process parameter columns\n",
    "process_params = [\n",
    "    'R_dw21', 'R_dw22', 'R_dw23', 'R_dw24', 'R_dw25', 'R_dw26',\n",
    "    'R_dw29', 'R_dw30', 'R_dw31', 'R_dw32', 'R_dw33'\n",
    "]\n",
    "\n",
    "def find_broken_spindle_records(df):\n",
    "    # 1. Find top 10 devices with most broken yarn occurrences\n",
    "    device_broken_counts = defaultdict(int)\n",
    "    spindle_broken_counts = defaultdict(int)  # 新增：记录每个锭位的断纱次数\n",
    "\n",
    "    # Count broken yarn occurrences for each device and spindle\n",
    "    for i in range(1, 101):\n",
    "        status_col = f\"D_dw{i}\"\n",
    "        if status_col in df.columns:\n",
    "            broken_records = df[df[status_col] == 3]\n",
    "            for _, row in broken_records.iterrows():\n",
    "                device_broken_counts[row['subsystem']] += 1\n",
    "                spindle_broken_counts[(row['subsystem'], f\"dw{i}\")] += 1  # 记录锭位断纱次数\n",
    "\n",
    "    if not device_broken_counts:\n",
    "        print(\"No broken yarn records found in the data\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Get top 10 devices with most broken yarns\n",
    "    top_devices = sorted(device_broken_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    top_device_names = [device[0] for device in top_devices]\n",
    "    print(\"Top 10 devices with most broken yarn occurrences:\")\n",
    "    for device, count in top_devices:\n",
    "        print(f\"{device}: {count} times\")\n",
    "\n",
    "    # 2. Extract data around broken yarn events for each device\n",
    "    result_data = []\n",
    "\n",
    "    for device_name in top_device_names:\n",
    "        device_data = df[df['subsystem'] == device_name].copy()\n",
    "        device_data['created_at'] = pd.to_datetime(device_data['created_at'])\n",
    "        device_data = device_data.sort_values('created_at')\n",
    "\n",
    "        # Track processed spindles for this device\n",
    "        processed_spindles = set()\n",
    "\n",
    "        # Find broken yarn events for each spindle\n",
    "        for spindle in range(1, 101):\n",
    "            status_col = f\"D_dw{spindle}\"\n",
    "            speed_col = f\"V_dw{spindle}\"\n",
    "            fullness_col = f\"P_dw{spindle}\"\n",
    "\n",
    "            if status_col not in device_data.columns:\n",
    "                continue\n",
    "\n",
    "            # Find all broken yarn records\n",
    "            broken_records = device_data[device_data[status_col] == 3]\n",
    "            if len(broken_records) == 0:\n",
    "                continue\n",
    "\n",
    "            # Process each broken yarn event\n",
    "            for _, broken_record in broken_records.iterrows():\n",
    "                current_time = broken_record['created_at']\n",
    "                current_idx = broken_record.name\n",
    "\n",
    "                # Get previous record\n",
    "                prev_records = device_data[device_data['created_at'] < current_time]\n",
    "                if len(prev_records) == 0:\n",
    "                    continue\n",
    "\n",
    "                prev_record = prev_records.iloc[-1]\n",
    "\n",
    "                # Validate previous record (speed not 0, status not broken)\n",
    "                if (prev_record[speed_col] == 0 or\n",
    "                    prev_record[status_col] == 3 or\n",
    "                    prev_record[fullness_col] == 0):\n",
    "                    continue\n",
    "\n",
    "                # Get next record\n",
    "                next_records = device_data[device_data['created_at'] > current_time]\n",
    "                if len(next_records) == 0:\n",
    "                    continue\n",
    "\n",
    "                next_record = next_records.iloc[0]\n",
    "\n",
    "                # Validate next record (same device)\n",
    "                if next_record['subsystem'] != device_name:\n",
    "                    continue\n",
    "\n",
    "                # Get broken count for this spindle\n",
    "                broken_count = spindle_broken_counts.get((device_name, f\"dw{spindle}\"), 0)\n",
    "\n",
    "                # Prepare process parameters\n",
    "                process_data = {}\n",
    "                for param in process_params:\n",
    "                    if param in prev_record:\n",
    "                        process_data[param] = prev_record[param]\n",
    "                    else:\n",
    "                        process_data[param] = np.nan\n",
    "\n",
    "                # Build result record\n",
    "                record = {\n",
    "                    'device_name': device_name,\n",
    "                    'spindle': f\"dw{spindle}\",\n",
    "                    'broken_start_time': current_time,\n",
    "                    'broken_count': broken_count,  # 新增断纱次数参数\n",
    "\n",
    "                    # Before broken data\n",
    "                    'prev_time': prev_record['created_at'],\n",
    "                    'prev_status': status_mapping.get(prev_record[status_col], \"Unknown\"),\n",
    "                    'prev_speed': prev_record[speed_col],\n",
    "                    'prev_fullness': prev_record[fullness_col],\n",
    "\n",
    "                    # At broken time data\n",
    "                    'broken_status': status_mapping.get(broken_record[status_col], \"Unknown\"),\n",
    "                    'broken_speed': broken_record[speed_col],\n",
    "                    'broken_fullness': broken_record[fullness_col],\n",
    "\n",
    "                    # After broken data\n",
    "                    'next_time': next_record['created_at'],\n",
    "                    'next_status': status_mapping.get(next_record[status_col], \"Unknown\"),\n",
    "                    'next_speed': next_record[speed_col],\n",
    "                    'next_fullness': next_record[fullness_col],\n",
    "                }\n",
    "\n",
    "                # Add process parameters\n",
    "                record.update(process_data)\n",
    "\n",
    "                result_data.append(record)\n",
    "                processed_spindles.add(spindle)\n",
    "\n",
    "                # Take only one broken yarn record per spindle\n",
    "                break\n",
    "\n",
    "            # Collect at least 3 spindles per device\n",
    "            if len(processed_spindles) >= 3:\n",
    "                break\n",
    "\n",
    "    if not result_data:\n",
    "        print(\"No qualified broken yarn records found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 3. Convert to DataFrame and save\n",
    "    result_df = pd.DataFrame(result_data)\n",
    "\n",
    "    # Reorder columns\n",
    "    cols_order = [\n",
    "        'device_name', 'spindle', 'broken_start_time', 'broken_count',\n",
    "        'prev_time', 'prev_status', 'prev_speed', 'prev_fullness',\n",
    "        'broken_status', 'broken_speed', 'broken_fullness',\n",
    "        'next_time', 'next_status', 'next_speed', 'next_fullness'\n",
    "    ] + process_params\n",
    "\n",
    "    result_df = result_df[cols_order]\n",
    "\n",
    "    # Save to CSV\n",
    "    result_df.to_csv(\"D:/code/junma/600000/0424/3.csv\", index=False)\n",
    "    print(\"\\nData saved to 3.csv\")\n",
    "\n",
    "    # Display sample results\n",
    "    print(\"\\nSample extracted data:\")\n",
    "    print(result_df.head())\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Example usage\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "result = find_broken_spindle_records(df)"
   ],
   "id": "8b6b9e10375083fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#停锭英文分析\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def preprocess_data(df, target_device=\"NX16-L102\"):\n",
    "    \"\"\"数据预处理：计算每分钟产量变化率\"\"\"\n",
    "    # 筛选目标设备数据并确保M_dw4=0\n",
    "    df = df[(df['subsystem'] == target_device) & (df['M_dw4'] == 0)].copy()\n",
    "    if len(df) == 0:\n",
    "        print(f\"错误：没有找到设备 {target_device} 的有效数据(M_dw4=0)\")\n",
    "        return None\n",
    "\n",
    "    # 转换时间列并排序\n",
    "    df['time'] = pd.to_datetime(df['created_at'])\n",
    "    df = df.sort_values('time')\n",
    "\n",
    "    # 计算时间差（分钟）和产量变化率\n",
    "    df['time_diff'] = df['time'].diff().dt.total_seconds() / 60\n",
    "    df['production_rate'] = df['C_dw2'].diff() / df['time_diff']\n",
    "\n",
    "    # 计算停锭占比 (M_dw2表示停锭数量)\n",
    "    df['stop_ratio'] = df['M_dw2'] / 100  # 100个锭位\n",
    "\n",
    "    # 过滤有效数据：时间差在2-3分钟之间且产量变化合理\n",
    "    valid_data = df[(df['time_diff'] >= 2) & (df['time_diff'] <= 3) &\n",
    "                    (df['production_rate'].notna()) &\n",
    "                    (df['production_rate'].abs() <= 10) &  # 假设每分钟产量变化不超过10kg\n",
    "                    (df['stop_ratio'] <= 0.20)]  # 只考虑停锭占比<=20%的情况\n",
    "\n",
    "    # 检查是否有满管锭位（所有锭位状态都不为0）\n",
    "    spindle_cols = [f\"D_dw{i}\" for i in range(1, 101)]\n",
    "    valid_data['all_running'] = (valid_data[spindle_cols] != 0).all(axis=1)\n",
    "    valid_data = valid_data[~valid_data['all_running']]\n",
    "\n",
    "    if len(valid_data) == 0:\n",
    "        print(\"错误：没有有效的分析数据（时间间隔、满管条件或停锭占比不符合）\")\n",
    "        return None\n",
    "\n",
    "    return valid_data\n",
    "\n",
    "def analyze_stop_spindle_impact(df, target_device=\"NX16-L102\"):\n",
    "    \"\"\"分析停锭状态占比对产量变化率的影响\"\"\"\n",
    "    df = preprocess_data(df, target_device)\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    # 根据停锭占比分组（0-5%, 5-10%, 10-15%, 15-20%）\n",
    "    bins = [0, 0.05, 0.10, 0.15, 0.20]\n",
    "    labels = [\"0-5%\", \"5-10%\", \"10-15%\", \"15-20%\"]\n",
    "\n",
    "    df['status_group'] = pd.cut(df['stop_ratio'],\n",
    "                               bins=bins,\n",
    "                               labels=labels,\n",
    "                               right=False)\n",
    "\n",
    "    # 计算每组的生产率统计量\n",
    "    results = []\n",
    "    for ratio_range, group in df.groupby('status_group'):\n",
    "        if len(group) >= 3:  # 至少需要3个数据点\n",
    "            results.append({\n",
    "                'device': target_device,\n",
    "                'status_ratio_group': ratio_range,\n",
    "                'avg_production_rate': group['production_rate'].mean(),\n",
    "                'std_production_rate': group['production_rate'].std(),\n",
    "                'sample_count': len(group),\n",
    "                'min_stop_ratio': group['stop_ratio'].min(),\n",
    "                'max_stop_ratio': group['stop_ratio'].max(),\n",
    "                'avg_stop_count': group['M_dw2'].mean()  # 平均停锭数量\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def visualize_impact(impact_df, device_name):\n",
    "    \"\"\"可视化停锭状态占比影响\"\"\"\n",
    "    if impact_df is None or len(impact_df) == 0:\n",
    "        print(\"没有可用的影响分析数据\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # 确保按停锭占比从小到大排序\n",
    "    impact_df = impact_df.sort_values('status_ratio_group', key=lambda x: x.str.extract('(\\d+)', expand=False).astype(float))\n",
    "\n",
    "    # 获取排序后的分组标签\n",
    "    ordered_groups = impact_df['status_ratio_group'].unique()\n",
    "\n",
    "    # 绘制柱状图\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(impact_df)))\n",
    "    bars = plt.bar(\n",
    "        range(len(ordered_groups)),  # 使用数字位置而不是直接使用标签\n",
    "        impact_df['avg_production_rate'],\n",
    "        color=colors,\n",
    "        yerr=impact_df['std_production_rate'],\n",
    "        capsize=8,\n",
    "        width=0.7,\n",
    "        edgecolor='black'\n",
    "    )\n",
    "\n",
    "    # 设置X轴标签为排序后的分组\n",
    "    plt.xticks(range(len(ordered_groups)), ordered_groups)\n",
    "\n",
    "    # 添加数值标签\n",
    "    for i, (bar, avg, std, n) in enumerate(zip(bars,\n",
    "                                             impact_df['avg_production_rate'],\n",
    "                                             impact_df['std_production_rate'],\n",
    "                                             impact_df['sample_count'])):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f\"{avg:.2f}±{std:.2f}\",\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "        # 在柱状图底部添加样本数和平均停锭数\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., 0,\n",
    "                f\"样本:{n}\\n停锭:{impact_df.iloc[i]['avg_stop_count']:.1f}\",\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.title(f\"设备 {device_name} 停锭状态占比与每分钟产量变化率的关系\\n(分析条件: M_dw4=0, 时间间隔2-3分钟, 停锭占比≤20%)\",\n",
    "             fontsize=12, pad=20)\n",
    "    plt.xlabel(\"停锭锭位占比分组\", fontsize=11)\n",
    "    plt.ylabel(\"平均产量变化率 (kg/min)\", fontsize=11)\n",
    "    plt.axhline(0, color='gray', linestyle='--')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # 添加趋势线\n",
    "    if len(impact_df) > 1:\n",
    "        x = np.arange(len(impact_df))\n",
    "        y = impact_df['avg_production_rate']\n",
    "        z = np.polyfit(x, y, 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(x, p(x), \"r--\",\n",
    "                linewidth=2, label=f\"趋势线 (斜率={z[0]:.2f})\")\n",
    "        plt.legend(fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载数据\n",
    "    try:\n",
    "        df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"加载数据失败: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # 检查必要列是否存在\n",
    "    required_cols = ['subsystem', 'created_at', 'C_dw2', 'M_dw2', 'M_dw4'] + [f'D_dw{i}' for i in range(1, 101)]\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"错误：数据中缺少必要的列: {missing_cols}\")\n",
    "        exit()\n",
    "\n",
    "    # 以NX16-L102为例进行分析\n",
    "    target_device = \"NX16-L102\"\n",
    "    print(f\"\\n开始分析设备: {target_device}\")\n",
    "\n",
    "    # 分析停锭状态影响\n",
    "    impact_df = analyze_stop_spindle_impact(df, target_device)\n",
    "\n",
    "    if impact_df is not None:\n",
    "        print(f\"\\n设备 {target_device} 生产效能与停锭状态关系分析结果:\")\n",
    "        print(impact_df[['status_ratio_group', 'avg_production_rate',\n",
    "                        'std_production_rate', 'sample_count', 'avg_stop_count']])\n",
    "\n",
    "        # 可视化结果\n",
    "        visualize_impact(impact_df, target_device)\n",
    "    else:\n",
    "        print(f\"无法分析设备 {target_device} 的数据\")"
   ],
   "id": "8812e96840af62b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#最清晰\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set global style parameters for paper-ready figures\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',  # Make all text bold\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.edgecolor': 'black',\n",
    "    'axes.linewidth': 4,  # Thicker axes lines\n",
    "    'xtick.color': 'black',\n",
    "    'ytick.color': 'black',\n",
    "    'xtick.major.width': 4,\n",
    "    'ytick.major.width': 4,\n",
    "    'xtick.major.size': 16,\n",
    "    'ytick.major.size': 16,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'savefig.facecolor': 'white',\n",
    "    'figure.dpi': 300,  # Higher resolution\n",
    "    'axes.labelsize': 24,  # Larger base size\n",
    "    'axes.titlesize': 28,\n",
    "    'xtick.labelsize': 26,\n",
    "    'ytick.labelsize': 26,\n",
    "    'legend.fontsize': 20,\n",
    "    'legend.title_fontsize': 22\n",
    "})\n",
    "\n",
    "def preprocess_data(df, target_device=\"NX16-L102\"):\n",
    "    \"\"\"Data preprocessing: Calculate production rate change per minute\"\"\"\n",
    "    # Filter target device data and ensure M_dw4=0\n",
    "    df = df[(df['subsystem'] == target_device) & (df['M_dw4'] == 0)].copy()\n",
    "    if len(df) == 0:\n",
    "        print(f\"Error: No valid data found for device {target_device} (M_dw4=0)\")\n",
    "        return None\n",
    "\n",
    "    # Convert time column and sort\n",
    "    df['time'] = pd.to_datetime(df['created_at'])\n",
    "    df = df.sort_values('time')\n",
    "\n",
    "    # Calculate time difference (minutes) and production rate change\n",
    "    df['time_diff'] = df['time'].diff().dt.total_seconds() / 60\n",
    "    df['production_rate'] = df['C_dw2'].diff() / df['time_diff']\n",
    "\n",
    "    # Calculate stop ratio (M_dw2 represents stopped spindles)\n",
    "    df['stop_ratio'] = df['M_dw2'] / 100  # 100 spindles total\n",
    "\n",
    "    # Filter valid data: time difference between 2-3 minutes and reasonable production rate\n",
    "    valid_data = df[(df['time_diff'] >= 2) & (df['time_diff'] <= 3) &\n",
    "                    (df['production_rate'].notna()) &\n",
    "                    (df['production_rate'].abs() <= 10) &  # Assume max production rate change is 10kg/min\n",
    "                    (df['stop_ratio'] <= 0.20)]  # Only consider stop ratio <=20%\n",
    "\n",
    "    # Check for full spindles (all spindle statuses not 0)\n",
    "    spindle_cols = [f\"D_dw{i}\" for i in range(1, 101)]\n",
    "    valid_data['all_running'] = (valid_data[spindle_cols] != 0).all(axis=1)\n",
    "    valid_data = valid_data[~valid_data['all_running']]\n",
    "\n",
    "    if len(valid_data) == 0:\n",
    "        print(\"Error: No valid data for analysis (time interval, full spindle condition or stop ratio not met)\")\n",
    "        return None\n",
    "\n",
    "    return valid_data\n",
    "\n",
    "def analyze_stop_spindle_impact(df, target_device=\"NX16-L102\"):\n",
    "    \"\"\"Analyze the impact of stopped spindle ratio on production rate change\"\"\"\n",
    "    df = preprocess_data(df, target_device)\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    # Group by stop ratio (0-5%, 5-10%, 10-15%, 15-20%)\n",
    "    bins = [0, 0.05, 0.10, 0.15, 0.20]\n",
    "    labels = [\"0-5%\", \"5-10%\", \"10-15%\", \"15-20%\"]\n",
    "\n",
    "    df['status_group'] = pd.cut(df['stop_ratio'],\n",
    "                               bins=bins,\n",
    "                               labels=labels,\n",
    "                               right=False)\n",
    "\n",
    "    # Calculate production rate statistics for each group\n",
    "    results = []\n",
    "    for ratio_range, group in df.groupby('status_group'):\n",
    "        if len(group) >= 3:  # Need at least 3 data points\n",
    "            results.append({\n",
    "                'device': target_device,\n",
    "                'status_ratio_group': ratio_range,\n",
    "                'avg_production_rate': group['production_rate'].mean(),\n",
    "                'std_production_rate': group['production_rate'].std(),\n",
    "                'sample_count': len(group),\n",
    "                'min_stop_ratio': group['stop_ratio'].min(),\n",
    "                'max_stop_ratio': group['stop_ratio'].max(),\n",
    "                'avg_stop_count': group['M_dw2'].mean()  # Average stopped spindles\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def visualize_impact(impact_df, device_name):\n",
    "    \"\"\"Visualize the impact of stopped spindle ratio with paper-ready formatting\"\"\"\n",
    "    if impact_df is None or len(impact_df) == 0:\n",
    "        print(\"No valid impact analysis data available\")\n",
    "        return\n",
    "\n",
    "    # Create figure with white background\n",
    "    fig, ax = plt.subplots(figsize=(16, 10), dpi=1200)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    # Ensure sorting by stop ratio from small to large\n",
    "    impact_df = impact_df.sort_values('status_ratio_group', key=lambda x: x.str.extract('(\\d+)', expand=False).astype(float))\n",
    "\n",
    "    # Get sorted group labels\n",
    "    ordered_groups = impact_df['status_ratio_group'].unique()\n",
    "\n",
    "    # Plot bar chart with enhanced visibility\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(impact_df)))\n",
    "    bars = ax.bar(\n",
    "        range(len(ordered_groups)),\n",
    "        impact_df['avg_production_rate'],\n",
    "        color=colors,\n",
    "        yerr=impact_df['std_production_rate'],\n",
    "        capsize=20,  # Larger error bar caps\n",
    "        width=0.7,\n",
    "        edgecolor='black',\n",
    "        linewidth=4  # Thicker bar edges\n",
    "    )\n",
    "\n",
    "    # Customize axes and labels\n",
    "    ax.set_xticks(range(len(ordered_groups)))\n",
    "    ax.set_xticklabels(ordered_groups, fontsize=30, fontweight='bold')  # Larger and bolder x-axis labels\n",
    "    ax.tick_params(axis='both', which='major', width=4, size=12, labelsize=30)\n",
    "\n",
    "    # Add value labels with increased size\n",
    "    for i, (bar, avg, std, n) in enumerate(zip(bars,\n",
    "                                             impact_df['avg_production_rate'],\n",
    "                                             impact_df['std_production_rate'],\n",
    "                                             impact_df['sample_count'])):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f\"{avg:.2f}±{std:.2f}\",\n",
    "               ha='center', va='bottom',\n",
    "               fontsize=26, fontweight='bold', color='black')\n",
    "\n",
    "        # # Add sample count and average stopped spindles at bottom\n",
    "        # ax.text(bar.get_x() + bar.get_width()/2., ax.get_ylim()[0],\n",
    "        #        f\"Samples: {n}\\nStops: {impact_df.iloc[i]['avg_stop_count']:.1f}\",\n",
    "        #        ha='center', va='bottom',\n",
    "        #        fontsize=26, fontweight='bold', color='black')\n",
    "\n",
    "    # Title and labels with increased size\n",
    "    # ax.set_title(f\"Device {device_name}: Stopped Spindle Ratio vs Production Rate Change\\n(Analysis Conditions:Time Interval 2-3min, Stop Ratio≤20%)\",\n",
    "    #             fontsize=28, pad=28, fontweight='bold')\n",
    "    ax.set_xlabel(\"Stopped Spindle Ratio Group\", fontsize=30, fontweight='bold', labelpad=16)\n",
    "    ax.set_ylabel(\"Average Production Rate Change (kg/min)\", fontsize=30, fontweight='bold', labelpad=16)\n",
    "\n",
    "    # Reference line and grid\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=4)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7, linewidth=3)\n",
    "\n",
    "    # Add trend line with enhanced visibility\n",
    "    if len(impact_df) > 1:\n",
    "        x = np.arange(len(impact_df))\n",
    "        y = impact_df['avg_production_rate']\n",
    "        z = np.polyfit(x, y, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax.plot(x, p(x), \"r--\",\n",
    "               linewidth=5, label=f\"Trend Line (Slope={z[0]:.2f})\")\n",
    "        ax.legend(fontsize=26, framealpha=1, edgecolor='black')\n",
    "\n",
    "    # Make spines (axes lines) thicker\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    try:\n",
    "        df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Data loading failed: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Check required columns\n",
    "    required_cols = ['subsystem', 'created_at', 'C_dw2', 'M_dw2', 'M_dw4'] + [f'D_dw{i}' for i in range(1, 101)]\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Error: Missing required columns in data: {missing_cols}\")\n",
    "        exit()\n",
    "\n",
    "    # Analyze NX16-L102 as example\n",
    "    target_device = \"NX16-L102\"\n",
    "    print(f\"\\nAnalyzing device: {target_device}\")\n",
    "\n",
    "    # Analyze stopped spindle impact\n",
    "    impact_df = analyze_stop_spindle_impact(df, target_device)\n",
    "\n",
    "    if impact_df is not None:\n",
    "        print(f\"\\nDevice {target_device} production efficiency vs stopped spindle status analysis results:\")\n",
    "        print(impact_df[['status_ratio_group', 'avg_production_rate',\n",
    "                        'std_production_rate', 'sample_count', 'avg_stop_count']])\n",
    "\n",
    "        # Visualize results with paper-ready formatting\n",
    "        visualize_impact(impact_df, target_device)\n",
    "    else:\n",
    "        print(f\"Unable to analyze data for device {target_device}\")"
   ],
   "id": "379ed6d7337e05c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# 设置全局样式 - 增强可读性\n",
    "plt.style.use('seaborn-v0_8')  # 使用 seaborn 样式\n",
    "plt.rcParams.update({\n",
    "    'font.sans-serif': 'Arial',        # 使用更清晰的Arial字体\n",
    "    'axes.unicode_minus': False,       # 正确显示负号\n",
    "    'axes.titlesize': 18,              # 增大标题字号\n",
    "    'axes.titleweight': 'bold',        # 标题加粗\n",
    "    'axes.labelsize': 16,              # 增大轴标签字号\n",
    "    'xtick.labelsize': 14,             # 增大X轴刻度字号\n",
    "    'ytick.labelsize': 14,             # 增大Y轴刻度字号\n",
    "    'legend.fontsize': 12,             # 增大图例字号\n",
    "    'figure.titlesize': 20,            # 图形总标题字号\n",
    "    'figure.titleweight': 'bold',      # 图形总标题加粗\n",
    "    'axes.labelweight': 'bold',        # 轴标签加粗\n",
    "    'font.weight': 'bold',             # 全局字体加粗\n",
    "    'grid.alpha': 0.3                  # 网格透明度\n",
    "})\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\", parse_dates=['created_at'])\n",
    "\n",
    "# 锭位状态映射\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Started\",\n",
    "    2: \"Full\",\n",
    "    3: \"Broken\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Lost Speed\",\n",
    "    7: \"Disconnected\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 准备锭位状态、速度和卷装程度的列名\n",
    "spindle_cols = [(f\"D_dw{i}\", f\"V_dw{i}\", f\"P_dw{i}\") for i in range(1, 101)\n",
    "                if f\"D_dw{i}\" in df.columns and f\"V_dw{i}\" in df.columns and f\"P_dw{i}\" in df.columns]\n",
    "\n",
    "# 找出断纱最多的前十台设备\n",
    "device_broken_counts = defaultdict(int)\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    broken_by_device = df[df[status_col] == 3].groupby('subsystem').size()\n",
    "    for device, count in broken_by_device.items():\n",
    "        device_broken_counts[device] += count\n",
    "\n",
    "# 转换为DataFrame并排序\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Count', ascending=False).head(10)\n",
    "\n",
    "    # 分析断纱前后速度变化 - 改进后的图表\n",
    "    plt.figure(figsize=(16, 9), facecolor='white', dpi=120)  # 增大尺寸和DPI\n",
    "    ax = plt.gca()\n",
    "    all_broken_records = []\n",
    "\n",
    "    for device_name in top_devices.index:\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        for status_col, speed_col, _ in spindle_cols:\n",
    "            broken_indices = device_data[device_data[status_col] == 3].index\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # 获取断纱事件的开始和结束\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "            if current_start is None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # 分析每个断纱事件\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                prev_idx = device_data.index.get_loc(start_idx) - 1\n",
    "                while prev_idx >= 0 and device_data.iloc[prev_idx][status_col] == 3:\n",
    "                    prev_idx -= 1\n",
    "                next_idx = device_data.index.get_loc(end_idx) + 1\n",
    "                while next_idx < len(device_data) and device_data.iloc[next_idx][status_col] == 3:\n",
    "                    next_idx += 1\n",
    "\n",
    "                if prev_idx >= 0 and next_idx < len(device_data):\n",
    "                    target_speed_col = f\"R_dw22\"\n",
    "                    target_speed = device_data.iloc[prev_idx][target_speed_col] if target_speed_col in device_data.columns else None\n",
    "\n",
    "                    try:\n",
    "                        speed_before = float(device_data.iloc[prev_idx][speed_col])\n",
    "                        speed_during = float(device_data.loc[start_idx, speed_col])\n",
    "                        speed_after = float(device_data.iloc[next_idx][speed_col])\n",
    "\n",
    "                        duration = (device_data.loc[end_idx, 'created_at'] - device_data.loc[start_idx, 'created_at']).total_seconds() / 60\n",
    "\n",
    "                        record = {\n",
    "                            'Device': device_name,\n",
    "                            'Spindle': status_col.split('_')[1],\n",
    "                            'Break Start Time': device_data.loc[start_idx, 'created_at'],\n",
    "                            'Break End Time': device_data.loc[end_idx, 'created_at'],\n",
    "                            'The speed 2 minutes before the yarn breakage': speed_before,\n",
    "                            'The speed during the yarn breakage': speed_during,\n",
    "                            'The speed 2 minutes after the yarn breakage': speed_after,\n",
    "                            'Target Speed': float(target_speed) if pd.notna(target_speed) else None,\n",
    "                            'Break Duration': duration\n",
    "                        }\n",
    "                        device_records.append(record)\n",
    "                        all_broken_records.append(record)\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "\n",
    "        # 绘制典型断纱事件（最多5个）\n",
    "        if device_records:\n",
    "            sample_records = pd.DataFrame(device_records).sample(min(5, len(device_records)), random_state=42)\n",
    "            for _, record in sample_records.iterrows():\n",
    "                line = plt.plot(['Before Break', 'During Break', 'After Break'],\n",
    "                              [record['The speed 2 minutes before the yarn breakage'], record['The speed during the yarn breakage'], record['The speed 2 minutes after the yarn breakage']],\n",
    "                              marker='o', linewidth=3, markersize=10, alpha=0.8,\n",
    "                              label=f\"{record['Device']}-{record['Spindle']} @ {record['Break Start Time'].strftime('%H:%M')}\")\n",
    "\n",
    "                if pd.notna(record['Target Speed']):\n",
    "                    plt.axhline(y=record['Target Speed'], color='gray', linestyle='--', alpha=0.5, linewidth=2)\n",
    "\n",
    "    # 设置图表样式 - 增强版\n",
    "    plt.title('Speed Changes Around Broken Spindle Events\\n(Top 10 Devices with Most Breaks)',\n",
    "              fontsize=18, pad=20)\n",
    "    plt.xlabel('Event Phase', fontsize=16, labelpad=10)\n",
    "    plt.ylabel('Spindle Speed (rpm)', fontsize=16, labelpad=10)\n",
    "\n",
    "    # 优化图例和网格\n",
    "    legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left',\n",
    "                        title='Device-Spindle @ Time', title_fontsize=13,\n",
    "                        framealpha=1, edgecolor='#333333')\n",
    "    plt.setp(legend.get_title(), fontweight='bold')\n",
    "\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存高质量图像\n",
    "    plt.savefig('speed_changes_high_quality.png', dpi=1200, bbox_inches='tight', pad_inches=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    # 绘制速度分布箱线图 - 改进版（根据要求优化）\n",
    "    speed_df = pd.DataFrame(all_broken_records)[['The speed 2 minutes before the yarn breakage', 'The speed during the yarn breakage', 'The speed 2 minutes after the yarn breakage']]\n",
    "\n",
    "    # 绘制速度分布图 - 终极优化版\n",
    "    plt.figure(figsize=(14, 8), facecolor='white', dpi=150)  # 进一步增大尺寸和DPI\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # 完全去除边框（根据要求）\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(3)  # 左侧Y轴加粗\n",
    "    ax.spines['bottom'].set_linewidth(3)  # 底部X轴加粗\n",
    "    ax.spines['left'].set_color('#333333')\n",
    "    ax.spines['bottom'].set_color('#333333')\n",
    "\n",
    "    # 绘制断纱前2分钟和断纱后2分钟的速度箱线图\n",
    "    boxprops = dict(linewidth=4, color='#1F77B4')  # 线条加粗到4pt\n",
    "    whiskerprops = dict(linewidth=4, color='#1F77B4')  # 须线同样加粗\n",
    "    medianprops = dict(linewidth=5, color='#FF2A00')  # 中位数线加粗到5pt，使用醒目的红色\n",
    "    flierprops = dict(marker='o', markersize=10,  # 增大离群点圆圈尺寸\n",
    "                      markerfacecolor='#FF7F0E', markeredgecolor='#333333',\n",
    "                      markeredgewidth=1.5, alpha=0.8)\n",
    "\n",
    "    # 绘制箱线图\n",
    "    bp = speed_df[['The speed 2 minutes before the yarn breakage', 'The speed 2 minutes after the yarn breakage']].boxplot(\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor='#1F77B4', alpha=0.95, **boxprops),  # 更明亮的蓝色\n",
    "        whiskerprops=whiskerprops,\n",
    "        medianprops=medianprops,\n",
    "        flierprops=flierprops,\n",
    "        widths=0.7,  # 进一步加宽箱体\n",
    "        grid=False  # 我们后面会自定义网格\n",
    "    )\n",
    "\n",
    "    # 断纱期间的速度以原点分布\n",
    "    for i, (_, row) in enumerate(speed_df.iterrows()):\n",
    "        speed_before = row['The speed 2 minutes before the yarn breakage']\n",
    "        speed_during = row['The speed during the yarn breakage']\n",
    "        speed_after = row['The speed 2 minutes after the yarn breakage']\n",
    "\n",
    "        # 计算偏差\n",
    "        deviation = speed_before - speed_during\n",
    "\n",
    "        # 标识偏差\n",
    "        if abs(deviation) < 1000:  # 无显著偏差\n",
    "            plt.plot(1, speed_during, 'go', markersize=12, alpha=0.8, label='No Significant Deviation' if i == 0 else \"\")\n",
    "        else:  # 显著偏差\n",
    "            plt.plot(1, speed_during, 'ro', markersize=12, alpha=0.8, label='Significant Deviation' if i == 0 else \"\")\n",
    "\n",
    "    # 设置标题和标签 - 更加加粗\n",
    "    plt.title('Spindle Speed Distribution Around Break Events (Top 6 Devices)',\n",
    "              fontsize=24, pad=25, fontweight='heavy', color='#333333')  # 使用heavy加粗\n",
    "    plt.xlabel('Event Phase', fontsize=22, labelpad=15, fontweight='bold', color='#333333')\n",
    "    plt.ylabel('Speed (rpm)', fontsize=22, labelpad=15, fontweight='bold', color='#333333')\n",
    "\n",
    "    # 设置刻度标签 - 更加加粗增大\n",
    "    plt.xticks([0, 1, 2], [\n",
    "        'Pre - 2min Speed',\n",
    "        'Breakage speed',\n",
    "        'Post - 2min Speed'\n",
    "    ], fontsize=20, fontweight='bold', color='#333333')\n",
    "    plt.yticks(fontsize=20, fontweight='bold', color='#333333')\n",
    "\n",
    "    # 自定义网格线 - 更明显但不喧宾夺主\n",
    "    ax.grid(True, axis='y', linestyle='--', linewidth=2, alpha=0.4, color='#666666')\n",
    "\n",
    "    # 添加渐变色背景\n",
    "    ax.set_facecolor('#F5F5F5')\n",
    "    ax.set_axisbelow(True)  # 将网格线放在箱线图下方\n",
    "\n",
    "    # 添加图例\n",
    "    plt.legend(fontsize=16, loc='upper right', framealpha=0.9)\n",
    "\n",
    "    # 调整布局\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存超高质量图像\n",
    "    plt.savefig('speed_distribution_ultimate.png',\n",
    "                dpi=1200,\n",
    "                bbox_inches='tight',\n",
    "                facecolor='white',\n",
    "                transparent=False)\n",
    "\n",
    "    plt.show()"
   ],
   "id": "62b98ac4fd13d241",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# 设置全局样式\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams.update({\n",
    "    'font.sans-serif': 'Arial',\n",
    "    'axes.unicode_minus': False,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16,\n",
    "    'figure.titleweight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'font.weight': 'bold',\n",
    "    'grid.alpha': 0.2\n",
    "})\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\", parse_dates=['created_at'])\n",
    "\n",
    "# 锭位状态映射\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Started\",\n",
    "    2: \"Full\",\n",
    "    3: \"Broken\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Lost Speed\",\n",
    "    7: \"Disconnected\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 准备锭位状态、速度和卷装程度的列名\n",
    "spindle_cols = [(f\"D_dw{i}\", f\"V_dw{i}\", f\"P_dw{i}\") for i in range(1, 101)\n",
    "                if f\"D_dw{i}\" in df.columns and f\"V_dw{i}\" in df.columns and f\"P_dw{i}\" in df.columns]\n",
    "\n",
    "# 找出断纱最多的前十台设备\n",
    "device_broken_counts = defaultdict(int)\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    broken_by_device = df[df[status_col] == 3].groupby('subsystem').size()\n",
    "    for device, count in broken_by_device.items():\n",
    "        device_broken_counts[device] += count\n",
    "\n",
    "# 转换为DataFrame并排序\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Count', ascending=False).head(10)\n",
    "\n",
    "    # 分析断纱事件\n",
    "    all_broken_records = []\n",
    "    for device_name in top_devices.index:\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        for status_col, speed_col, _ in spindle_cols:\n",
    "            broken_indices = device_data[device_data[status_col] == 3].index\n",
    "            if len(broken_indices) ==0:\n",
    "                continue\n",
    "\n",
    "            # 获取断纱事件的开始和结束\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "            if current_start is None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # 分析每个断纱事件\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                try:\n",
    "                    speed_during = float(device_data.loc[start_idx, speed_col])\n",
    "                    record = {\n",
    "                        'Device': device_name,\n",
    "                        'Spindle': status_col.split('_')[1],\n",
    "                        'Break Start Time': device_data.loc[start_idx, 'created_at'],\n",
    "                        'Break End Time': device_data.loc[end_idx, 'created_at'],\n",
    "                        'The speed during the yarn breakage': speed_during\n",
    "                    }\n",
    "                    device_records.append(record)\n",
    "                    all_broken_records.append(record)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "\n",
    "    # 绘制断纱速度的散点图和 R_dw22 的箱线图\n",
    "    plt.figure(figsize=(8, 6), facecolor='white', dpi=600)  # 调整为更小的画布\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # 去除顶部和右侧边框\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(2)  # 左侧Y轴加粗\n",
    "    ax.spines['bottom'].set_linewidth(2)  # 底部X轴加粗\n",
    "    ax.spines['left'].set_color('#333333')\n",
    "    ax.spines['bottom'].set_color('#333333')\n",
    "\n",
    "    # 设置背景颜色\n",
    "    ax.set_facecolor('#FFFFFF')  # 纯白背景\n",
    "\n",
    "    # 绘制散点图（实际速度）\n",
    "    speed_df = pd.DataFrame(all_broken_records)[['The speed during the yarn breakage']]\n",
    "    for i, (_, row) in enumerate(speed_df.iterrows()):\n",
    "        speed_during = row['The speed during the yarn breakage']\n",
    "        plt.plot(1, speed_during, 'o', markersize=10, alpha=0.7, color='#D62728', label='Actual Speed' if i == 0 else '')\n",
    "\n",
    "    # 绘制 R_dw22 的箱线图（设定速度）\n",
    "    r_dw22_df = df[df['R_dw22'].notna()]['R_dw22']\n",
    "    boxprops = dict(linewidth=3, color='#1F77B4')  # 线条加粗\n",
    "    whiskerprops = dict(linewidth=3, color='#1F77B4')  # 须线加粗\n",
    "    medianprops = dict(linewidth=4, color='red')  # 中位数线加粗\n",
    "    flierprops = dict(marker='o', markersize=8,  # 离群点圆圈尺寸\n",
    "                      markerfacecolor='#FF7F0E', markeredgecolor='#333333',\n",
    "                      markeredgewidth=1, alpha=0.8)\n",
    "\n",
    "    # 绘制箱线图\n",
    "    bp = ax.boxplot(\n",
    "        r_dw22_df,\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor='#1F77B4', alpha=0.9, **boxprops),  # 更明亮的蓝色\n",
    "        whiskerprops=whiskerprops,\n",
    "        medianprops=medianprops,\n",
    "        flierprops=flierprops,\n",
    "        widths=0.3,  # 箱线图宽度\n",
    "        positions=[1]  # 将箱线图固定在Breakage Speed位置\n",
    "    )\n",
    "\n",
    "    # 设置标题和标签\n",
    "    plt.title('Spindle Speed Distribution During Breakage Events', fontsize=14, pad=10, fontweight='bold', color='#333333')\n",
    "    plt.xlabel('Breakage Phase', fontsize=12, labelpad=8, fontweight='bold', color='#333333')\n",
    "    plt.ylabel('Speed (rpm)', fontsize=12, labelpad=8, fontweight='bold', color='#333333')\n",
    "\n",
    "    # 设置刻度标签\n",
    "    plt.xticks([1], ['Breakage Speed'], fontsize=10, fontweight='bold', color='#333333')\n",
    "    plt.yticks(fontsize=10, fontweight='bold', color='#333333')\n",
    "\n",
    "    # 自定义网格线\n",
    "    ax.grid(True, axis='y', linestyle='--', linewidth=1, alpha=0.2, color='#666666')\n",
    "\n",
    "    # 添加图例（实际速度）\n",
    "    actual_speed_legend = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#D62728', markersize=10, label='Actual Speed')\n",
    "    # 添加图例（设定速度）\n",
    "    target_speed_legend = plt.Line2D([0], [0], color='#1F77B4', linewidth=6, label='Target Speed')\n",
    "\n",
    "    # 将图例放置在图片内部右上角\n",
    "    plt.legend(handles=[actual_speed_legend, target_speed_legend], loc='upper right', fontsize=10, framealpha=0.9, bbox_to_anchor=(1.01, 1))\n",
    "\n",
    "    # 调整布局\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存高质量图像\n",
    "    plt.savefig('breakage_speed_distribution_optimized.png', dpi=600, bbox_inches='tight', facecolor='white', transparent=False)\n",
    "\n",
    "    plt.show()"
   ],
   "id": "1f227dbe561b993e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# 设置全局样式\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams.update({\n",
    "    'font.sans-serif': 'Arial',\n",
    "    'axes.unicode_minus': False,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16,\n",
    "    'figure.titleweight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'font.weight': 'bold',\n",
    "    'grid.alpha': 0.2\n",
    "})\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\", parse_dates=['created_at'])\n",
    "\n",
    "# 锭位状态映射\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Started\",\n",
    "    2: \"Full\",\n",
    "    3: \"Broken\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Lost Speed\",\n",
    "    7: \"Disconnected\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 准备锭位状态、速度和卷装程度的列名\n",
    "spindle_cols = [(f\"D_dw{i}\", f\"V_dw{i}\", f\"P_dw{i}\") for i in range(1, 101)\n",
    "                if f\"D_dw{i}\" in df.columns and f\"V_dw{i}\" in df.columns and f\"P_dw{i}\" in df.columns]\n",
    "\n",
    "# 找出断纱最多的前十台设备\n",
    "device_broken_counts = defaultdict(int)\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    broken_by_device = df[df[status_col] == 3].groupby('subsystem').size()\n",
    "    for device, count in broken_by_device.items():\n",
    "        device_broken_counts[device] += count\n",
    "\n",
    "# 转换为DataFrame并排序\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Count', ascending=False).head(10)\n",
    "\n",
    "    # 分析断纱事件\n",
    "    all_broken_records = []\n",
    "    for device_name in top_devices.index:\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        for status_col, speed_col, _ in spindle_cols:\n",
    "            broken_indices = device_data[device_data[status_col] == 3].index\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # 获取断纱事件的开始和结束\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "            if current_start is None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # 分析每个断纱事件\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                try:\n",
    "                    speed_during = float(device_data.loc[start_idx, speed_col])\n",
    "                    record = {\n",
    "                        'Device': device_name,\n",
    "                        'Spindle': status_col.split('_')[1],\n",
    "                        'Break Start Time': device_data.loc[start_idx, 'created_at'],\n",
    "                        'Break End Time': device_data.loc[end_idx, 'created_at'],\n",
    "                        'The speed during the yarn breakage': speed_during\n",
    "                    }\n",
    "                    device_records.append(record)\n",
    "                    all_broken_records.append(record)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "\n",
    "    # 绘制断纱速度的散点图和 R_dw22 的箱线图\n",
    "    plt.figure(figsize=(6, 6), facecolor='#F0F0F0', dpi=600)  # 正方形画布，全局背景为灰色\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # 将图片内全部填充为灰色\n",
    "    ax.set_facecolor('#F0F0F0')\n",
    "\n",
    "    # 去除顶部和右侧边框\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # 加粗 X 轴和 Y 轴的数据分割线外面的小轴，类似指示线\n",
    "    ax.spines['left'].set_linewidth(3)  # 左侧 Y 轴加粗\n",
    "    ax.spines['bottom'].set_linewidth(3)  # 底部 X 轴加粗\n",
    "    ax.spines['left'].set_color('#333333')\n",
    "    ax.spines['bottom'].set_color('#333333')\n",
    "\n",
    "    # 绘制散点图（实际速度）\n",
    "    speed_df = pd.DataFrame(all_broken_records)[['The speed during the yarn breakage']]\n",
    "    for i, (_, row) in enumerate(speed_df.iterrows()):\n",
    "        speed_during = row['The speed during the yarn breakage']\n",
    "        plt.plot(1, speed_during, 'o', markersize=10, alpha=0.7, color='#D62728', label='Actual Speed' if i == 0 else '')\n",
    "\n",
    "    # 绘制 R_dw22 的箱线图（设定速度）\n",
    "    r_dw22_df = df[df['R_dw22'].notna()]['R_dw22']\n",
    "    boxprops = dict(linewidth=3, color='#1F77B4')  # 线条加粗\n",
    "    whiskerprops = dict(linewidth=3, color='#1F77B4')  # 须线加粗\n",
    "    medianprops = dict(linewidth=4, color='red')  # 中位数线加粗\n",
    "    flierprops = dict(marker='o', markersize=8,  # 离群点圆圈尺寸\n",
    "                      markerfacecolor='#FF7F0E', markeredgecolor='#333333',\n",
    "                      markeredgewidth=1, alpha=0.8)\n",
    "\n",
    "    # 绘制箱线图\n",
    "    bp = ax.boxplot(\n",
    "        r_dw22_df,\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor='#1F77B4', alpha=0.9, **boxprops),  # 更明亮的蓝色\n",
    "        whiskerprops=whiskerprops,\n",
    "        medianprops=medianprops,\n",
    "        flierprops=flierprops,\n",
    "        widths=0.3,  # 箱线图宽度\n",
    "        positions=[1]  # 将箱线图固定在Breakage Speed位置\n",
    "    )\n",
    "\n",
    "    # 设置标题和标签\n",
    "    plt.title('Spindle Speed Distribution During Breakage Events', fontsize=14, pad=10, fontweight='bold', color='#333333')\n",
    "    plt.xlabel('Breakage Phase', fontsize=12, labelpad=8, fontweight='bold', color='#333333')\n",
    "    plt.ylabel('Speed (rpm)', fontsize=12, labelpad=8, fontweight='bold', color='#333333')\n",
    "\n",
    "    # 设置刻度标签\n",
    "    plt.xticks([1], ['Breakage Speed'], fontsize=10, fontweight='bold', color='#333333')\n",
    "    plt.yticks(fontsize=10, fontweight='bold', color='#333333')\n",
    "\n",
    "    # 自定义网格线\n",
    "    ax.grid(True, axis='y', linestyle='--', linewidth=1, alpha=0.2, color='#666666')\n",
    "\n",
    "    # 添加图例（实际速度）\n",
    "    actual_speed_legend = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#D62728', markersize=10, label='Actual Speed')\n",
    "    # 添加图例（设定速度）\n",
    "    target_speed_legend = plt.Line2D([0], [0], color='#1F77B4', linewidth=6, label='Target Speed')\n",
    "\n",
    "    # 将图例放置在图片内部右上角\n",
    "    plt.legend(handles=[actual_speed_legend, target_speed_legend], loc='upper right', fontsize=10, framealpha=0.9, bbox_to_anchor=(1.01, 1))\n",
    "\n",
    "    # 调整布局\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存高质量图像\n",
    "    plt.savefig('breakage_speed_distribution_optimized.png', dpi=600, bbox_inches='tight', facecolor='#F0F0F0', transparent=False)\n",
    "\n",
    "    plt.show()"
   ],
   "id": "3051cce6994ddd99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# 设置全局样式\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams.update({\n",
    "    'font.sans-serif': 'Arial',\n",
    "    'axes.unicode_minus': False,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16,\n",
    "    'figure.titleweight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'font.weight': 'bold',\n",
    "    'grid.alpha': 0.2\n",
    "})\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\", parse_dates=['created_at'])\n",
    "\n",
    "# 锭位状态映射\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Started\",\n",
    "    2: \"Full\",\n",
    "    3: \"Broken\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Lost Speed\",\n",
    "    7: \"Disconnected\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 准备锭位状态、速度和卷装程度的列名\n",
    "spindle_cols = [(f\"D_dw{i}\", f\"V_dw{i}\", f\"P_dw{i}\") for i in range(1, 101)\n",
    "                if f\"D_dw{i}\" in df.columns and f\"V_dw{i}\" in df.columns and f\"P_dw{i}\" in df.columns]\n",
    "\n",
    "# 找出断纱最多的前十台设备\n",
    "device_broken_counts = defaultdict(int)\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    broken_by_device = df[df[status_col] == 3].groupby('subsystem').size()\n",
    "    for device, count in broken_by_device.items():\n",
    "        device_broken_counts[device] += count\n",
    "\n",
    "# 转换为DataFrame并排序\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Count', ascending=False).head(10)\n",
    "\n",
    "    # 分析断纱事件\n",
    "    all_broken_records = []\n",
    "    for device_name in top_devices.index:\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        for status_col, speed_col, _ in spindle_cols:\n",
    "            broken_indices = device_data[device_data[status_col] == 3].index\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # 获取断纱事件的开始和结束\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "            if current_start is None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # 分析每个断纱事件\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                try:\n",
    "                    speed_during = float(device_data.loc[start_idx, speed_col])\n",
    "                    record = {\n",
    "                        'Device': device_name,\n",
    "                        'Spindle': status_col.split('_')[1],\n",
    "                        'Break Start Time': device_data.loc[start_idx, 'created_at'],\n",
    "                        'Break End Time': device_data.loc[end_idx, 'created_at'],\n",
    "                        'The speed during the yarn breakage': speed_during\n",
    "                    }\n",
    "                    device_records.append(record)\n",
    "                    all_broken_records.append(record)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "\n",
    "    # 绘制断纱速度的散点图和 R_dw22 的箱线图\n",
    "    plt.figure(figsize=(5, 4), facecolor='#F0F0F0', dpi=300)  # 进一步压缩宽度，确保正方形\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # 将图片内全部填充为灰色\n",
    "    ax.set_facecolor('#F0F0F0')\n",
    "\n",
    "    # 去除顶部和右侧边框\n",
    "\n",
    "    # 调整下、左边框样式\n",
    "    ax.spines['right'].set_linewidth(2)  # 加粗边框线\n",
    "    ax.spines['top'].set_linewidth(2)    # 加粗边框线\n",
    "    ax.spines['right'].set_color('black')\n",
    "    ax.spines['top'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(2)  # 加粗边框线\n",
    "    ax.spines['left'].set_linewidth(2)    # 加粗边框线\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['left'].set_color('black')\n",
    "    # 加粗 X 轴和 Y 轴的数据分割线外面的小轴，类似指示线\n",
    "    # ax.spines['left'].set_linewidth(3)  # 左侧 Y 轴加粗\n",
    "    # ax.spines['bottom'].set_linewidth(3)  # 底部 X 轴加粗\n",
    "    # ax.spines['left'].set_color('#333333')\n",
    "    # ax.spines['bottom'].set_color('#333333')\n",
    "\n",
    "    # 绘制散点图（实际速度）\n",
    "    speed_df = pd.DataFrame(all_broken_records)[['The speed during the yarn breakage']]\n",
    "    for i, (_, row) in enumerate(speed_df.iterrows()):\n",
    "        speed_during = row['The speed during the yarn breakage']\n",
    "        plt.plot(1, speed_during, 'o', markersize=10, alpha=0.7, color='#D62728', label='Actual Speed' if i == 0 else '')\n",
    "\n",
    "    # 绘制 R_dw22 的箱线图（设定速度）\n",
    "    r_dw22_df = df[df['R_dw22'].notna()]['R_dw22']\n",
    "    boxprops = dict(linewidth=3, color='#1F77B4')  # 线条加粗\n",
    "    whiskerprops = dict(linewidth=3, color='#1F77B4')  # 须线加粗\n",
    "    medianprops = dict(linewidth=4, color='red')  # 中位数线加粗\n",
    "    flierprops = dict(marker='o', markersize=8,  # 离群点圆圈尺寸\n",
    "                      markerfacecolor='#FF7F0E', markeredgecolor='#333333',\n",
    "                      markeredgewidth=1, alpha=0.8)\n",
    "\n",
    "    # 绘制箱线图\n",
    "    bp = ax.boxplot(\n",
    "        r_dw22_df,\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor='#1F77B4', alpha=0.9, **boxprops),  # 更明亮的蓝色\n",
    "        whiskerprops=whiskerprops,\n",
    "        medianprops=medianprops,\n",
    "        flierprops=flierprops,\n",
    "        widths=0.3,  # 箱线图宽度\n",
    "        positions=[1]  # 将箱线图固定在Breakage Speed位置\n",
    "    )\n",
    "\n",
    "    # 设置标题和标签\n",
    "    # plt.title('Spindle Speed Distribution During Breakage Events', fontsize=14, pad=10, fontweight='bold', color='#333333')\n",
    "    plt.xlabel('Breakage Phase', fontsize=12, labelpad=8, fontweight='bold', color='#333333')\n",
    "    plt.ylabel('Speed (rpm)', fontsize=12, labelpad=8, fontweight='bold', color='#333333')\n",
    "\n",
    "    # 设置刻度标签\n",
    "    plt.xticks([1], ['Breakage Speed'], fontsize=10, fontweight='bold', color='#333333')\n",
    "    plt.yticks(fontsize=10, fontweight='bold', color='#333333')\n",
    "\n",
    "    # 自定义网格线\n",
    "    ax.grid(True, axis='y', linestyle='--', linewidth=1, alpha=0.2, color='#666666')\n",
    "\n",
    "    # 添加图例（实际速度）\n",
    "    actual_speed_legend = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#D62728', markersize=10, label='Actual Speed')\n",
    "    # 添加图例（设定速度）\n",
    "    target_speed_legend = plt.Line2D([0], [0], color='#1F77B4', linewidth=6, label='Target Speed')\n",
    "\n",
    "    # 将图例放置在图片内部右上角\n",
    "    plt.legend(handles=[actual_speed_legend, target_speed_legend], loc='upper right', fontsize=8, framealpha=0.9, bbox_to_anchor=(1.01, 1))\n",
    "\n",
    "    # 调整布局\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存高质量图像\n",
    "    plt.savefig('breakage_speed_distribution_optimized.png', dpi=300, bbox_inches='tight', facecolor='#F0F0F0', transparent=False)\n",
    "\n",
    "    plt.show()"
   ],
   "id": "9b0d78ff1e446e21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# 设置全局样式\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams.update({\n",
    "    'font.sans-serif': 'Arial',\n",
    "    'axes.unicode_minus': False,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16,\n",
    "    'figure.titleweight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'font.weight': 'bold',\n",
    "    'grid.alpha': 0.2\n",
    "})\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\", parse_dates=['created_at'])\n",
    "\n",
    "# 锭位状态映射\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Started\",\n",
    "    2: \"Full\",\n",
    "    3: \"Broken\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Lost Speed\",\n",
    "    7: \"Disconnected\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 准备锭位状态、速度和卷装程度的列名\n",
    "spindle_cols = [(f\"D_dw{i}\", f\"V_dw{i}\", f\"P_dw{i}\") for i in range(1, 101)\n",
    "                if f\"D_dw{i}\" in df.columns and f\"V_dw{i}\" in df.columns and f\"P_dw{i}\" in df.columns]\n",
    "\n",
    "# 找出断纱最多的前十台设备\n",
    "device_broken_counts = defaultdict(int)\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    broken_by_device = df[df[status_col] == 3].groupby('subsystem').size()\n",
    "    for device, count in broken_by_device.items():\n",
    "        device_broken_counts[device] += count\n",
    "\n",
    "# 转换为DataFrame并排序\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Count', ascending=False).head(10)\n",
    "\n",
    "    # 分析断纱事件\n",
    "    all_broken_records = []\n",
    "    for device_name in top_devices.index:\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        for status_col, speed_col, _ in spindle_cols:\n",
    "            broken_indices = device_data[device_data[status_col] == 3].index\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # 获取断纱事件的开始和结束\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "            if current_start is None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # 分析每个断纱事件\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                try:\n",
    "                    speed_during = float(device_data.loc[start_idx, speed_col])\n",
    "                    record = {\n",
    "                        'Device': device_name,\n",
    "                        'Spindle': status_col.split('_')[1],\n",
    "                        'Break Start Time': device_data.loc[start_idx, 'created_at'],\n",
    "                        'Break End Time': device_data.loc[end_idx, 'created_at'],\n",
    "                        'The speed during the yarn breakage': speed_during\n",
    "                    }\n",
    "                    device_records.append(record)\n",
    "                    all_broken_records.append(record)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "\n",
    "    # 绘制断纱速度的散点图和 R_dw22 的箱线图\n",
    "    plt.figure(figsize=(5, 4), facecolor='#F0F0F0', dpi=300)  # 进一步压缩宽度，确保正方形\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # 将图片内全部填充为灰色\n",
    "    ax.set_facecolor('#F0F0F0')\n",
    "\n",
    "    # 加粗图片的四个边框\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(3)  # 加粗所有边框\n",
    "        spine.set_color('#333333')  # 设置边框颜色\n",
    "\n",
    "    # 绘制散点图（实际速度）\n",
    "    speed_df = pd.DataFrame(all_broken_records)[['The speed during the yarn breakage']]\n",
    "    for i, (_, row) in enumerate(speed_df.iterrows()):\n",
    "        speed_during = row['The speed during the yarn breakage']\n",
    "        plt.plot(1, speed_during, 'o', markersize=10, alpha=0.7, color='#D62728', label='Actual Speed' if i == 0 else '')\n",
    "\n",
    "    # 绘制 R_dw22 的箱线图（设定速度）\n",
    "    r_dw22_df = df[df['R_dw22'].notna()]['R_dw22']\n",
    "    boxprops = dict(linewidth=3, color='#1F77B4')  # 线条加粗\n",
    "    whiskerprops = dict(linewidth=3, color='#1F77B4')  # 须线加粗\n",
    "    medianprops = dict(linewidth=4, color='red')  # 中位数线加粗\n",
    "    flierprops = dict(marker='o', markersize=8,  # 离群点圆圈尺寸\n",
    "                      markerfacecolor='#FF7F0E', markeredgecolor='#333333',\n",
    "                      markeredgewidth=1, alpha=0.8)\n",
    "\n",
    "    # 绘制箱线图\n",
    "    bp = ax.boxplot(\n",
    "        r_dw22_df,\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor='#1F77B4', alpha=0.9, **boxprops),  # 更明亮的蓝色\n",
    "        whiskerprops=whiskerprops,\n",
    "        medianprops=medianprops,\n",
    "        flierprops=flierprops,\n",
    "        widths=0.3,  # 箱线图宽度\n",
    "        positions=[1]  # 将箱线图固定在Breakage Speed位置\n",
    "    )\n",
    "\n",
    "    # 设置标题和标签\n",
    "    # plt.title('Spindle Speed Distribution During Breakage Events', fontsize=14, pad=10, fontweight='bold', color='#333333')\n",
    "    plt.xlabel('Breakage Phase', fontsize=12, labelpad=8, fontweight='bold', color='#333333')\n",
    "    plt.ylabel('Speed (r/min)', fontsize=12, labelpad=8, fontweight='bold', color='#333333')\n",
    "\n",
    "    # 设置刻度标签\n",
    "    plt.xticks([1], ['Breakage Speed'], fontsize=10, fontweight='bold', color='#333333')\n",
    "    plt.yticks(fontsize=10, fontweight='bold', color='#333333')\n",
    "\n",
    "    # 自定义网格线\n",
    "    ax.grid(True, axis='y', linestyle='--', linewidth=1, alpha=0.2, color='#666666')\n",
    "\n",
    "    # 添加图例（实际速度）\n",
    "    actual_speed_legend = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#D62728', markersize=10, label='Actual Speed')\n",
    "    # 添加图例（设定速度）\n",
    "    target_speed_legend = plt.Line2D([0], [0], color='#1F77B4', linewidth=6, label='Target Speed')\n",
    "\n",
    "    # 将图例放置在图片内部右上角\n",
    "    plt.legend(handles=[actual_speed_legend, target_speed_legend], loc='upper right', fontsize=10, framealpha=0.9, bbox_to_anchor=(1.02, 1))\n",
    "\n",
    "    # 调整布局\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存高质量图像\n",
    "    plt.savefig('breakage_speed_distribution_optimized.png', dpi=1200, bbox_inches='tight', facecolor='#F0F0F0', transparent=False)\n",
    "\n",
    "    plt.show()"
   ],
   "id": "cb9d325e46433672",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# 设置全局样式\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams.update({\n",
    "    'font.sans-serif': 'Arial',\n",
    "    'axes.unicode_minus': False,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelsize': 5,\n",
    "    'xtick.labelsize': 7,\n",
    "    'ytick.labelsize': 7,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16,\n",
    "    'figure.titleweight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'font.weight': 'bold',\n",
    "    'grid.alpha': 0.2\n",
    "})\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\", parse_dates=['created_at'])\n",
    "\n",
    "# 锭位状态映射\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Started\",\n",
    "    2: \"Full\",\n",
    "    3: \"Broken\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Lost Speed\",\n",
    "    7: \"Disconnected\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 准备锭位状态、速度和卷装程度的列名\n",
    "spindle_cols = [(f\"D_dw{i}\", f\"V_dw{i}\", f\"P_dw{i}\") for i in range(1, 101)\n",
    "                if f\"D_dw{i}\" in df.columns and f\"V_dw{i}\" in df.columns and f\"P_dw{i}\" in df.columns]\n",
    "\n",
    "# 找出断纱最多的前十台设备\n",
    "device_broken_counts = defaultdict(int)\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    broken_by_device = df[df[status_col] == 3].groupby('subsystem').size()\n",
    "    for device, count in broken_by_device.items():\n",
    "        device_broken_counts[device] += count\n",
    "\n",
    "# 转换为DataFrame并排序\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Count', ascending=False).head(10)\n",
    "\n",
    "    # 分析断纱事件\n",
    "    all_broken_records = []\n",
    "    for device_name in top_devices.index:\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        for status_col, speed_col, _ in spindle_cols:\n",
    "            broken_indices = device_data[device_data[status_col] == 3].index\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # 获取断纱事件的开始和结束\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "            if current_start is None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # 分析每个断纱事件\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                try:\n",
    "                    speed_during = float(device_data.loc[start_idx, speed_col])\n",
    "                    record = {\n",
    "                        'Device': device_name,\n",
    "                        'Spindle': status_col.split('_')[1],\n",
    "                        'Break Start Time': device_data.loc[start_idx, 'created_at'],\n",
    "                        'Break End Time': device_data.loc[end_idx, 'created_at'],\n",
    "                        'The speed during the yarn breakage': speed_during\n",
    "                    }\n",
    "                    device_records.append(record)\n",
    "                    all_broken_records.append(record)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "\n",
    "    # 绘制断纱速度的散点图和 R_dw22 的箱线图\n",
    "    plt.figure(figsize=(5, 4), facecolor='white', dpi=300)  # 背景为白色\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # 只保留框线内的底色\n",
    "    ax.set_facecolor('#F0F0F0')  # 框线内填充灰色\n",
    "\n",
    "    # 加粗图片的四个边框\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(2)  # 加粗所有边框\n",
    "        spine.set_color('#333333')  # 设置边框颜色\n",
    "\n",
    "    # 绘制散点图（实际速度）\n",
    "    speed_df = pd.DataFrame(all_broken_records)[['The speed during the yarn breakage']]\n",
    "    for i, (_, row) in enumerate(speed_df.iterrows()):\n",
    "        speed_during = row['The speed during the yarn breakage']\n",
    "        plt.plot(1, speed_during, 'o', markersize=10, alpha=0.7, color='#D62728', label='Actual Speed' if i == 0 else '')\n",
    "\n",
    "    # 绘制 R_dw22 的箱线图（设定速度）\n",
    "    r_dw22_df = df[df['R_dw22'].notna()]['R_dw22']\n",
    "    boxprops = dict(linewidth=3, color='#1F77B4')  # 线条加粗\n",
    "    whiskerprops = dict(linewidth=3, color='#1F77B4')  # 须线加粗\n",
    "    medianprops = dict(linewidth=4, color='red')  # 中位数线加粗\n",
    "    flierprops = dict(marker='o', markersize=8,  # 离群点圆圈尺寸\n",
    "                      markerfacecolor='#FF7F0E', markeredgecolor='#333333',\n",
    "                      markeredgewidth=1, alpha=0.8)\n",
    "\n",
    "    # 绘制箱线图\n",
    "    bp = ax.boxplot(\n",
    "        r_dw22_df,\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor='#1F77B4', alpha=0.9, **boxprops),  # 更明亮的蓝色\n",
    "        whiskerprops=whiskerprops,\n",
    "        medianprops=medianprops,\n",
    "        flierprops=flierprops,\n",
    "        widths=0.3,  # 箱线图宽度\n",
    "        positions=[1]  # 将箱线图固定在Breakage Speed位置\n",
    "    )\n",
    "\n",
    "    # 设置标题和标签\n",
    "    plt.xlabel('Breakage Phase', fontsize=11, labelpad=8, fontweight='bold', color='#333333')\n",
    "    plt.ylabel('Speed (r/min)', fontsize=11, labelpad=8, fontweight='bold', color='#333333')\n",
    "\n",
    "    # 设置刻度标签\n",
    "    plt.xticks([1], ['Breakage Speed'], fontsize=9, fontweight='bold', color='#333333')\n",
    "    plt.yticks(fontsize=9, fontweight='bold', color='#333333')\n",
    "\n",
    "    # 自定义网格线\n",
    "    ax.grid(True, axis='y', linestyle='--', linewidth=1, alpha=0.2, color='#666666')\n",
    "\n",
    "    # 加粗 X 轴和 Y 轴的数据分割线外面的小轴（类似指示线）\n",
    "    ax.tick_params(axis='x', which='both', length=6, width=2, colors='#333333')  # 加粗X轴刻度线\n",
    "    ax.tick_params(axis='y', which='both', length=6, width=2, colors='#333333')  # 加粗Y轴刻度线\n",
    "\n",
    "    # 添加指示速度的刻度线（如 8000、2000 等）\n",
    "    for speed in [2000, 4000, 6000, 8000]:\n",
    "        ax.axhline(y=speed, color='#333333', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "    # 添加图例（实际速度）\n",
    "    actual_speed_legend = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#D62728', markersize=10, label='Actual Speed')\n",
    "    # 添加图例（设定速度）\n",
    "    target_speed_legend = plt.Line2D([0], [0], color='#1F77B4', linewidth=6, label='Target Speed')\n",
    "\n",
    "    # 将图例放置在图片内部右上角\n",
    "    plt.legend(handles=[actual_speed_legend, target_speed_legend], loc='upper right', fontsize=10, framealpha=0.9, bbox_to_anchor=(1.03, 1))\n",
    "\n",
    "    # 调整布局\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存高质量图像\n",
    "    plt.savefig('breakage_speed_distribution_optimized.png', dpi=300, bbox_inches='tight', facecolor='white', transparent=False)\n",
    "\n",
    "    plt.show()"
   ],
   "id": "353a548db9b536f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# 设置全局样式 - 增强可读性\n",
    "plt.style.use('seaborn-v0_8')  # 使用 seaborn 样式\n",
    "plt.rcParams.update({\n",
    "    'font.sans-serif': 'Arial',        # 使用更清晰的Arial字体\n",
    "    'axes.unicode_minus': False,       # 正确显示负号\n",
    "    'axes.titlesize': 18,              # 增大标题字号\n",
    "    'axes.titleweight': 'bold',        # 标题加粗\n",
    "    'axes.labelsize': 16,              # 增大轴标签字号\n",
    "    'xtick.labelsize': 14,             # 增大X轴刻度字号\n",
    "    'ytick.labelsize': 14,             # 增大Y轴刻度字号\n",
    "    'legend.fontsize': 12,             # 增大图例字号\n",
    "    'figure.titlesize': 20,            # 图形总标题字号\n",
    "    'figure.titleweight': 'bold',      # 图形总标题加粗\n",
    "    'axes.labelweight': 'bold',        # 轴标签加粗\n",
    "    'font.weight': 'bold',             # 全局字体加粗\n",
    "    'grid.alpha': 0.3                  # 网格透明度\n",
    "})\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\", parse_dates=['created_at'])\n",
    "\n",
    "# 锭位状态映射\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Started\",\n",
    "    2: \"Full\",\n",
    "    3: \"Broken\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Lost Speed\",\n",
    "    7: \"Disconnected\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 准备锭位状态、速度和卷装程度的列名\n",
    "spindle_cols = [(f\"D_dw{i}\", f\"V_dw{i}\", f\"P_dw{i}\") for i in range(1, 101)\n",
    "                if f\"D_dw{i}\" in df.columns and f\"V_dw{i}\" in df.columns and f\"P_dw{i}\" in df.columns]\n",
    "\n",
    "# 找出断纱最多的前十台设备\n",
    "device_broken_counts = defaultdict(int)\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    broken_by_device = df[df[status_col] == 3].groupby('subsystem').size()\n",
    "    for device, count in broken_by_device.items():\n",
    "        device_broken_counts[device] += count\n",
    "\n",
    "# 转换为DataFrame并排序\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Count', ascending=False).head(10)\n",
    "\n",
    "    # 分析断纱前后速度变化 - 改进后的图表\n",
    "    plt.figure(figsize=(16, 9), facecolor='white', dpi=120)  # 增大尺寸和DPI\n",
    "    ax = plt.gca()\n",
    "    all_broken_records = []\n",
    "\n",
    "    for device_name in top_devices.index:\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        for status_col, speed_col, _ in spindle_cols:\n",
    "            broken_indices = device_data[device_data[status_col] == 3].index\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # 获取断纱事件的开始和结束\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "            if current_start is not None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # 分析每个断纱事件\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                prev_idx = device_data.index.get_loc(start_idx) - 1\n",
    "                while prev_idx >= 0 and device_data.iloc[prev_idx][status_col] == 3:\n",
    "                    prev_idx -= 1\n",
    "                next_idx = device_data.index.get_loc(end_idx) + 1\n",
    "                while next_idx < len(device_data) and device_data.iloc[next_idx][status_col] == 3:\n",
    "                    next_idx += 1\n",
    "\n",
    "                if prev_idx >= 0 and next_idx < len(device_data):\n",
    "                    target_speed_col = f\"R_dw22\"\n",
    "                    target_speed = device_data.iloc[prev_idx][target_speed_col] if target_speed_col in device_data.columns else None\n",
    "\n",
    "                    try:\n",
    "                        speed_before = float(device_data.iloc[prev_idx][speed_col])\n",
    "                        speed_during = float(device_data.loc[start_idx, speed_col])\n",
    "                        speed_after = float(device_data.iloc[next_idx][speed_col])\n",
    "\n",
    "                        duration = (device_data.loc[end_idx, 'created_at'] - device_data.loc[start_idx, 'created_at']).total_seconds() / 60\n",
    "\n",
    "                        record = {\n",
    "                            'Device': device_name,\n",
    "                            'Spindle': status_col.split('_')[1],\n",
    "                            'Break Start Time': device_data.loc[start_idx, 'created_at'],\n",
    "                            'Break End Time': device_data.loc[end_idx, 'created_at'],\n",
    "                            'Speed Before': speed_before,\n",
    "                            'Speed During Break': speed_during,\n",
    "                            'Speed After': speed_after,\n",
    "                            'Target Speed': float(target_speed) if pd.notna(target_speed) else None,\n",
    "                            'Break Duration': duration\n",
    "                        }\n",
    "                        device_records.append(record)\n",
    "                        all_broken_records.append(record)\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "\n",
    "        # 绘制典型断纱事件（最多5个）\n",
    "        if device_records:\n",
    "            sample_records = pd.DataFrame(device_records).sample(min(5, len(device_records)), random_state=42)\n",
    "            for _, record in sample_records.iterrows():\n",
    "                line = plt.plot(['Before Break', 'During Break', 'After Break'],\n",
    "                              [record['Speed Before'], record['Speed During Break'], record['Speed After']],\n",
    "                              marker='o', linewidth=3, markersize=10, alpha=0.8,\n",
    "                              label=f\"{record['Device']}-{record['Spindle']} @ {record['Break Start Time'].strftime('%H:%M')}\")\n",
    "\n",
    "                if pd.notna(record['Target Speed']):\n",
    "                    plt.axhline(y=record['Target Speed'], color='gray', linestyle='--', alpha=0.5, linewidth=2)\n",
    "\n",
    "    # 设置图表样式 - 增强版\n",
    "    plt.title('Speed Changes Around Broken Spindle Events\\n(Top 10 Devices with Most Breaks)',\n",
    "              fontsize=18, pad=20)\n",
    "    plt.xlabel('Event Phase', fontsize=16, labelpad=10)\n",
    "    plt.ylabel('Spindle Speed (rpm)', fontsize=16, labelpad=10)\n",
    "\n",
    "    # 优化图例和网格\n",
    "    legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left',\n",
    "                        title='Device-Spindle @ Time', title_fontsize=13,\n",
    "                        framealpha=1, edgecolor='#333333')\n",
    "    plt.setp(legend.get_title(), fontweight='bold')\n",
    "\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存高质量图像\n",
    "    plt.savefig('speed_changes_high_quality.png', dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 绘制速度分布箱线图 - 改进版（根据要求优化）\n",
    "plt.figure(figsize=(12, 7), facecolor='white', dpi=120)\n",
    "ax = plt.gca()\n",
    "\n",
    "# 设置边框样式 - 添加白色边框\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(2.5)\n",
    "    spine.set_color('#333333')\n",
    "\n",
    "# 使用更明亮的蓝色和更粗的线条\n",
    "boxprops = dict(linewidth=3, color='royalblue')\n",
    "whiskerprops = dict(linewidth=3, color='darkblue')\n",
    "medianprops = dict(linewidth=4, color='firebrick')\n",
    "\n",
    "# 绘制箱线图 - 使用明亮的蓝色\n",
    "bp = speed_changes[['Speed Before', 'Speed During Break', 'Speed After']].boxplot(\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='#5B9BD5', alpha=0.9, **boxprops),  # 更明亮的蓝色\n",
    "    whiskerprops=whiskerprops,\n",
    "    medianprops=medianprops,\n",
    "    flierprops=dict(marker='o', markersize=6, alpha=0.6),\n",
    "    widths=0.6  # 加宽箱体\n",
    ")\n",
    "\n",
    "# 设置标题和标签 - 加粗\n",
    "plt.title('Spindle Speed Distribution Around Break Events',\n",
    "          fontsize=20, pad=20, fontweight='bold')\n",
    "plt.xlabel('Event Phase', fontsize=18, labelpad=12, fontweight='bold')\n",
    "plt.ylabel('Speed (rpm)', fontsize=18, labelpad=12, fontweight='bold')\n",
    "\n",
    "# 设置刻度标签 - 加粗\n",
    "plt.xticks(fontsize=16, fontweight='bold')\n",
    "plt.yticks(fontsize=16, fontweight='bold')\n",
    "\n",
    "# 网格线设置\n",
    "plt.grid(True, linestyle='--', alpha=0.3, linewidth=1.5)\n",
    "\n",
    "# 添加浅色背景\n",
    "ax.set_facecolor('#F9F9F9')\n",
    "\n",
    "# 调整布局\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存超高质量图像\n",
    "plt.savefig('speed_distribution_enhanced.png',\n",
    "            dpi=350,\n",
    "            bbox_inches='tight',\n",
    "            facecolor='white',\n",
    "            edgecolor='#333333',\n",
    "            linewidth=2)\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "8ea91c45467ddb39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 数据准备\n",
    "data = {\n",
    "    'feature': ['V$_{1}$', 'P$_{1}$', 'R$_{3}$_380','R$_{3}$_375', 'R$_{6}$_600', 'R$_{2}$_9200',\n",
    "                'R$_{4}$_300', 'R$_{2}$_9500', 'R$_{5}$_8.2', 'R$_{1}$_1300', 'R$_{2}$_9000',\n",
    "                'R$_{3}$_443', 'R$_{5}$_9.1', 'R$_{1}$_2000', 'R$_{6}$_1100', 'R$_{5}$_11.2', 'R$_{3}$_365'],\n",
    "    'importance': [0.414, 0.391, 0.054, 0.032, 0.019, 0.018,\n",
    "                   0.015, 0.011, 0.010, 0.009, 0.008,\n",
    "                   0.006, 0.005, 0.004, 0.002, 0.001, 0.001]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 按照重要性从高到低排序\n",
    "df = df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# 设置绘图风格\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.size': 20,\n",
    "    # 'font.weight'='bold',  # 加粗标签字体\n",
    "    # 'color'='black'\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "\n",
    "    'axes.edgecolor': 'black',\n",
    "    'axes.linewidth': 4,  # Thicker axes lines\n",
    "    'xtick.color': 'black',\n",
    "    'ytick.color': 'black',\n",
    "    'axes.labelsize': 26,\n",
    "    'axes.titlesize': 26,\n",
    "    'xtick.labelsize': 24,\n",
    "    'ytick.labelsize': 24,\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'figure.figsize': (12, 8),\n",
    "    'axes.grid': True,\n",
    "    'grid.linestyle': '--',\n",
    "    'grid.alpha': 0.3,\n",
    "    'grid.color': 'gray',\n",
    "    # 'axes.facecolor': 'white', 可以保留也可以去掉这一行，下面会显式设置\n",
    "})\n",
    "\n",
    "# 创建颜色映射 (蓝色渐变，颜色更鲜明)\n",
    "colors = plt.cm.Blues(np.linspace(0.3, 0.9, len(df)))\n",
    "\n",
    "# 创建图形\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# 显式设置绘图区域（坐标轴内）的背景颜色为白色\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# 绘制横向柱状图，反转特征顺序\n",
    "bars = ax.barh(\n",
    "    df['feature'][::-1],\n",
    "    df['importance'][::-1],\n",
    "    color=colors[::-1],\n",
    "    height=0.8,\n",
    "    edgecolor='black',\n",
    "    linewidth=2  # 加粗边框线\n",
    ")\n",
    "\n",
    "# 添加数据标签\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(\n",
    "        width + 0.001,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f'{width:.3f}',\n",
    "        va='center',\n",
    "        ha='left',\n",
    "        fontsize=22,\n",
    "        fontweight='bold',  # 加粗标签字体\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "# 设置坐标轴\n",
    "ax.set_xlabel('Feature Importance Score', fontweight='bold', labelpad=26)\n",
    "ax.set_ylabel('Features', fontweight='bold', labelpad=26)\n",
    "# ax.set_title('Feature Importance Ranking', pad=30, fontweight='bold')\n",
    "\n",
    "# 调整 x 轴范围\n",
    "ax.set_xlim(0, df['importance'].max() * 1.2)\n",
    "\n",
    "# 隐藏上、右边框\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# 调整下、左边框样式\n",
    "ax.spines['bottom'].set_linewidth(3)  # 加粗边框线\n",
    "ax.spines['left'].set_linewidth(3)    # 加粗边框线\n",
    "ax.spines['bottom'].set_color('black')\n",
    "ax.spines['left'].set_color('black')\n",
    "\n",
    "# 调整刻度标签颜色和粗细\n",
    "ax.tick_params(axis='x', colors='black', width=2, length=6)\n",
    "ax.tick_params(axis='y', colors='black', width=2, length=6)\n",
    "\n",
    "# 调整刻度标签颜色和粗细\n",
    "# ax.tick_params(axis='x', colors='black', width=2, length=6, labelsize=24, labelweight='bold')\n",
    "# ax.tick_params(axis='y', colors='black', width=2, length=6, labelsize=24, labelweight='bold')\n",
    "\n",
    "\n",
    "# 'font.weight'='bold',  # 加粗标签字体\n",
    "    # 'color'='black'\n",
    "\n",
    "# 设置 y 轴标签字体为斜体\n",
    "for tick in ax.get_yticklabels():\n",
    "    tick.set_fontstyle('italic')\n",
    "\n",
    "# 调整子图布局\n",
    "plt.subplots_adjust(left=0.15, right=0.95, top=0.9, bottom=0.1)\n",
    "\n",
    "# 调整布局\n",
    "# plt.tight_layout()\n",
    "\n",
    "# 保存图像\n",
    "# plt.savefig('feature_importance_bold_clear.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ],
   "id": "1de6e34f641276b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 数据准备\n",
    "data = {\n",
    "    'feature': ['V$_{1}$', 'P$_{1}$', 'R$_{3}$_380','R$_{3}$_375', 'R$_{6}$_600', 'R$_{2}$_9200',\n",
    "                'R$_{4}$_300', 'R$_{2}$_9500', 'R$_{5}$_8.2', 'R$_{1}$_1300', 'R$_{2}$_9000',\n",
    "                'R$_{3}$_443', 'R$_{5}$_9.1', 'R$_{1}$_2000', 'R$_{6}$_1100', 'R$_{5}$_11.2', 'R$_{3}$_365'],\n",
    "    'importance': [0.414, 0.391, 0.054, 0.032, 0.019, 0.018,\n",
    "                   0.015, 0.011, 0.010, 0.009, 0.008,\n",
    "                   0.006, 0.005, 0.004, 0.002, 0.001, 0.001]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 按照重要性从高到低排序\n",
    "df = df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# 设置绘图风格\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.size': 20,\n",
    "    # 'font.weight'='bold',  # 加粗标签字体\n",
    "    # 'color'='black'\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.edgecolor': 'black',\n",
    "    'axes.linewidth': 4,  # Thicker axes lines\n",
    "    'xtick.color': 'black',\n",
    "    'ytick.color': 'black',\n",
    "    'axes.labelsize': 26,\n",
    "    'axes.titlesize': 26,\n",
    "    'xtick.labelsize': 24,\n",
    "    'ytick.labelsize': 24,\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'figure.figsize': (12, 8),\n",
    "    'axes.grid': True,\n",
    "    'grid.linestyle': '--',\n",
    "    'grid.alpha': 0.3,\n",
    "    'grid.color': 'gray',\n",
    "    # 'axes.facecolor': 'white', 可以保留也可以去掉这一行，下面会显式设置\n",
    "})\n",
    "\n",
    "# 创建颜色映射 (蓝色渐变，颜色更鲜明)\n",
    "colors = plt.cm.Blues(np.linspace(0.3, 0.9, len(df)))\n",
    "\n",
    "# 创建图形\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# 显式设置绘图区域（坐标轴内）的背景颜色为白色\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# 绘制横向柱状图，反转特征顺序\n",
    "bars = ax.barh(\n",
    "    df['feature'][::-1],\n",
    "    df['importance'][::-1],\n",
    "    color=colors[::-1],\n",
    "    height=0.8,\n",
    "    edgecolor='black',\n",
    "    linewidth=2  # 加粗边框线\n",
    ")\n",
    "\n",
    "# 添加数据标签\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(\n",
    "        width + 0.001,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f'{width:.3f}',\n",
    "        va='center',\n",
    "        ha='left',\n",
    "        fontsize=22,\n",
    "        fontweight='bold',  # 加粗标签字体\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "# 设置坐标轴\n",
    "ax.set_xlabel('Feature Importance Score', fontweight='bold', labelpad=26)\n",
    "ax.set_ylabel('Features', fontweight='bold', labelpad=26)\n",
    "# ax.set_title('Feature Importance Ranking', pad=30, fontweight='bold')\n",
    "\n",
    "# 调整 x 轴范围\n",
    "ax.set_xlim(0, df['importance'].max() * 1.2)\n",
    "\n",
    "# 隐藏上、右边框\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# 调整下、左边框样式\n",
    "ax.spines['bottom'].set_linewidth(3)  # 加粗边框线\n",
    "ax.spines['left'].set_linewidth(3)    # 加粗边框线\n",
    "ax.spines['bottom'].set_color('black')\n",
    "ax.spines['left'].set_color('black')\n",
    "\n",
    "# 调整刻度标签颜色和粗细\n",
    "ax.tick_params(axis='x', colors='black', width=2, length=6, labelsize=24)\n",
    "ax.tick_params(axis='y', colors='black', width=2, length=6, labelsize=24)\n",
    "\n",
    "# # 设置刻度标签加粗\n",
    "# for tick in ax.get_xticklabels():\n",
    "#     tick.set_fontweight('bold')\n",
    "# for tick in ax.get_yticklabels():\n",
    "#     tick.set_fontweight('bold')\n",
    "\n",
    "# 'font.weight'='bold',  # 加粗标签字体\n",
    "    # 'color'='black'\n",
    "\n",
    "# 设置 y 轴标签字体为斜体\n",
    "for tick in ax.get_yticklabels():\n",
    "    tick.set_fontstyle('italic')\n",
    "\n",
    "# 调整子图布局\n",
    "plt.subplots_adjust(left=0.15, right=0.95, top=0.9, bottom=0.1)\n",
    "\n",
    "# 调整布局\n",
    "# plt.tight_layout()\n",
    "\n",
    "# 保存图像\n",
    "# plt.savefig('feature_importance_bold_clear.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ],
   "id": "6c859328d899884c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#print(f\"Unable to analyze data for device {target_device}\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# 设置全局样式 - 增强可读性\n",
    "plt.style.use('seaborn-v0_8')  # 使用 seaborn 样式\n",
    "plt.rcParams.update({\n",
    "    'font.sans-serif': 'Arial',  # 使用更清晰的Arial字体\n",
    "    'axes.unicode_minus': False,  # 正确显示负号\n",
    "    'axes.titlesize': 18,  # 增大标题字号\n",
    "    'axes.titleweight': 'bold',  # 标题加粗\n",
    "    'axes.labelsize': 16,  # 增大轴标签字号\n",
    "    'xtick.labelsize': 14,  # 增大X轴刻度字号\n",
    "    'ytick.labelsize': 14,  # 增大Y轴刻度字号\n",
    "    'legend.fontsize': 12,  # 增大图例字号\n",
    "    'figure.titlesize': 20,  # 图形总标题字号\n",
    "    'figure.titleweight': 'bold',  # 图形总标题加粗\n",
    "    'axes.labelweight': 'bold',  # 轴标签加粗\n",
    "    'font.weight': 'bold',  # 全局字体加粗\n",
    "    'grid.alpha': 0.3  # 网格透明度\n",
    "})\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\", parse_dates=['created_at'])\n",
    "\n",
    "# 锭位状态映射\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Started\",\n",
    "    2: \"Full\",\n",
    "    3: \"Broken\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Lost Speed\",\n",
    "    7: \"Disconnected\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 准备锭位状态、速度和卷装程度的列名\n",
    "spindle_cols = [(f\"D_dw{i}\", f\"V_dw{i}\", f\"P_dw{i}\") for i in range(1, 101)\n",
    "                if f\"D_dw{i}\" in df.columns and f\"V_dw{i}\" in df.columns and f\"P_dw{i}\" in df.columns]\n",
    "\n",
    "# 找出断纱最多的前十台设备\n",
    "device_broken_counts = defaultdict(int)\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    broken_by_device = df[df[status_col] == 3].groupby('subsystem').size()\n",
    "    for device, count in broken_by_device.items():\n",
    "        device_broken_counts[device] += count\n",
    "\n",
    "# 转换为DataFrame并排序\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Count', ascending=False).head(10)\n",
    "\n",
    "    # 分析断纱前后速度变化 - 改进后的图表\n",
    "    plt.figure(figsize=(16, 9), facecolor='white', dpi=120)  # 增大尺寸和DPI\n",
    "    ax = plt.gca()\n",
    "    all_broken_records = []\n",
    "\n",
    "    for device_name in top_devices.index:\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        for status_col, speed_col, _ in spindle_cols:\n",
    "            broken_indices = device_data[device_data[status_col] == 3].index\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # 获取断纱事件的开始和结束\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "            if current_start is not None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # 分析每个断纱事件\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                prev_idx = device_data.index.get_loc(start_idx) - 1\n",
    "                while prev_idx >= 0 and device_data.iloc[prev_idx][status_col] == 3:\n",
    "                    prev_idx -= 1\n",
    "                next_idx = device_data.index.get_loc(end_idx) + 1\n",
    "                while next_idx < len(device_data) and device_data.iloc[next_idx][status_col] == 3:\n",
    "                    next_idx += 1\n",
    "\n",
    "                if prev_idx >= 0 and next_idx < len(device_data):\n",
    "                    target_speed_col = f\"R_dw22\"\n",
    "                    target_speed = device_data.iloc[prev_idx][\n",
    "                        target_speed_col] if target_speed_col in device_data.columns else None\n",
    "\n",
    "                    try:\n",
    "                        speed_before = float(device_data.iloc[prev_idx][speed_col])\n",
    "                        speed_during = float(device_data.loc[start_idx, speed_col])\n",
    "                        speed_after = float(device_data.iloc[next_idx][speed_col])\n",
    "\n",
    "                        duration = (device_data.loc[end_idx, 'created_at'] - device_data.loc[\n",
    "                            start_idx, 'created_at']).total_seconds() / 60\n",
    "\n",
    "                        record = {\n",
    "                            'Device': device_name,\n",
    "                            'Spindle': status_col.split('_')[1],\n",
    "                            'Break Start Time': device_data.loc[start_idx, 'created_at'],\n",
    "                            'Break End Time': device_data.loc[end_idx, 'created_at'],\n",
    "                            'Speed Before': speed_before,\n",
    "                            'Speed During Break': speed_during,\n",
    "                            'Speed After': speed_after,\n",
    "                            'Target Speed': float(target_speed) if pd.notna(target_speed) else None,\n",
    "                            'Break Duration': duration\n",
    "                        }\n",
    "                        device_records.append(record)\n",
    "                        all_broken_records.append(record)\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "\n",
    "        # 绘制典型断纱事件（最多5个）\n",
    "        if device_records:\n",
    "            sample_records = pd.DataFrame(device_records).sample(min(5, len(device_records)), random_state=42)\n",
    "            for _, record in sample_records.iterrows():\n",
    "                line = plt.plot(['Before Break', 'During Break', 'After Break'],\n",
    "                                [record['Speed Before'], record['Speed During Break'], record['Speed After']],\n",
    "                                marker='o', linewidth=3, markersize=10, alpha=0.8,\n",
    "                                label=f\"{record['Device']}-{record['Spindle']} @ {record['Break Start Time'].strftime('%H:%M')}\")\n",
    "\n",
    "                if pd.notna(record['Target Speed']):\n",
    "                    plt.axhline(y=record['Target Speed'], color='gray', linestyle='--', alpha=0.5, linewidth=2)"
   ],
   "id": "34cd47e97060631e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # Use SimHei font\n",
    "plt.rcParams['axes.unicode_minus'] = False  # Correctly display negative signs\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "\n",
    "# Spindle status mapping\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Started\",\n",
    "    2: \"Full\",\n",
    "    3: \"Broken\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Lost Speed\",\n",
    "    7: \"Disconnected\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 1. Prepare spindle status, speed, and fullness column names (ensure one-to-one correspondence)\n",
    "spindle_cols = []\n",
    "for i in range(1, 101):\n",
    "    status_col = f\"D_dw{i}\"\n",
    "    speed_col = f\"V_dw{i}\"\n",
    "    fullness_col = f\"P_dw{i}\"\n",
    "    if status_col in df.columns and speed_col in df.columns and fullness_col in df.columns:\n",
    "        spindle_cols.append((status_col, speed_col, fullness_col))\n",
    "\n",
    "# 2. Find the top 10 devices with the most broken spindles\n",
    "device_broken_counts = defaultdict(int)\n",
    "\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    # Find all broken records and count by device\n",
    "    broken_mask = df[status_col] == 3\n",
    "    if broken_mask.any():\n",
    "        broken_by_device = df[broken_mask].groupby('subsystem').size()\n",
    "        for device, count in broken_by_device.items():\n",
    "            device_broken_counts[device] += count\n",
    "\n",
    "# Convert to DataFrame and sort\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Count', ascending=False).head(10)\n",
    "    print(\"\\nTop 10 devices with the most broken spindles:\")\n",
    "    print(top_devices)\n",
    "\n",
    "    # 3. Analyze the speed changes before and after broken spindles for these devices\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    all_broken_records = []\n",
    "\n",
    "    for device_name in top_devices.index:\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        for status_col, speed_col, _ in spindle_cols:\n",
    "            # Find all broken records for this device and spindle\n",
    "            broken_mask = (device_data[status_col] == 3)\n",
    "            broken_indices = device_data[broken_mask].index\n",
    "\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # Get all broken events' start and end\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue  # Continuous broken status\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "\n",
    "            if current_start is not None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # Analyze speed before and after each broken event\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                # Get the last non-broken record before the break\n",
    "                prev_idx = device_data.index.get_loc(start_idx) - 1\n",
    "                while prev_idx >= 0 and device_data.iloc[prev_idx][status_col] == 3:\n",
    "                    prev_idx -= 1\n",
    "\n",
    "                # Get the first non-broken record after the break\n",
    "                next_idx = device_data.index.get_loc(end_idx) + 1\n",
    "                while next_idx < len(device_data) and device_data.iloc[next_idx][status_col] == 3:\n",
    "                    next_idx += 1\n",
    "\n",
    "                if prev_idx >= 0 and next_idx < len(device_data):\n",
    "                    # Get target speed R_dw22 (ensure it corresponds to the current spindle)\n",
    "                    spindle_num = status_col.split('_')[1].replace('dw', '')\n",
    "                    target_speed_col = f\"R_dw22\"\n",
    "\n",
    "                    # Get target speed from the record before the break\n",
    "                    if target_speed_col in device_data.columns:\n",
    "                        target_speed = device_data.iloc[prev_idx][target_speed_col]\n",
    "                    else:\n",
    "                        target_speed = None\n",
    "\n",
    "                    record = {\n",
    "                        'Device': device_name,\n",
    "                        'Spindle': status_col.split('_')[1],\n",
    "                        'Break Start Time': device_data.loc[start_idx, 'created_at'],\n",
    "                        'Break End Time': device_data.loc[end_idx, 'created_at'],\n",
    "                        'Speed Before': device_data.iloc[prev_idx][speed_col],\n",
    "                        'Speed During Break': device_data.loc[start_idx, speed_col],\n",
    "                        'Speed After': device_data.iloc[next_idx][speed_col],\n",
    "                        'Target Speed': target_speed,\n",
    "                        'Break Duration': (device_data.loc[end_idx, 'created_at'] - device_data.loc[start_idx, 'created_at']).total_seconds() / 60\n",
    "                    }\n",
    "                    device_records.append(record)\n",
    "                    all_broken_records.append(record)\n",
    "\n",
    "        # Plot typical broken events for this device (up to 5)\n",
    "        if device_records:\n",
    "            sample_records = pd.DataFrame(device_records).sample(min(5, len(device_records)), random_state=42)\n",
    "            for _, record in sample_records.iterrows():\n",
    "                plt.plot(['Before Break', 'During Break', 'After Break'],\n",
    "                         [record['Speed Before'], record['Speed During Break'], record['Speed After']],\n",
    "                         marker='o',\n",
    "                         label=f\"{record['Device']}-{record['Spindle']}@{record['Break Start Time'].strftime('%H:%M')}\")\n",
    "\n",
    "                # If target speed exists, add a reference line\n",
    "                if pd.notna(record['Target Speed']):\n",
    "                    plt.axhline(y=record['Target Speed'], color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Set up the chart\n",
    "    plt.title('Speed Changes Before and After Broken Spindles for Top 10 Devices')\n",
    "    plt.xlabel('Time Point')\n",
    "    plt.ylabel('Actual Speed')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Calculate and display speed change statistics\n",
    "    if all_broken_records:\n",
    "        speed_changes = pd.DataFrame(all_broken_records)\n",
    "        speed_changes['Speed Drop'] = speed_changes['Speed Before'] - speed_changes['Speed During Break']\n",
    "        speed_changes['Speed Recovery'] = speed_changes['Speed After'] - speed_changes['Speed During Break']\n",
    "\n",
    "        # Calculate deviation from target speed\n",
    "        if 'Target Speed' in speed_changes.columns:\n",
    "            speed_changes['Deviation Before'] = speed_changes['Speed Before'] - speed_changes['Target Speed']\n",
    "            speed_changes['Deviation After'] = speed_changes['Speed After'] - speed_changes['Target Speed']\n",
    "\n",
    "        # Filter records with zero speed before\n",
    "        speed_changes = speed_changes[speed_changes['Speed Before'] != 0]\n",
    "\n",
    "        print(\"\\nSpeed Change Statistics Before and After Broken Spindles:\")\n",
    "        print(speed_changes[['Device', 'Spindle', 'Break Start Time', 'Break Duration',\n",
    "                            'Speed Before', 'Speed During Break', 'Speed After', 'Target Speed',\n",
    "                            'Speed Drop', 'Speed Recovery']].describe())\n",
    "\n",
    "        # Plot speed change boxplot (improved version)\n",
    "        plt.figure(figsize=(20, 10))\n",
    "\n",
    "        # Create subplots\n",
    "        ax1 = plt.subplot(1, 2, 1)\n",
    "        speed_changes[['Speed Before', 'Speed During Break', 'Speed After']].boxplot(ax=ax1)\n",
    "        ax1.set_title('Speed Distribution Before and After Broken Spindles (Boxplot)')\n",
    "        ax1.set_ylabel('Speed')\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        # Add target speed reference line\n",
    "        if 'Target Speed' in speed_changes.columns and not speed_changes['Target Speed'].isna().all():\n",
    "            avg_target_speed = speed_changes['Target Speed'].mean()\n",
    "            ax1.axhline(y=avg_target_speed, color='r', linestyle='--', label='Average Target Speed')\n",
    "            ax1.legend()\n",
    "\n",
    "        # Plot speed deviation boxplot\n",
    "        ax2 = plt.subplot(1, 2, 2)\n",
    "        if 'Deviation Before' in speed_changes.columns and 'Deviation After' in speed_changes.columns:\n",
    "            speed_changes[['Deviation Before', 'Deviation After']].boxplot(ax=ax2)\n",
    "            ax2.set_title('Speed Deviation from Target (Boxplot)')\n",
    "            ax2.set_ylabel('Speed Deviation')\n",
    "            ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "            ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Group statistics by device\n",
    "        print(\"\\nBroken Spindle Statistics by Device:\")\n",
    "        device_stats = speed_changes.groupby('Device').agg({\n",
    "            'Spindle': 'count',\n",
    "            'Break Duration': 'mean',\n",
    "            'Speed Drop': 'mean',\n",
    "            'Speed Recovery': 'mean',\n",
    "            'Target Speed': 'mean',\n",
    "            'Speed Before': ['mean', 'std'],\n",
    "            'Deviation Before': ['mean', 'std']\n",
    "        })\n",
    "        device_stats.columns = ['Broken Count', 'Average Duration (Minutes)', 'Average Speed Drop',\n",
    "                               'Average Speed Recovery', 'Average Target Speed',\n",
    "                               'Average Speed Before', 'Speed Before Std',\n",
    "                               'Average Deviation Before', 'Deviation Before Std']\n",
    "        print(device_stats)\n",
    "\n",
    "        # Analyze safe operating range (improved version)\n",
    "        if 'Target Speed' in speed_changes.columns and not speed_changes['Target Speed'].isna().all():\n",
    "            # Calculate global standard deviation range\n",
    "            global_speed_std = speed_changes['Speed Before'].std()\n",
    "            global_target_speed = speed_changes['Target Speed'].mean()\n",
    "            global_safe_lower = global_target_speed - 2 * global_speed_std\n",
    "            global_safe_upper = global_target_speed + 2 * global_speed_std\n",
    "\n",
    "            # Calculate standard deviation range for each device\n",
    "            safe_speed_range = speed_changes.groupby('Device').agg({\n",
    "                'Speed Before': ['mean', 'std'],\n",
    "                'Target Speed': 'mean',\n",
    "                'Deviation Before': ['mean', 'std']\n",
    "            })\n",
    "            safe_speed_range.columns = ['Average Speed Before', 'Speed Before Std',\n",
    "                                       'Average Target Speed',\n",
    "                                       'Average Deviation Before', 'Deviation Before Std']\n",
    "\n",
    "            safe_speed_range['Safe Lower Limit'] = safe_speed_range['Average Target Speed'] - 2 * safe_speed_range['Deviation Before Std']\n",
    "            safe_speed_range['Safe Upper Limit'] = safe_speed_range['Average Target Speed'] + 2 * safe_speed_range['Deviation Before Std']\n",
    "\n",
    "            # Ensure standard deviation range is reasonable\n",
    "            safe_speed_range['Safe Lower Limit'] = safe_speed_range['Safe Lower Limit'].apply(lambda x: max(x, 0))\n",
    "\n",
    "            print(\"\\nGlobal Standard Deviation Range (Based on ±2σ):\")\n",
    "            print(f\"Target Speed: {global_target_speed:.2f}\")\n",
    "            print(f\"Standard Deviation Range: {global_safe_lower:.2f} - {global_safe_upper:.2f}\")\n",
    "            print(f\"Standard Deviation: {global_speed_std:.2f}\")\n",
    "\n",
    "            print(\"\\nSuggested Standard Deviation Range by Device (Based on ±2σ):\")\n",
    "            print(safe_speed_range[['Average Target Speed', 'Deviation Before Std',\n",
    "                                  'Safe Lower Limit', 'Safe Upper Limit']])\n",
    "\n",
    "            # Plot standard deviation range chart\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            plt.errorbar(safe_speed_range.index,\n",
    "                        safe_speed_range['Average Speed Before'],\n",
    "                        yerr=2*safe_speed_range['Speed Before Std'],\n",
    "                        fmt='o', color='b', ecolor='r',\n",
    "                        capsize=5, label='Actual Speed ±2σ')\n",
    "            plt.scatter(safe_speed_range.index,\n",
    "                       safe_speed_range['Average Target Speed'],\n",
    "                       color='g', label='Target Speed')\n",
    "            plt.title('Actual Speed vs. Target Speed Comparison and Standard Deviation Range by Device')\n",
    "            plt.xlabel('Device')\n",
    "            plt.ylabel('Speed')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.legend()\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"\\nNo qualified records found before and after broken spindles\")\n",
    "else:\n",
    "    print(\"\\nNo broken spindle records found in the data\")"
   ],
   "id": "7ca5bd9276e0cdf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# 设置中文字体和正确的负号显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "# 设置全局字体大小为原来的两倍，并加粗\n",
    "plt.rcParams['font.weight'] = 'bold'  # 全局加粗\n",
    "plt.rcParams['axes.titlesize'] = 18 * 2  # 标题大小\n",
    "plt.rcParams['axes.labelsize'] = 14 * 2  # 坐标轴标签大小\n",
    "plt.rcParams['font.size'] = 12 * 2  # 字体大小\n",
    "plt.rcParams['legend.fontsize'] = 12 * 2  # 图例大小\n",
    "plt.rcParams['xtick.labelsize'] = 12 * 2  # x轴刻度标签大小\n",
    "plt.rcParams['ytick.labelsize'] = 12 * 2  # y轴刻度标签大小\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "\n",
    "# Spindle status mapping\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Started\",\n",
    "    2: \"Full tube\",\n",
    "    3: \"Broken yarn\",\n",
    "    5: \"Overheating\",\n",
    "    6: \"Stalled\",\n",
    "    7: \"Dropped line\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 1. 准备纺锤状态、速度和管满度列名（确保一对一对应）\n",
    "spindle_cols = []\n",
    "for i in range(1, 101):\n",
    "    status_col = f\"D_dw{i}\"\n",
    "    speed_col = f\"V_dw{i}\"\n",
    "    fullness_col = f\"P_dw{i}\"\n",
    "    if status_col in df.columns and speed_col in df.columns and fullness_col in df.columns:\n",
    "        spindle_cols.append((status_col, speed_col, fullness_col))\n",
    "\n",
    "# 2. 找到断纱最多的前10台设备\n",
    "device_broken_counts = defaultdict(int)\n",
    "\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    # 找到所有断纱记录并按设备分组\n",
    "    broken_mask = df[status_col] == 3\n",
    "    if broken_mask.any():\n",
    "        broken_by_device = df[broken_mask].groupby('subsystem').size()\n",
    "        for device, count in broken_by_device.items():\n",
    "            device_broken_counts[device] += count\n",
    "\n",
    "# 转换为DataFrame并排序\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Yarn Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Yarn Count', ascending=False).head(10)\n",
    "    print(\"\\nTop 10 devices with the most broken yarns:\")\n",
    "    print(top_devices)\n",
    "\n",
    "    # 3. 分析这些设备断纱前后速度变化\n",
    "    plt.figure(figsize=(20*2, 10*2), dpi=300)  # 增加DPI以提高分辨率\n",
    "    all_broken_records = []\n",
    "\n",
    "    for device_name in top_devices.index:\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        for status_col, speed_col, _ in spindle_cols:\n",
    "            # 找到该设备上的纺锤断纱记录\n",
    "            broken_mask = (device_data[status_col] == 3)\n",
    "            broken_indices = device_data[broken_mask].index\n",
    "\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # 获取所有断纱事件\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue  # 连续的断纱状态\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "\n",
    "            if current_start is not None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # 分析每个断纱事件前后的速度变化\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                # 找到断纱前的最后一条正常记录\n",
    "                prev_idx = device_data.index.get_loc(start_idx) - 1\n",
    "                while prev_idx >= 0 and device_data.iloc[prev_idx][status_col] == 3:\n",
    "                    prev_idx -= 1\n",
    "\n",
    "                # 找到断纱后的第一条正常记录\n",
    "                next_idx = device_data.index.get_loc(end_idx) + 1\n",
    "                while next_idx < len(device_data) and device_data.iloc[next_idx][status_col] == 3:\n",
    "                    next_idx += 1\n",
    "\n",
    "                if prev_idx >= 0 and next_idx < len(device_data):\n",
    "                    # 获取目标速度 R_dw22（确保与当前纺锤对应）\n",
    "                    spindle_num = status_col.split('_')[1].replace('dw', '')\n",
    "                    target_speed_col = f\"R_dw{spindle_num}\" if spindle_num == \"22\" else f\"R_dw22\"\n",
    "\n",
    "                    # 从断纱前记录中获取目标速度\n",
    "                    if target_speed_col in device_data.columns:\n",
    "                        target_speed = device_data.iloc[prev_idx][target_speed_col]\n",
    "                    else:\n",
    "                        target_speed = None\n",
    "\n",
    "                    record = {\n",
    "                        'Device': device_name,\n",
    "                        'Spindle': status_col.split('_')[1],\n",
    "                        'Break Start Time': device_data.loc[start_idx, 'created_at'],\n",
    "                        'Break End Time': device_data.loc[end_idx, 'created_at'],\n",
    "                        'Speed Before': device_data.iloc[prev_idx][speed_col],\n",
    "                        'Speed During Break': device_data.loc[start_idx, speed_col],\n",
    "                        'Speed After': device_data.iloc[next_idx][speed_col],\n",
    "                        'Target Speed': target_speed,\n",
    "                        'Break Duration': (device_data.loc[end_idx, 'created_at'] - device_data.loc[start_idx, 'created_at']).total_seconds() / 60\n",
    "                    }\n",
    "                    device_records.append(record)\n",
    "                    all_broken_records.append(record)\n",
    "\n",
    "        # 绘制典型断纱事件速度变化（每台设备最多5个样本）\n",
    "        if device_records:\n",
    "            sample_records = pd.DataFrame(device_records).sample(min(5, len(device_records)), random_state=42)\n",
    "            for _, record in sample_records.iterrows():\n",
    "                plt.plot(['Before Break', 'During Break', 'After Break'],\n",
    "                         [record['Speed Before'], record['Speed During Break'], record['Speed After']],\n",
    "                         marker='o', markersize=20 * 2, linewidth=8 * 2,  # 增加标记和线条大小\n",
    "                         markeredgecolor='blue', markeredgewidth=6,  # 增加标记边缘\n",
    "                         label=f\"{record['Device']}-{record['Spindle']}@{record['Break Start Time'].strftime('%H:%M')}\")\n",
    "\n",
    "                # 凡是存在目标速度，添加参考线\n",
    "                if pd.notna(record['Target Speed']):\n",
    "                    plt.axhline(y=record['Target Speed'], color='gray', linestyle='--', alpha=0.5, linewidth=4 * 2)\n",
    "\n",
    "                # 断纱期间速度分布圆圈圈填充颜色\n",
    "                plt.scatter(['During Break'],\n",
    "                          [record['Speed During Break']],\n",
    "                          color='red', marker='o', s=200*2, edgecolors='black', linewidths=6)  # 填充颜色红色，标记大小和边缘\n",
    "\n",
    "    # 设置绘图设置\n",
    "    plt.title('Speed Changes Before, During, and After Breaks in Top 10 Devices', fontsize=18*2, fontweight='bold')\n",
    "    plt.xlabel('Time Point', fontsize=14*2, fontweight='bold')\n",
    "    plt.ylabel('Actual Speed', fontsize=14*2, fontweight='bold')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12*2, borderaxespad=0., title_fontsize=12*2, title='Legend')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout(rect=[0, 0, 0.8, 1])  # 调整布局以适应图例放置\n",
    "    plt.show()\n",
    "\n",
    "    # 4. 计算并显示速度变化统计\n",
    "    if all_broken_records:\n",
    "        speed_changes = pd.DataFrame(all_broken_records)\n",
    "        speed_changes['Speed Decrease'] = speed_changes['Speed Before'] - speed_changes['Speed During Break']\n",
    "        speed_changes['Speed Recovery'] = speed_changes['Speed After'] - speed_changes['Speed During Break']\n",
    "\n",
    "        # 计算与目标速度的偏差\n",
    "        if 'Target Speed' in speed_changes.columns:\n",
    "            speed_changes['Speed Before Deviation'] = speed_changes['Speed Before'] - speed_changes['Target Speed']\n",
    "            speed_changes['Speed After Deviation'] = speed_changes['Speed After'] - speed_changes['Target Speed']\n",
    "\n",
    "        # 过滤掉速度为0的记录\n",
    "        speed_changes = speed_changes[speed_changes['Speed Before'] != 0]\n",
    "\n",
    "        print(\"\\nSpeed changes before, during, and after breaks:\")\n",
    "        print(speed_changes[['Device', 'Spindle', 'Break Start Time', 'Break Duration',\n",
    "                            'Speed Before', 'Speed During Break', 'Speed After', 'Target Speed',\n",
    "                            'Speed Decrease', 'Speed Recovery']].describe())\n",
    "\n",
    "        # 绘制速度变化箱线图（改进版本）\n",
    "        plt.figure(figsize=(20*2, 12*2), dpi=300)  # 增加DPI以提高分辨率\n",
    "\n",
    "        # 创建子图\n",
    "        ax1 = plt.subplot(1, 2, 1)\n",
    "        boxplot = speed_changes[['Speed Before', 'Speed During Break', 'Speed After']].boxplot(ax=ax1, widths=0.8, patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightblue', edgecolor='black', lw=12, alpha=0.8),\n",
    "            whiskerprops=dict(color='black', lw=12),\n",
    "            capprops=dict(color='black', lw=12),\n",
    "            medianprops=dict(color='black', lw=12),\n",
    "            flierprops=dict(color='red', markeredgecolor='red', markerfacecolor='red', markeredgewidth=6, markersize=20))  # 增加标记大小和标记边缘\n",
    "        ax1.set_title('Boxplot of Speed Before, During, and After Breaks', fontsize=16*2, fontweight='bold')\n",
    "        ax1.set_ylabel('Speed', fontsize=14*2, fontweight='bold')\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        # 添加目标速度参考线\n",
    "        if 'Target Speed' in speed_changes.columns and not speed_changes['Target Speed'].isna().all():\n",
    "            avg_target_speed = speed_changes['Target Speed'].mean()\n",
    "            ax1.axhline(y=avg_target_speed, color='r', linestyle='--', label='Average Target Speed', lw=4 * 2)\n",
    "            ax1.legend(fontsize=12*2, title_fontsize=12*2, title='Legend')\n",
    "\n",
    "        # 绘制速度偏差箱线图\n",
    "        ax2 = plt.subplot(1, 2, 2)\n",
    "        if 'Speed Before Deviation' in speed_changes.columns and 'Speed After Deviation' in speed_changes.columns:\n",
    "            boxplot = speed_changes[['Speed Before Deviation', 'Speed After Deviation']].boxplot(ax=ax2, widths=0.8, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightgreen', edgecolor='black', lw=12, alpha=0.8),\n",
    "                whiskerprops=dict(color='black', lw=12),\n",
    "                capprops=dict(color='black', lw=12),\n",
    "                medianprops=dict(color='black', lw=12),\n",
    "                flierprops=dict(color='orange', markeredgecolor='orange', markerfacecolor='orange', markeredgewidth=6, markersize=20))  # 增加标记大小和标记边缘\n",
    "            ax2.set_title('Boxplot of Speed Deviation Before and After Breaks', fontsize=16*2, fontweight='bold')\n",
    "            ax2.set_ylabel('Speed Deviation', fontsize=14*2, fontweight='bold')\n",
    "            ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5, lw=4 * 2)\n",
    "            ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 按设备统计断纱情况\n",
    "        print(\"\\nBreak statistics by device:\")\n",
    "        device_stats = speed_changes.groupby('Device').agg({\n",
    "            'Spindle': 'count',\n",
    "            'Break Duration': 'mean',\n",
    "            'Speed Decrease': 'mean',\n",
    "            'Speed Recovery': 'mean',\n",
    "            'Target Speed': 'mean',\n",
    "            'Speed Before': ['mean', 'std'],\n",
    "            'Speed Before Deviation': ['mean', 'std']\n",
    "        })\n",
    "        device_stats.columns = ['Break Count', 'Average Duration (minutes)', 'Average Speed Decrease',\n",
    "                               'Average Speed Recovery', 'Average Target Speed',\n",
    "                               'Average Speed Before', 'Speed Before Std Dev',\n",
    "                               'Average Speed Before Deviation', 'Speed Before Deviation Std Dev']\n",
    "        print(device_stats)\n",
    "\n",
    "        # 分析安全操作范围（改进版本）\n",
    "        if 'Target Speed' in speed_changes.columns and not speed_changes['Target Speed'].isna().all():\n",
    "            # 计算全局标准偏差范围\n",
    "            global_speed_std = speed_changes['Speed Before'].std()\n",
    "            global_target_speed = speed_changes['Target Speed'].mean()\n",
    "            global_safe_lower = global_target_speed - 2 * global_speed_std\n",
    "            global_safe_upper = global_target_speed + 2 * global_speed_std\n",
    "\n",
    "            # 计算每个设备的标准偏差范围\n",
    "            safe_speed_range = speed_changes.groupby('Device').agg({\n",
    "                'Speed Before': ['mean', 'std'],\n",
    "                'Target Speed': 'mean',\n",
    "                'Speed Before Deviation': ['mean', 'std']\n",
    "            })\n",
    "            safe_speed_range.columns = ['Average Speed Before', 'Speed Before Std Dev',\n",
    "                                       'Average Target Speed',\n",
    "                                       'Average Speed Before Deviation', 'Speed Before Deviation Std Dev']\n",
    "\n",
    "            safe_speed_range['Safe Lower Bound'] = safe_speed_range['Average Target Speed'] - 2 * safe_speed_range['Speed Before Deviation Std Dev']\n",
    "            safe_speed_range['Safe Upper Bound'] = safe_speed_range['Average Target Speed'] + 2 * safe_speed_range['Speed Before Deviation Std Dev']\n",
    "\n",
    "            # 确保标准偏差范围合理\n",
    "            safe_speed_range['Safe Lower Bound'] = safe_speed_range['Safe Lower Bound'].apply(lambda x: max(x, 0))\n",
    "\n",
    "            print(\"\\nGlobal speed range based on ±2σ:\")\n",
    "            print(f\"Target Speed: {global_target_speed:.2f}\")\n",
    "            print(f\"Speed Range: {global_safe_lower:.2f} - {global_safe_upper:.2f}\")\n",
    "            print(f\"Standard Deviation: {global_speed_std:.2f}\")\n",
    "\n",
    "            print(\"\\nRecommended speed ranges based on ±2σ for each device:\")\n",
    "            print(safe_speed_range[['Average Target Speed', 'Speed Before Deviation Std Dev',\n",
    "                                  'Safe Lower Bound', 'Safe Upper Bound']])\n",
    "\n",
    "            # 绘制标准偏差范围图表\n",
    "            plt.figure(figsize=(20*2, 12*2), dpi=300)  # 增加DPI以提高分辨率\n",
    "            plt.errorbar(safe_speed_range.index,\n",
    "                         safe_speed_range['Average Speed Before'],\n",
    "                         yerr=2*safe_speed_range['Speed Before Std Dev'],\n",
    "                         fmt='o', color='b', ecolor='r',\n",
    "                         capsize=15*2, label='Actual Speed ±2σ', lw=4, capthick=4)\n",
    "            plt.scatter(safe_speed_range.index,\n",
    "                        safe_speed_range['Average Target Speed'],\n",
    "                        color='g', label='Target Speed', s=200*2)\n",
    "            plt.title('Comparison of Actual Speed and Target Speed with ±2σ Range by Device', fontsize=20*2, fontweight='bold')\n",
    "            plt.xlabel('Device', fontsize=18*2, fontweight='bold')\n",
    "            plt.ylabel('Speed', fontsize=18*2, fontweight='bold')\n",
    "            plt.xticks(rotation=45, fontsize=14*2)\n",
    "            plt.legend(fontsize=14*2, title_fontsize=14*2, title='Legend')\n",
    "            plt.grid(True, linestyle='--', alpha=0.9)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"\\nNo break records with valid speed changes found\")\n",
    "else:\n",
    "    print(\"\\nNo break records found in the data\")"
   ],
   "id": "338b38201f376be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set English display\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "\n",
    "# Specify 6 target devices\n",
    "target_devices = ['NX16-L102', 'NX23-R103', 'NX24-L102', 'NX48-L102', 'NX71-R103', 'NX72-L102']\n",
    "\n",
    "# Store statistical results\n",
    "results = {}\n",
    "\n",
    "for device in target_devices:\n",
    "    # Filter device data\n",
    "    device_data = df[df['subsystem'] == device]\n",
    "    valid_speeds = []\n",
    "\n",
    "    for i in range(1, 101):\n",
    "        status_col = f\"D_dw{i}\"\n",
    "        speed_col = f\"V_dw{i}\"\n",
    "\n",
    "        if status_col in device_data.columns and speed_col in device_data.columns:\n",
    "            # Filter valid data: status 1 or 2 and speed > 0\n",
    "            valid = device_data[(device_data[status_col].isin([1, 2])) &\n",
    "                               (device_data[speed_col] > 0)][speed_col]\n",
    "            valid_speeds.extend(valid.dropna().tolist())\n",
    "\n",
    "    if valid_speeds:\n",
    "        # Calculate statistics\n",
    "        median = np.median(valid_speeds)\n",
    "        q1 = np.quantile(valid_speeds, 0.25)\n",
    "        q3 = np.quantile(valid_speeds, 0.75)\n",
    "        std = np.std(valid_speeds)\n",
    "\n",
    "        results[device] = {\n",
    "            'Median': median,\n",
    "            'Q1': q1,\n",
    "            'Q3': q3,\n",
    "            'Std Dev': std,\n",
    "            'IQR Range': (q1, q3),  # Using IQR as safety range\n",
    "            'Std Dev Range': (max(0, median - 1*std), median + 1*std)  # ±1σ range\n",
    "        }\n",
    "\n",
    "# Create comparison plot\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Prepare plot data\n",
    "medians = [results[dev]['Median'] for dev in target_devices]\n",
    "quartile_ranges = [results[dev]['IQR Range'] for dev in target_devices]\n",
    "std_ranges = [results[dev]['Std Dev Range'] for dev in target_devices]\n",
    "\n",
    "# Calculate error ranges\n",
    "quartile_lower = [medians[i] - quartile_ranges[i][0] for i in range(len(medians))]\n",
    "quartile_upper = [quartile_ranges[i][1] - medians[i] for i in range(len(medians))]\n",
    "quartile_error = [quartile_lower, quartile_upper]\n",
    "\n",
    "std_lower = [medians[i] - std_ranges[i][0] for i in range(len(medians))]\n",
    "std_upper = [std_ranges[i][1] - medians[i] for i in range(len(medians))]\n",
    "std_error = [std_lower, std_upper]\n",
    "\n",
    "# Plot ±1σ range (drawn first, in background)\n",
    "x_pos = np.arange(len(target_devices)) + 1\n",
    "plt.errorbar(x_pos, medians,\n",
    "             yerr=std_error,\n",
    "             fmt='o', color='darkblue', ecolor='lightblue', elinewidth=4,\n",
    "             capsize=6, markersize=8, alpha=0.7, label='±1 Std Dev Range')\n",
    "\n",
    "# Plot IQR range (drawn second, in foreground)\n",
    "plt.errorbar(x_pos, medians,\n",
    "             yerr=quartile_error,\n",
    "             fmt='o', color='darkred', ecolor='lightcoral', elinewidth=6,\n",
    "             capsize=7, markersize=10, label='IQR Range (Q1-Q3)')\n",
    "\n",
    "# Add statistical markers\n",
    "for i, dev in enumerate(target_devices):\n",
    "    # Median marker\n",
    "    plt.text(x_pos[i], medians[i]+0.05*medians[i], f\"{medians[i]:.1f}\",\n",
    "             ha='center', va='bottom', fontsize=14, color='darkgreen', weight='bold')\n",
    "    # Std Dev marker\n",
    "    plt.text(x_pos[i], std_ranges[i][0]-0.1*medians[i], f\"σ={results[dev]['Std Dev']:.1f}\",\n",
    "             ha='center', va='top', fontsize=12, color='blue', weight='bold')\n",
    "\n",
    "# Set plot properties\n",
    "plt.xticks(x_pos, target_devices, rotation=15, fontsize=14, weight='bold')\n",
    "plt.title('Actual vs Target Speed Comparison (±1σ Range vs IQR Range)',\n",
    "          fontsize=18, pad=20, weight='bold')\n",
    "plt.xlabel('Device ID', fontsize=16, weight='bold')\n",
    "plt.ylabel('Speed Value', fontsize=16, weight='bold')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "\n",
    "# Keep legend size but make it bold\n",
    "plt.legend(loc='upper right', fontsize=12, framealpha=1, edgecolor='black')\n",
    "\n",
    "# Adjust y-axis range\n",
    "y_min = min([r['Std Dev Range'][0] for r in results.values()]) * 0.95\n",
    "y_max = max([r['Std Dev Range'][1] for r in results.values()]) * 1.05\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Output statistical results\n",
    "print(\"Speed Statistical Analysis and Range Comparison:\\n\")\n",
    "for dev in target_devices:\n",
    "    stats = results[dev]\n",
    "    print(f\"Device {dev}:\")\n",
    "    print(f\"• Median Speed: {stats['Median']:.2f}\")\n",
    "    print(f\"• Standard Deviation: {stats['Std Dev']:.2f} (±1σ Range: {stats['Std Dev Range'][0]:.2f}~{stats['Std Dev Range'][1]:.2f})\")\n",
    "    print(f\"• IQR Range: {stats['IQR Range'][0]:.2f} ~ {stats['IQR Range'][1]:.2f}\")\n",
    "    print(f\"• Range Comparison: IQR range is {(stats['Std Dev Range'][1]-stats['Std Dev Range'][0] - (stats['IQR Range'][1]-stats['IQR Range'][0]))/(stats['Std Dev Range'][1]-stats['Std Dev Range'][0])*100:.1f}% narrower than ±1σ range\")\n",
    "    print(\"• Note: Blue range (±1σ) shows actual variation, red range (IQR) is suggested safety bounds\\n\")"
   ],
   "id": "50282684dac44e5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set English display with larger, bolder fonts\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['axes.titlesize'] = 28  # Increased title font size\n",
    "plt.rcParams['axes.labelsize'] = 26  # Increased axis label font size\n",
    "plt.rcParams['xtick.labelsize'] = 24  # Increased X-axis tick font size\n",
    "plt.rcParams['ytick.labelsize'] = 24  # Increased Y-axis tick font size\n",
    "plt.rcParams['legend.fontsize'] = 22  # Increased legend font size\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "\n",
    "# Specify 6 target devices\n",
    "target_devices = ['NX16-L102', 'NX23-R103', 'NX24-L102', 'NX48-L102', 'NX71-R103', 'NX72-L102']\n",
    "\n",
    "# Store statistical results\n",
    "results = {}\n",
    "\n",
    "for device in target_devices:\n",
    "    # Filter device data\n",
    "    device_data = df[df['subsystem'] == device]\n",
    "    valid_speeds = []\n",
    "\n",
    "    for i in range(1, 101):\n",
    "        status_col = f\"D_dw{i}\"\n",
    "        speed_col = f\"V_dw{i}\"\n",
    "\n",
    "        if status_col in device_data.columns and speed_col in device_data.columns:\n",
    "            # Filter valid data: status 1 or 2 and speed > 0\n",
    "            valid = device_data[(device_data[status_col].isin([1, 2])) &\n",
    "                               (device_data[speed_col] > 0)][speed_col]\n",
    "            valid_speeds.extend(valid.dropna().tolist())\n",
    "\n",
    "    if valid_speeds:\n",
    "        # Calculate statistics\n",
    "        median = np.median(valid_speeds)\n",
    "        q1 = np.quantile(valid_speeds, 0.25)\n",
    "        q3 = np.quantile(valid_speeds, 0.75)\n",
    "        std = np.std(valid_speeds)\n",
    "\n",
    "        results[device] = {\n",
    "            'Median': median,\n",
    "            'Q1': q1,\n",
    "            'Q3': q3,\n",
    "            'Std Dev': std,\n",
    "            'IQR Range': (q1, q3),  # Using IQR as safety range\n",
    "            'Std Dev Range': (max(0, median - 1*std), median + 1*std)  # ±1σ range\n",
    "        }\n",
    "\n",
    "# Create comparison plot with larger figure size\n",
    "plt.figure(figsize=(20, 12), facecolor='white')\n",
    "\n",
    "# Set axes facecolor to white\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# Prepare plot data\n",
    "medians = [results[dev]['Median'] for dev in target_devices]\n",
    "quartile_ranges = [results[dev]['IQR Range'] for dev in target_devices]\n",
    "std_ranges = [results[dev]['Std Dev Range'] for dev in target_devices]\n",
    "\n",
    "# Calculate error ranges\n",
    "quartile_lower = [medians[i] - quartile_ranges[i][0] for i in range(len(medians))]\n",
    "quartile_upper = [quartile_ranges[i][1] - medians[i] for i in range(len(medians))]\n",
    "quartile_error = [quartile_lower, quartile_upper]\n",
    "\n",
    "std_lower = [medians[i] - std_ranges[i][0] for i in range(len(medians))]\n",
    "std_upper = [std_ranges[i][1] - medians[i] for i in range(len(medians))]\n",
    "std_error = [std_lower, std_upper]\n",
    "\n",
    "# Plot ±1σ range (drawn first, in background)\n",
    "x_pos = np.arange(len(target_devices)) + 1\n",
    "plt.errorbar(x_pos, medians,\n",
    "             yerr=std_error,\n",
    "             fmt='o', color='darkblue', ecolor='lightblue', elinewidth=24,\n",
    "             capsize=16, markersize=12, alpha=0.7, label='±1 Standard Deviation Range',\n",
    "             markeredgewidth=3)\n",
    "\n",
    "# Plot IQR range (drawn second, in foreground)\n",
    "plt.errorbar(x_pos, medians,\n",
    "             yerr=quartile_error,\n",
    "             fmt='o', color='darkred', ecolor='lightcoral', elinewidth=32,\n",
    "             capsize=18, markersize=14, label='Interquartile Range (Q1-Q3)',\n",
    "             markeredgewidth=3)\n",
    "\n",
    "# Add statistical markers inside the plot area with adjusted positions\n",
    "for i, dev in enumerate(target_devices):\n",
    "    plt.text(x_pos[i], std_ranges[i][0] + 0.02*medians[i], f\"σ={results[dev]['Std Dev']:.1f}\",\n",
    "         ha='center', va='bottom', fontsize=24, color='black', weight='bold',\n",
    "         bbox=dict(facecolor='white', edgecolor='blue', boxstyle='round,pad=0.5', linewidth=6))\n",
    "    plt.text(x_pos[i], medians[i] - 0.05*medians[i], f\"{medians[i]:.1f}\",\n",
    "             ha='center', va='top', fontsize=24, color='white', weight='bold',\n",
    "             bbox=dict(facecolor='darkgreen', edgecolor='white', boxstyle='round,pad=0.5', linewidth=6))\n",
    "\n",
    "# Set plot properties with thicker fonts and lines\n",
    "plt.xticks(x_pos, target_devices, rotation=15, fontsize=24, weight='bold')\n",
    "plt.yticks(fontsize=24, weight='bold')\n",
    "plt.title('Speed Comparison with Statistical Ranges(Top 6 devices)',  # Simplified title\n",
    "          fontsize=30, pad=25, weight='bold')  # Increased title font size\n",
    "plt.xlabel('Device Identification', fontsize=28, weight='bold', labelpad=15)  # More descriptive label\n",
    "plt.ylabel('Measured Speed (units)', fontsize=28, weight='bold', labelpad=15)  # Added units placeholder\n",
    "\n",
    "# Enhanced grid lines (dashed and more visible)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7, linewidth=4, color='gray')\n",
    "plt.grid(axis='x', linestyle=':', alpha=0.5, linewidth=3, color='lightgray')\n",
    "\n",
    "# Enhanced legend (even thicker lines)\n",
    "legend = plt.legend(loc='upper right', fontsize=22, framealpha=1,\n",
    "                   edgecolor='black', facecolor='white')\n",
    "legend.get_frame().set_linewidth(6)\n",
    "for line in legend.get_lines():\n",
    "    line.set_linewidth(16)\n",
    "\n",
    "# Adjust y-axis range to ensure all data is visible\n",
    "y_min = min([r['Std Dev Range'][0] for r in results.values()]) * 0.95\n",
    "y_max = max([r['Std Dev Range'][1] for r in results.values()]) * 1.05\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "# Make spines (axes lines) WHITE and very thick\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color('white')\n",
    "    spine.set_linewidth(16)\n",
    "\n",
    "# Add horizontal dashed lines at major y-ticks for better readability\n",
    "ax.yaxis.grid(True, which='major', linestyle='--', linewidth=6, alpha=0.7, color='gray')\n",
    "\n",
    "# Make all text elements bold and increase contrast\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontweight('bold')\n",
    "    item.set_color('black')  # Ensure high contrast\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f51caf2769f24530",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 创建对比图，并设置较大的图像尺寸和更高的分辨率\n",
    "plt.figure(figsize=(20, 12), facecolor='white', dpi=300)\n",
    "\n",
    "# 设置坐标轴背景为白色\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# 准备绘图数据\n",
    "medians = [results[dev]['Median'] for dev in target_devices]\n",
    "quartile_ranges = [results[dev]['IQR Range'] for dev in target_devices]\n",
    "std_ranges = [results[dev]['Std Dev Range'] for dev in target_devices]\n",
    "\n",
    "# 计算误差范围\n",
    "quartile_lower = [medians[i] - quartile_ranges[i][0] for i in range(len(medians))]\n",
    "quartile_upper = [quartile_ranges[i][1] - medians[i] for i in range(len(medians))]\n",
    "quartile_error = [quartile_lower, quartile_upper]\n",
    "\n",
    "std_lower = [medians[i] - std_ranges[i][0] for i in range(len(medians))]\n",
    "std_upper = [std_ranges[i][1] - medians[i] for i in range(len(medians))]\n",
    "std_error = [std_lower, std_upper]\n",
    "\n",
    "# 绘制 ±1σ 范围（先绘制，在背景中）\n",
    "x_pos = np.arange(len(target_devices)) + 1\n",
    "plt.errorbar(x_pos, medians,\n",
    "             yerr=std_error,\n",
    "             fmt='o', color='darkblue', ecolor='lightblue', elinewidth=24,\n",
    "             capsize=16, markersize=12, alpha=0.7, label='±1 Standard Deviation Range',\n",
    "             markeredgewidth=3)\n",
    "\n",
    "# 绘制 IQR 范围（后绘制，在前景中）\n",
    "plt.errorbar(x_pos, medians,\n",
    "             yerr=quartile_error,\n",
    "             fmt='o', color='darkred', ecolor='lightcoral', elinewidth=32,\n",
    "             capsize=18, markersize=14, label='Interquartile Range',\n",
    "             markeredgewidth=3)\n",
    "\n",
    "# 在绘图区域内添加统计标记，并调整位置\n",
    "for i, dev in enumerate(target_devices):\n",
    "    plt.text(x_pos[i], std_ranges[i][0] + 0.02*medians[i], f\"σ={results[dev]['Std Dev']:.1f}\",\n",
    "         ha='center', va='bottom', fontsize=28, color='black', weight='bold',\n",
    "         bbox=dict(facecolor='white', edgecolor='blue', boxstyle='round,pad=0.5', linewidth=6))\n",
    "    plt.text(x_pos[i], medians[i] - 0.05*medians[i], f\"{medians[i]:.1f}\",\n",
    "             ha='center', va='top', fontsize=28, color='white', weight='bold',\n",
    "             bbox=dict(facecolor='darkgreen', edgecolor='white', boxstyle='round,pad=0.5', linewidth=6))\n",
    "\n",
    "# 定义新的设备名称\n",
    "new_device_names = ['D16-L1', 'D23-R1', 'D24-L1', 'D48-L1', 'D71-R1', 'D72-L1']\n",
    "\n",
    "# 设置绘图属性，包括更粗的字体和线条\n",
    "plt.xticks(x_pos, new_device_names, rotation=15, fontsize=28, weight='bold')  # 使用新的设备名称\n",
    "plt.yticks(fontsize=28, weight='bold')\n",
    "plt.title('Speed Comparison with Statistical Ranges(Top 6 devices)',  # 简化的标题\n",
    "          fontsize=36, pad=25, weight='bold')  # 增加标题字体大小\n",
    "plt.xlabel('Device Identification', fontsize=32, weight='bold', labelpad=15)  # 更详细的标签\n",
    "plt.ylabel('Measured Speed (units)', fontsize=32, weight='bold', labelpad=15)  # 添加单位占位符\n",
    "\n",
    "# 增强网格线（虚线且更可见）\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7, linewidth=4, color='gray')\n",
    "plt.grid(axis='x', linestyle=':', alpha=0.5, linewidth=3, color='lightgray')\n",
    "\n",
    "# 增强图例（更粗的线条）\n",
    "legend = plt.legend(loc='upper right', fontsize=28, framealpha=1,\n",
    "                   edgecolor='black', facecolor='white')\n",
    "legend.get_frame().set_linewidth(6)\n",
    "for text in legend.get_texts():\n",
    "    text.set_fontsize(28)\n",
    "    text.set_fontweight('bold')\n",
    "for line in legend.get_lines():\n",
    "    line.set_linewidth(16)\n",
    "\n",
    "# 调整 Y 轴范围以确保所有数据可见\n",
    "y_min = min([r['Std Dev Range'][0] for r in results.values()]) * 0.95\n",
    "y_max = max([r['Std Dev Range'][1] for r in results.values()]) * 1.05\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "# 设置坐标轴线为白色且非常粗\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color('white')\n",
    "    spine.set_linewidth(16)\n",
    "\n",
    "# 在主要 Y 刻度处添加水平虚线以提高可读性\n",
    "ax.yaxis.grid(True, which='major', linestyle='--', linewidth=6, alpha=0.7, color='gray')\n",
    "\n",
    "# 使所有文本元素加粗并增加对比度\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontweight('bold')\n",
    "    item.set_color('black')  # 确保高对比度\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f7872cfab775bbc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 设置全局样式参数\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',  # 所有文本加粗\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.edgecolor': 'black',\n",
    "    'axes.linewidth': 4,  # 更粗的坐标轴线\n",
    "    'xtick.color': 'black',\n",
    "    'ytick.color': 'black',\n",
    "    'xtick.major.width': 4,\n",
    "    'ytick.major.width': 4,\n",
    "    'xtick.major.size': 10,\n",
    "    'ytick.major.size': 10,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'savefig.facecolor': 'white',\n",
    "    'figure.dpi': 300,  # 更高的分辨率\n",
    "    'axes.labelsize': 24,  # 更大的基础字体大小\n",
    "    'axes.titlesize': 28,\n",
    "    'xtick.labelsize': 22,\n",
    "    'ytick.labelsize': 22,\n",
    "    'legend.fontsize': 20,\n",
    "    'legend.title_fontsize': 22\n",
    "})\n",
    "\n",
    "# 创建对比图，并设置较大的图像尺寸和更高的分辨率\n",
    "plt.figure(figsize=(20, 12), facecolor='white', dpi=300)\n",
    "\n",
    "# 设置坐标轴背景为白色\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# 准备绘图数据\n",
    "medians = [results[dev]['Median'] for dev in target_devices]\n",
    "quartile_ranges = [results[dev]['IQR Range'] for dev in target_devices]\n",
    "std_ranges = [results[dev]['Std Dev Range'] for dev in target_devices]\n",
    "\n",
    "# 计算误差范围\n",
    "quartile_lower = [medians[i] - quartile_ranges[i][0] for i in range(len(medians))]\n",
    "quartile_upper = [quartile_ranges[i][1] - medians[i] for i in range(len(medians))]\n",
    "quartile_error = [quartile_lower, quartile_upper]\n",
    "\n",
    "std_lower = [medians[i] - std_ranges[i][0] for i in range(len(medians))]\n",
    "std_upper = [std_ranges[i][1] - medians[i] for i in range(len(medians))]\n",
    "std_error = [std_lower, std_upper]\n",
    "\n",
    "# 绘制 ±1σ 范围（先绘制，在背景中）\n",
    "x_pos = np.arange(len(target_devices)) + 1\n",
    "plt.errorbar(x_pos, medians,\n",
    "             yerr=std_error,\n",
    "             fmt='o', color='darkblue', ecolor='lightblue', elinewidth=24,\n",
    "             capsize=16, markersize=12, alpha=0.7, label='±1 Standard Deviation Range',\n",
    "             markeredgewidth=3)\n",
    "\n",
    "# 绘制 IQR 范围（后绘制，在前景中）\n",
    "plt.errorbar(x_pos, medians,\n",
    "             yerr=quartile_error,\n",
    "             fmt='o', color='darkred', ecolor='lightcoral', elinewidth=32,\n",
    "             capsize=18, markersize=14, label='Interquartile Range',\n",
    "             markeredgewidth=3)\n",
    "\n",
    "# 在绘图区域内添加统计标记，并调整位置\n",
    "for i, dev in enumerate(target_devices):\n",
    "    plt.text(x_pos[i], std_ranges[i][0] + 0.02*medians[i], f\"σ={results[dev]['Std Dev']:.1f}\",\n",
    "         ha='center', va='bottom', fontsize=28, color='black', weight='bold',\n",
    "         bbox=dict(facecolor='white', edgecolor='blue', boxstyle='round,pad=0.5', linewidth=6))\n",
    "    plt.text(x_pos[i], medians[i] - 0.05*medians[i], f\"{medians[i]:.1f}\",\n",
    "             ha='center', va='top', fontsize=28, color='white', weight='bold',\n",
    "             bbox=dict(facecolor='darkgreen', edgecolor='white', boxstyle='round,pad=0.5', linewidth=6))\n",
    "\n",
    "# 定义新的设备名称\n",
    "new_device_names = ['D16-L1', 'D23-R1', 'D24-L1', 'D48-L1', 'D71-R1', 'D72-L1']\n",
    "\n",
    "# 设置绘图属性，包括更粗的字体和线条\n",
    "plt.xticks(x_pos, new_device_names, rotation=15, fontsize=30, weight='bold')  # 使用新的设备名称\n",
    "plt.yticks(fontsize=30, weight='bold')\n",
    "# plt.title('Speed Comparison with Statistical Ranges(Top 6 devices)',  # 简化的标题\n",
    "#           fontsize=36, pad=25, weight='bold')  # 增加标题字体大小\n",
    "plt.xlabel('Device Identification', fontsize=40, weight='bold', labelpad=15)  # 更详细的标签\n",
    "plt.ylabel('Measured Speed (units)', fontsize=40, weight='bold', labelpad=15)  # 添加单位占位符\n",
    "\n",
    "# 增强网格线（虚线且更可见）\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7, linewidth=4, color='gray')\n",
    "plt.grid(axis='x', linestyle=':', alpha=0.5, linewidth=3, color='lightgray')\n",
    "\n",
    "# 增强图例（更粗的线条）\n",
    "legend = plt.legend(loc='upper right', fontsize=28, framealpha=1,\n",
    "                   edgecolor='black', facecolor='white')\n",
    "legend.get_frame().set_linewidth(6)\n",
    "for text in legend.get_texts():\n",
    "    text.set_fontsize(28)\n",
    "    text.set_fontweight('bold')\n",
    "for line in legend.get_lines():\n",
    "    line.set_linewidth(16)\n",
    "\n",
    "# 调整 Y 轴范围以确保所有数据可见\n",
    "y_min = min([r['Std Dev Range'][0] for r in results.values()]) * 0.95\n",
    "y_max = max([r['Std Dev Range'][1] for r in results.values()]) * 1.05\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "# 设置坐标轴线为白色且非常粗\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color('white')\n",
    "    spine.set_linewidth(16)\n",
    "\n",
    "# 在主要 Y 刻度处添加水平虚线以提高可读性\n",
    "ax.yaxis.grid(True, which='major', linestyle='--', linewidth=6, alpha=0.7, color='gray')\n",
    "\n",
    "# 使所有文本元素加粗并增加对比度\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontweight('bold')\n",
    "    item.set_color('black')  # 确保高对比度\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "384854978d8e46b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 断纱与卷装程度交点分析\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.optimize import brentq\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "# 锭位状态映射\n",
    "status_mapping = {\n",
    "    0: \"停锭\",\n",
    "    1: \"启动\",\n",
    "    2: \"满管\",\n",
    "    3: \"断纱\",\n",
    "    5: \"过热\",\n",
    "    6: \"失速\",\n",
    "    7: \"掉线\",\n",
    "    11: \"异常\"\n",
    "}\n",
    "\n",
    "# 1. 准备锭位状态、速度和卷装程度的列名（确保一一对应）\n",
    "spindle_cols = []\n",
    "for i in range(1, 101):\n",
    "    status_col = f\"D_dw{i}\"\n",
    "    speed_col = f\"V_dw{i}\"\n",
    "    fullness_col = f\"P_dw{i}\"\n",
    "    if status_col in df.columns and speed_col in df.columns and fullness_col in df.columns:\n",
    "        spindle_cols.append((status_col, speed_col, fullness_col))\n",
    "\n",
    "# 2. 找出断纱最多的前十台设备\n",
    "device_broken_counts = defaultdict(int)\n",
    "\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    # 找出所有断纱记录并按设备分组计数\n",
    "    broken_mask = df[status_col] == 3\n",
    "    if broken_mask.any():\n",
    "        broken_by_device = df[broken_mask].groupby('subsystem').size()\n",
    "        for device, count in broken_by_device.items():\n",
    "            device_broken_counts[device] += count\n",
    "\n",
    "# 转换为DataFrame并排序\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['断纱次数'])\n",
    "    top_devices = top_devices.sort_values('断纱次数', ascending=False).head(10)\n",
    "    print(\"\\n断纱次数最多的前十台设备:\")\n",
    "    print(top_devices)\n",
    "\n",
    "    # 3. 分析这些设备的断纱前后卷装程度变化\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    all_broken_records = []\n",
    "\n",
    "    # 存储所有设备的正常和断纱状态卷装程度\n",
    "    all_normal_fullness = []\n",
    "    all_broken_fullness = []\n",
    "\n",
    "    for device_name in top_devices.index:\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        # 收集该设备的正常和断纱状态卷装程度\n",
    "        device_normal_fullness = []\n",
    "        device_broken_fullness = []\n",
    "\n",
    "        for status_col, _, fullness_col in spindle_cols:\n",
    "            # 正常状态(状态1:启动)\n",
    "            normal_mask = (device_data[status_col] == 1)\n",
    "            device_normal_fullness.extend(device_data[normal_mask][fullness_col].dropna().values)\n",
    "\n",
    "            # 断纱状态(状态3:断纱)\n",
    "            broken_mask = (device_data[status_col] == 3)\n",
    "            device_broken_fullness.extend(device_data[broken_mask][fullness_col].dropna().values)\n",
    "\n",
    "            # 找出该设备该锭位的所有断纱记录\n",
    "            broken_indices = device_data[broken_mask].index\n",
    "\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # 获取所有断纱事件的开始和结束\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue  # 连续断纱状态\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "\n",
    "            if current_start is not None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # 对每个断纱事件分析前后卷装程度\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                # 获取断纱前的最后一个非断纱记录\n",
    "                prev_idx = device_data.index.get_loc(start_idx) - 1\n",
    "                while prev_idx >= 0 and device_data.iloc[prev_idx][status_col] == 3:\n",
    "                    prev_idx -= 1\n",
    "\n",
    "                # 获取断纱后的第一个非断纱记录\n",
    "                next_idx = device_data.index.get_loc(end_idx) + 1\n",
    "                while next_idx < len(device_data) and device_data.iloc[next_idx][status_col] == 3:\n",
    "                    next_idx += 1\n",
    "\n",
    "                if prev_idx >= 0 and next_idx < len(device_data):\n",
    "                    record = {\n",
    "                        '设备': device_name,\n",
    "                        '锭位': status_col.split('_')[1],\n",
    "                        '断纱开始时间': device_data.loc[start_idx, 'created_at'],\n",
    "                        '断纱结束时间': device_data.loc[end_idx, 'created_at'],\n",
    "                        '前卷装程度': device_data.iloc[prev_idx][fullness_col],\n",
    "                        '断纱时卷装程度': device_data.loc[start_idx, fullness_col],\n",
    "                        '后卷装程度': device_data.iloc[next_idx][fullness_col],\n",
    "                        '断纱持续时间': (device_data.loc[end_idx, 'created_at'] - device_data.loc[start_idx, 'created_at']).total_seconds() / 60\n",
    "                    }\n",
    "                    device_records.append(record)\n",
    "                    all_broken_records.append(record)\n",
    "\n",
    "        # 绘制该设备的典型断纱事件（最多5个）\n",
    "        if device_records:\n",
    "            sample_records = pd.DataFrame(device_records).sample(min(5, len(device_records)), random_state=42)\n",
    "            for _, record in sample_records.iterrows():\n",
    "                plt.plot(['断纱前', '断纱时', '断纱后'],\n",
    "                         [record['前卷装程度'], record['断纱时卷装程度'], record['后卷装程度']],\n",
    "                         marker='o',\n",
    "                         label=f\"{record['设备']}-{record['锭位']}@{record['断纱开始时间'].strftime('%H:%M')}\")\n",
    "\n",
    "        # 添加到全局集合\n",
    "        all_normal_fullness.extend(device_normal_fullness)\n",
    "        all_broken_fullness.extend(device_broken_fullness)\n",
    "\n",
    "    # 设置图表\n",
    "    plt.axhline(y=100, color='gray', linestyle='--', label='目标卷装程度(100%)')\n",
    "    plt.title('前十台设备典型断纱事件前后卷装程度变化')\n",
    "    plt.xlabel('时间点')\n",
    "    plt.ylabel('卷装程度(%)')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 4. 计算安全操作阈值\n",
    "    if all_normal_fullness and all_broken_fullness:\n",
    "        # 转换为numpy数组并移除无效值\n",
    "        normal_data = np.array(all_normal_fullness)\n",
    "        normal_data = normal_data[~np.isnan(normal_data)]\n",
    "\n",
    "        broken_data = np.array(all_broken_fullness)\n",
    "        broken_data = broken_data[~np.isnan(broken_data)]\n",
    "\n",
    "        # 计算核密度估计\n",
    "        kde_normal = gaussian_kde(normal_data)\n",
    "        kde_broken = gaussian_kde(broken_data)\n",
    "\n",
    "        # 定义寻找交点的函数\n",
    "        def find_intersection(x):\n",
    "            return kde_normal(x) - kde_broken(x)\n",
    "\n",
    "        # 寻找交点\n",
    "        min_val = min(min(normal_data), min(broken_data))\n",
    "        max_val = max(max(normal_data), max(broken_data))\n",
    "\n",
    "        try:\n",
    "            intersection = brentq(find_intersection, min_val, max_val)\n",
    "            print(f\"\\n安全操作阈值(正常与断纱分布交点): {intersection:.2f}%\")\n",
    "\n",
    "            # 绘制分布曲线和交点\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            x = np.linspace(min_val, max_val, 1000)\n",
    "\n",
    "            plt.plot(x, kde_normal(x), label='正常状态分布')\n",
    "            plt.plot(x, kde_broken(x), label='断纱状态分布')\n",
    "            plt.axvline(x=intersection, color='red', linestyle='--', label=f'交点: {intersection:.2f}%')\n",
    "\n",
    "            plt.title('正常状态与断纱状态的卷装程度分布')\n",
    "            plt.xlabel('卷装程度(%)')\n",
    "            plt.ylabel('概率密度')\n",
    "            plt.legend()\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.show()\n",
    "\n",
    "            # 计算安全区间\n",
    "            safe_min = np.percentile(normal_data, 5)  # 取正常状态的下5%分位数作为下限\n",
    "            safe_max = 100  # 上限为100%\n",
    "            print(f\"\\n推荐的安全操作区间: {safe_min:.2f}% 至 {safe_max:.2f}%\")\n",
    "\n",
    "            # 计算当前数据在安全区间内的比例\n",
    "            in_safe_zone = np.sum((normal_data >= safe_min) & (normal_data <= safe_max)) / len(normal_data) * 100\n",
    "            print(f\"正常状态数据在安全区间内的比例: {in_safe_zone:.2f}%\")\n",
    "\n",
    "            # 计算断纱数据在安全区间外的比例\n",
    "            broken_out_safe = np.sum((broken_data < safe_min) | (broken_data > safe_max)) / len(broken_data) * 100\n",
    "            print(f\"断纱状态数据在安全区间外的比例: {broken_out_safe:.2f}%\")\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"\\n无法找到正常与断纱状态的分布交点\")\n",
    "            intersection = None\n",
    "\n",
    "    # 5. 计算并显示卷装程度变化统计信息\n",
    "    if all_broken_records:\n",
    "        fullness_changes = pd.DataFrame(all_broken_records)\n",
    "        fullness_changes['卷装程度下降量'] = fullness_changes['前卷装程度'] - fullness_changes['断纱时卷装程度']\n",
    "        fullness_changes['卷装程度恢复量'] = fullness_changes['后卷装程度'] - fullness_changes['断纱时卷装程度']\n",
    "\n",
    "        print(\"\\n断纱前后卷装程度变化统计:\")\n",
    "        print(fullness_changes[['设备', '锭位', '断纱开始时间', '断纱持续时间',\n",
    "                              '前卷装程度', '断纱时卷装程度', '后卷装程度',\n",
    "                              '卷装程度下降量', '卷装程度恢复量']].describe())\n",
    "\n",
    "        # 绘制卷装程度变化箱线图\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        fullness_changes[['前卷装程度', '断纱时卷装程度', '后卷装程度']].boxplot()\n",
    "        plt.title('断纱前后卷装程度分布')\n",
    "        plt.ylabel('卷装程度(%)')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "        # 按设备分组统计\n",
    "        print(\"\\n按设备分组的断纱与卷装程度统计:\")\n",
    "        device_stats = fullness_changes.groupby('设备').agg({\n",
    "            '锭位': 'count',\n",
    "            '断纱持续时间': 'mean',\n",
    "            '卷装程度下降量': 'mean',\n",
    "            '卷装程度恢复量': 'mean'\n",
    "        }).rename(columns={'锭位': '断纱次数', '断纱持续时间': '平均持续时间(分钟)'})\n",
    "        print(device_stats)\n",
    "\n",
    "        # 6. 分析断纱与卷装程度的关联性\n",
    "        print(\"\\n断纱与卷装程度的关联性分析:\")\n",
    "        # 计算断纱前卷装程度的分布\n",
    "        print(\"\\n断纱前卷装程度分布:\")\n",
    "        print(fullness_changes['前卷装程度'].describe())\n",
    "\n",
    "        # 计算断纱前卷装程度低于交点的比例\n",
    "        if intersection:\n",
    "            low_fullness_ratio = (fullness_changes['前卷装程度'] < intersection).mean() * 100\n",
    "            print(f\"\\n断纱前卷装程度低于交点({intersection:.2f}%)的比例: {low_fullness_ratio:.2f}%\")\n",
    "\n",
    "        # 绘制断纱前卷装程度直方图\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(fullness_changes['前卷装程度'], bins=20, edgecolor='black')\n",
    "        if intersection:\n",
    "            plt.axvline(x=intersection, color='red', linestyle='--', label=f'交点: {intersection:.2f}%')\n",
    "        plt.title('断纱前卷装程度分布')\n",
    "        plt.xlabel('卷装程度(%)')\n",
    "        plt.ylabel('频次')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\n没有找到符合条件的断纱前后记录\")\n",
    "else:\n",
    "    print(\"\\n数据中没有发现断纱锭位记录\")"
   ],
   "id": "c71c0f1b30aa888b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Broken Yarn and Package Fullness Intersection Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "# Set higher DPI for better image quality\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['legend.fontsize'] = 10  # Increased legend font size\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # Using HeiTi font\n",
    "plt.rcParams['axes.unicode_minus'] = False  # Correct minus sign display\n",
    "\n",
    "# Spindle status mapping\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Running\",\n",
    "    2: \"Full Package\",\n",
    "    3: \"Broken Yarn\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Speed Loss\",\n",
    "    7: \"Disconnected\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 1. Prepare spindle status, speed, and fullness columns (ensure one-to-one correspondence)\n",
    "spindle_cols = []\n",
    "for i in range(1, 101):\n",
    "    status_col = f\"D_dw{i}\"\n",
    "    speed_col = f\"V_dw{i}\"\n",
    "    fullness_col = f\"P_dw{i}\"\n",
    "    if status_col in df.columns and speed_col in df.columns and fullness_col in df.columns:\n",
    "        spindle_cols.append((status_col, speed_col, fullness_col))\n",
    "\n",
    "# 2. Find top 10 devices with most broken yarn occurrences\n",
    "device_broken_counts = defaultdict(int)\n",
    "\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    # Find all broken yarn records and count by device\n",
    "    broken_mask = df[status_col] == 3\n",
    "    if broken_mask.any():\n",
    "        broken_by_device = df[broken_mask].groupby('subsystem').size()\n",
    "        for device, count in broken_by_device.items():\n",
    "            device_broken_counts[device] += count\n",
    "\n",
    "# Convert to DataFrame and sort\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Count', ascending=False).head(10)\n",
    "    print(\"\\nTop 10 Devices with Most Broken Yarn Occurrences:\")\n",
    "    print(top_devices)\n",
    "\n",
    "    # 3. Analyze package fullness changes before and after broken yarn for these devices\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    all_broken_records = []\n",
    "\n",
    "    # Store package fullness for all devices in normal and broken states\n",
    "    all_normal_fullness = []\n",
    "    all_broken_fullness = []\n",
    "\n",
    "    for device_name in top_devices.index:\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        # Collect package fullness for this device in normal and broken states\n",
    "        device_normal_fullness = []\n",
    "        device_broken_fullness = []\n",
    "\n",
    "        for status_col, _, fullness_col in spindle_cols:\n",
    "            # Normal state (status 1: Running)\n",
    "            normal_mask = (device_data[status_col] == 1)\n",
    "            device_normal_fullness.extend(device_data[normal_mask][fullness_col].dropna().values)\n",
    "\n",
    "            # Broken state (status 3: Broken Yarn)\n",
    "            broken_mask = (device_data[status_col] == 3)\n",
    "            device_broken_fullness.extend(device_data[broken_mask][fullness_col].dropna().values)\n",
    "\n",
    "            # Find all broken yarn records for this spindle\n",
    "            broken_indices = device_data[broken_mask].index\n",
    "\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # Get start and end of all broken yarn events\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue  # Continuous broken state\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "\n",
    "            if current_start is not None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # Analyze package fullness before and after each broken yarn event\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                # Get last non-broken record before broken yarn\n",
    "                prev_idx = device_data.index.get_loc(start_idx) - 1\n",
    "                while prev_idx >= 0 and device_data.iloc[prev_idx][status_col] == 3:\n",
    "                    prev_idx -= 1\n",
    "\n",
    "                # Get first non-broken record after broken yarn\n",
    "                next_idx = device_data.index.get_loc(end_idx) + 1\n",
    "                while next_idx < len(device_data) and device_data.iloc[next_idx][status_col] == 3:\n",
    "                    next_idx += 1\n",
    "\n",
    "                if prev_idx >= 0 and next_idx < len(device_data):\n",
    "                    record = {\n",
    "                        'Device': device_name,\n",
    "                        'Spindle': status_col.split('_')[1],\n",
    "                        'Broken Start Time': device_data.loc[start_idx, 'created_at'],\n",
    "                        'Broken End Time': device_data.loc[end_idx, 'created_at'],\n",
    "                        'Pre-Fullness': device_data.iloc[prev_idx][fullness_col],\n",
    "                        'Broken Fullness': device_data.loc[start_idx, fullness_col],\n",
    "                        'Post-Fullness': device_data.iloc[next_idx][fullness_col],\n",
    "                        'Duration (min)': (device_data.loc[end_idx, 'created_at'] - device_data.loc[start_idx, 'created_at']).total_seconds() / 60\n",
    "                    }\n",
    "                    device_records.append(record)\n",
    "                    all_broken_records.append(record)\n",
    "\n",
    "        # Plot typical broken yarn events for this device (max 5)\n",
    "        if device_records:\n",
    "            sample_records = pd.DataFrame(device_records).sample(min(5, len(device_records)), random_state=42)\n",
    "            for _, record in sample_records.iterrows():\n",
    "                plt.plot(['Pre-Broken', 'Broken', 'Post-Broken'],\n",
    "                         [record['Pre-Fullness'], record['Broken Fullness'], record['Post-Fullness']],\n",
    "                         marker='o',\n",
    "                         markersize=6,  # Increased marker size\n",
    "                         linewidth=2,   # Thicker lines\n",
    "                         label=f\"{record['Device']}-{record['Spindle']}@{record['Broken Start Time'].strftime('%H:%M')}\")\n",
    "\n",
    "        # Add to global collections\n",
    "        all_normal_fullness.extend(device_normal_fullness)\n",
    "        all_broken_fullness.extend(device_broken_fullness)\n",
    "\n",
    "    # Configure plot\n",
    "    plt.axhline(y=100, color='gray', linestyle='--', label='Target Fullness (100%)')\n",
    "    plt.title('Package Fullness Changes Around Broken Yarn Events\\n(Top 10 Devices)', fontsize=12)\n",
    "    plt.xlabel('Time Point', fontsize=10)\n",
    "    plt.ylabel('Package Fullness (%)', fontsize=10)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Calculate safe operation threshold\n",
    "    if all_normal_fullness and all_broken_fullness:\n",
    "        # Convert to numpy arrays and remove invalid values\n",
    "        normal_data = np.array(all_normal_fullness)\n",
    "        normal_data = normal_data[~np.isnan(normal_data)]\n",
    "\n",
    "        broken_data = np.array(all_broken_fullness)\n",
    "        broken_data = broken_data[~np.isnan(broken_data)]\n",
    "\n",
    "        # Calculate kernel density estimation\n",
    "        kde_normal = gaussian_kde(normal_data)\n",
    "        kde_broken = gaussian_kde(broken_data)\n",
    "\n",
    "        # Define function to find intersection\n",
    "        def find_intersection(x):\n",
    "            return kde_normal(x) - kde_broken(x)\n",
    "\n",
    "        # Find intersection\n",
    "        min_val = min(min(normal_data), min(broken_data))\n",
    "        max_val = max(max(normal_data), max(broken_data))\n",
    "\n",
    "        try:\n",
    "            intersection = brentq(find_intersection, min_val, max_val)\n",
    "            print(f\"\\nSafe Operation Threshold (Normal vs Broken Distribution Intersection): {intersection:.2f}%\")\n",
    "\n",
    "            # Plot distribution curves and intersection\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            x = np.linspace(min_val, max_val, 1000)\n",
    "\n",
    "            plt.plot(x, kde_normal(x), linewidth=2, label='Normal State Distribution')\n",
    "            plt.plot(x, kde_broken(x), linewidth=2, label='Broken State Distribution')\n",
    "            plt.axvline(x=intersection, color='red', linestyle='--', linewidth=2,\n",
    "                       label=f'Intersection: {intersection:.2f}%')\n",
    "\n",
    "            plt.title('Package Fullness Distribution: Normal vs Broken States', fontsize=12)\n",
    "            plt.xlabel('Package Fullness (%)', fontsize=10)\n",
    "            plt.ylabel('Probability Density', fontsize=10)\n",
    "            plt.legend(fontsize=10)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Calculate safe range\n",
    "            safe_min = np.percentile(normal_data, 5)  # Lower 5% percentile of normal state as lower bound\n",
    "            safe_max = 100  # Upper bound is 100%\n",
    "            print(f\"\\nRecommended Safe Operation Range: {safe_min:.2f}% to {safe_max:.2f}%\")\n",
    "\n",
    "            # Calculate percentage of current data within safe range\n",
    "            in_safe_zone = np.sum((normal_data >= safe_min) & (normal_data <= safe_max)) / len(normal_data) * 100\n",
    "            print(f\"Percentage of Normal State Data in Safe Range: {in_safe_zone:.2f}%\")\n",
    "\n",
    "            # Calculate percentage of broken data outside safe range\n",
    "            broken_out_safe = np.sum((broken_data < safe_min) | (broken_data > safe_max)) / len(broken_data) * 100\n",
    "            print(f\"Percentage of Broken State Data Outside Safe Range: {broken_out_safe:.2f}%\")\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"\\nCannot find intersection between normal and broken state distributions\")\n",
    "            intersection = None\n",
    "\n",
    "    # 5. Calculate and display package fullness change statistics\n",
    "    if all_broken_records:\n",
    "        fullness_changes = pd.DataFrame(all_broken_records)\n",
    "        fullness_changes['Fullness Decrease'] = fullness_changes['Pre-Fullness'] - fullness_changes['Broken Fullness']\n",
    "        fullness_changes['Fullness Recovery'] = fullness_changes['Post-Fullness'] - fullness_changes['Broken Fullness']\n",
    "\n",
    "        print(\"\\nPackage Fullness Change Statistics Around Broken Yarn:\")\n",
    "        print(fullness_changes[['Device', 'Spindle', 'Broken Start Time', 'Duration (min)',\n",
    "                              'Pre-Fullness', 'Broken Fullness', 'Post-Fullness',\n",
    "                              'Fullness Decrease', 'Fullness Recovery']].describe())\n",
    "\n",
    "        # Plot boxplot of package fullness changes\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        fullness_changes[['Pre-Fullness', 'Broken Fullness', 'Post-Fullness']].boxplot()\n",
    "        plt.title('Package Fullness Distribution Around Broken Yarn', fontsize=12)\n",
    "        plt.ylabel('Package Fullness (%)', fontsize=10)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Group statistics by device\n",
    "        print(\"\\nBroken Yarn and Package Fullness Statistics by Device:\")\n",
    "        device_stats = fullness_changes.groupby('Device').agg({\n",
    "            'Spindle': 'count',\n",
    "            'Duration (min)': 'mean',\n",
    "            'Fullness Decrease': 'mean',\n",
    "            'Fullness Recovery': 'mean'\n",
    "        }).rename(columns={'Spindle': 'Broken Count', 'Duration (min)': 'Avg Duration (min)'})\n",
    "        print(device_stats)\n",
    "\n",
    "        # 6. Analyze relationship between broken yarn and package fullness\n",
    "        print(\"\\nRelationship Between Broken Yarn and Package Fullness:\")\n",
    "        # Calculate distribution of pre-broken package fullness\n",
    "        print(\"\\nPre-Broken Package Fullness Distribution:\")\n",
    "        print(fullness_changes['Pre-Fullness'].describe())\n",
    "\n",
    "        # Calculate percentage of pre-broken fullness below intersection\n",
    "        if intersection:\n",
    "            low_fullness_ratio = (fullness_changes['Pre-Fullness'] < intersection).mean() * 100\n",
    "            print(f\"\\nPercentage of Pre-Broken Fullness Below Intersection ({intersection:.2f}%): {low_fullness_ratio:.2f}%\")\n",
    "\n",
    "        # Plot histogram of pre-broken package fullness\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(fullness_changes['Pre-Fullness'], bins=20, edgecolor='black')\n",
    "        if intersection:\n",
    "            plt.axvline(x=intersection, color='red', linestyle='--', linewidth=2,\n",
    "                       label=f'Intersection: {intersection:.2f}%')\n",
    "        plt.title('Pre-Broken Package Fullness Distribution', fontsize=12)\n",
    "        plt.xlabel('Package Fullness (%)', fontsize=10)\n",
    "        plt.ylabel('Frequency', fontsize=10)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nNo qualified pre- and post-broken records found\")\n",
    "else:\n",
    "    print(\"\\nNo broken yarn spindle records found in the data\")"
   ],
   "id": "d94e2d8e479ad314",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Broken Yarn and Package Fullness Intersection Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "# Set higher DPI for better image quality\n",
    "plt.rcParams['figure.dpi'] = 1200\n",
    "plt.rcParams['savefig.dpi'] = 1200\n",
    "plt.rcParams['legend.fontsize'] = 28  # Increased legend font size\n",
    "plt.rcParams['axes.titlesize'] = 36\n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # Using HeiTi font\n",
    "plt.rcParams['axes.unicode_minus'] = False  # Correct minus sign display\n",
    "\n",
    "# Spindle status mapping\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Running\",\n",
    "    2: \"Full Package\",\n",
    "    3: \"Broken Yarn\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Speed Loss\",\n",
    "    7: \"Disconnected\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 1. Prepare spindle status, speed, and fullness columns (ensure one-to-one correspondence)\n",
    "spindle_cols = []\n",
    "for i in range(1, 101):\n",
    "    status_col = f\"D_dw{i}\"\n",
    "    speed_col = f\"V_dw{i}\"\n",
    "    fullness_col = f\"P_dw{i}\"\n",
    "    if status_col in df.columns and speed_col in df.columns and fullness_col in df.columns:\n",
    "        spindle_cols.append((status_col, speed_col, fullness_col))\n",
    "\n",
    "# 2. Find top 10 devices with most broken yarn occurrences\n",
    "device_broken_counts = defaultdict(int)\n",
    "\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    # Find all broken yarn records and count by device\n",
    "    broken_mask = df[status_col] == 3\n",
    "    if broken_mask.any():\n",
    "        broken_by_device = df[broken_mask].groupby('subsystem').size()\n",
    "        for device, count in broken_by_device.items():\n",
    "            device_broken_counts[device] += count\n",
    "\n",
    "# Convert to DataFrame and sort\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Count', ascending=False).head(10)\n",
    "    print(\"\\nTop 10 Devices with Most Broken Yarn Occurrences:\")\n",
    "    print(top_devices)\n",
    "\n",
    "    # 3. Analyze package fullness changes before and after broken yarn for these devices\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    all_broken_records = []\n",
    "\n",
    "    # Store package fullness for all devices in normal and broken states\n",
    "    all_normal_fullness = []\n",
    "    all_broken_fullness = []\n",
    "\n",
    "    for device_name in top_devices.index:\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        # Collect package fullness for this device in normal and broken states\n",
    "        device_normal_fullness = []\n",
    "        device_broken_fullness = []\n",
    "\n",
    "        for status_col, _, fullness_col in spindle_cols:\n",
    "            # Normal state (status 1: Running)\n",
    "            normal_mask = (device_data[status_col] == 1)\n",
    "            device_normal_fullness.extend(device_data[normal_mask][fullness_col].dropna().values)\n",
    "\n",
    "            # Broken state (status 3: Broken Yarn)\n",
    "            broken_mask = (device_data[status_col] == 3)\n",
    "            device_broken_fullness.extend(device_data[broken_mask][fullness_col].dropna().values)\n",
    "\n",
    "            # Find all broken yarn records for this spindle\n",
    "            broken_indices = device_data[broken_mask].index\n",
    "\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # Get start and end of all broken yarn events\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue  # Continuous broken state\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "\n",
    "            if current_start is not None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # Analyze package fullness before and after each broken yarn event\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                # Get last non-broken record before broken yarn\n",
    "                prev_idx = device_data.index.get_loc(start_idx) - 1\n",
    "                while prev_idx >= 0 and device_data.iloc[prev_idx][status_col] == 3:\n",
    "                    prev_idx -= 1\n",
    "\n",
    "                # Get first non-broken record after broken yarn\n",
    "                next_idx = device_data.index.get_loc(end_idx) + 1\n",
    "                while next_idx < len(device_data) and device_data.iloc[next_idx][status_col] == 3:\n",
    "                    next_idx += 1\n",
    "\n",
    "                if prev_idx >= 0 and next_idx < len(device_data):\n",
    "                    record = {\n",
    "                        'Device': device_name,\n",
    "                        'Spindle': status_col.split('_')[1],\n",
    "                        'Broken Start Time': device_data.loc[start_idx, 'created_at'],\n",
    "                        'Broken End Time': device_data.loc[end_idx, 'created_at'],\n",
    "                        'Pre-Fullness': device_data.iloc[prev_idx][fullness_col],\n",
    "                        'Broken Fullness': device_data.loc[start_idx, fullness_col],\n",
    "                        'Post-Fullness': device_data.iloc[next_idx][fullness_col],\n",
    "                        'Duration (min)': (device_data.loc[end_idx, 'created_at'] - device_data.loc[start_idx, 'created_at']).total_seconds() / 60\n",
    "                    }\n",
    "                    device_records.append(record)\n",
    "                    all_broken_records.append(record)\n",
    "\n",
    "        # Plot typical broken yarn events for this device (max 5)\n",
    "        if device_records:\n",
    "            sample_records = pd.DataFrame(device_records).sample(min(5, len(device_records)), random_state=42)\n",
    "            for _, record in sample_records.iterrows():\n",
    "                plt.plot(['Pre-Broken', 'Broken', 'Post-Broken'],\n",
    "                         [record['Pre-Fullness'], record['Broken Fullness'], record['Post-Fullness']],\n",
    "                         marker='o',\n",
    "                         markersize=6,  # Increased marker size\n",
    "                         linewidth=2,   # Thicker lines\n",
    "                         label=f\"{record['Device']}-{record['Spindle']}@{record['Broken Start Time'].strftime('%H:%M')}\")\n",
    "\n",
    "        # Add to global collections\n",
    "        all_normal_fullness.extend(device_normal_fullness)\n",
    "        all_broken_fullness.extend(device_broken_fullness)\n",
    "\n",
    "    # Configure plot\n",
    "    plt.axhline(y=100, color='gray', linestyle='--', label='Target Fullness (100%)')\n",
    "    plt.title('Package Fullness Changes Around Broken Yarn Events\\n(Top 10 Devices)', fontsize=36)\n",
    "    plt.xlabel('Time Point', fontsize=32)\n",
    "    plt.ylabel('Package Fullness (%)', fontsize=32)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=28)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Calculate safe operation threshold\n",
    "if all_normal_fullness and all_broken_fullness:\n",
    "    # Convert to numpy arrays and remove invalid values\n",
    "    normal_data = np.array(all_normal_fullness)\n",
    "    normal_data = normal_data[~np.isnan(normal_data)]\n",
    "\n",
    "    broken_data = np.array(all_broken_fullness)\n",
    "    broken_data = broken_data[~np.isnan(broken_data)]\n",
    "\n",
    "    # Calculate kernel density estimation\n",
    "    kde_normal = gaussian_kde(normal_data)\n",
    "    kde_broken = gaussian_kde(broken_data)\n",
    "\n",
    "    # Define function to find intersection\n",
    "    def find_intersection(x):\n",
    "        return kde_normal(x) - kde_broken(x)\n",
    "\n",
    "    # Find intersection\n",
    "    min_val = min(min(normal_data), min(broken_data))\n",
    "    max_val = max(max(normal_data), max(broken_data))\n",
    "\n",
    "    try:\n",
    "        intersection = brentq(find_intersection, min_val, max_val)\n",
    "        print(f\"\\nSafe Operation Threshold (Normal vs Broken Distribution Intersection): {intersection:.2f}%\")\n",
    "\n",
    "        # Plot distribution curves and intersection\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        x = np.linspace(min_val, max_val, 1000)\n",
    "\n",
    "        # 设置绘图风格\n",
    "        plt.style.use('default')\n",
    "        plt.rcParams.update({\n",
    "            # 'font.family': 'Arial',\n",
    "            'font.size': 20,\n",
    "            'axes.labelsize': 20,\n",
    "            'axes.titlesize': 26,\n",
    "            'xtick.labelsize': 18,\n",
    "            'ytick.labelsize': 18,\n",
    "            'figure.dpi': 1200,\n",
    "            'savefig.dpi': 1200,\n",
    "            'figure.figsize': (12, 8),\n",
    "            'axes.grid': True,\n",
    "            'grid.linestyle': '--',\n",
    "            'grid.alpha': 0.3,\n",
    "            'grid.color': 'gray',\n",
    "        })\n",
    "\n",
    "        plt.rcParams['axes.labelweight'] = 'bold'\n",
    "        plt.rcParams['axes.titleweight'] = 'bold'\n",
    "\n",
    "        # 显式设置绘图区域（坐标轴内）的背景颜色为白色\n",
    "        ax = plt.gca()\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "        # 绘制分布曲线\n",
    "        plt.plot(x, kde_normal(x), linewidth=2, label='Normal State Distribution')\n",
    "        plt.plot(x, kde_broken(x), linewidth=2, label='Broken State Distribution')\n",
    "        plt.axvline(x=intersection, color='red', linestyle='--', linewidth=2,\n",
    "                    label=f'Intersection: {intersection:.2f}%')\n",
    "\n",
    "        # 设置标题和标签\n",
    "        plt.title('Package Fullness Distribution: Normal vs Broken States', fontsize=26, fontweight='bold', pad=30)\n",
    "        plt.xlabel('Package Fullness (%)', fontsize=20, fontweight='bold', labelpad=15)\n",
    "        plt.ylabel('Probability Density', fontsize=20, fontweight='bold', labelpad=15)\n",
    "\n",
    "        # 设置图例\n",
    "        plt.legend(fontsize=18, loc='upper right')\n",
    "\n",
    "        # 隐藏上、右边框\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "\n",
    "        # 调整下、左边框样式\n",
    "        ax.spines['bottom'].set_linewidth(3)  # 加粗边框线\n",
    "        ax.spines['left'].set_linewidth(3)    # 加粗边框线\n",
    "        ax.spines['bottom'].set_color('black')\n",
    "        ax.spines['left'].set_color('black')\n",
    "\n",
    "        # 调整刻度标签颜色和粗细\n",
    "        ax.tick_params(axis='x', colors='black', width=2, length=6)\n",
    "        ax.tick_params(axis='y', colors='black', width=2, length=6)\n",
    "\n",
    "        # 调整子图布局\n",
    "        plt.subplots_adjust(left=0.15, right=0.95, top=0.9, bottom=0.1)\n",
    "\n",
    "        # 调整布局\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # 保存图像\n",
    "        plt.savefig('package_fullness_distribution.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "        # 显示图形\n",
    "        plt.show()\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"\\nCannot find intersection between normal and broken state distributions\")\n",
    "        intersection = None\n",
    "\n",
    "    # 5. Calculate and display package fullness change statistics\n",
    "    if all_broken_records:\n",
    "        fullness_changes = pd.DataFrame(all_broken_records)\n",
    "        fullness_changes['Fullness Decrease'] = fullness_changes['Pre-Fullness'] - fullness_changes['Broken Fullness']\n",
    "        fullness_changes['Fullness Recovery'] = fullness_changes['Post-Fullness'] - fullness_changes['Broken Fullness']\n",
    "\n",
    "        print(\"\\nPackage Fullness Change Statistics Around Broken Yarn:\")\n",
    "        print(fullness_changes[['Device', 'Spindle', 'Broken Start Time', 'Duration (min)',\n",
    "                              'Pre-Fullness', 'Broken Fullness', 'Post-Fullness',\n",
    "                              'Fullness Decrease', 'Fullness Recovery']].describe())\n",
    "\n",
    "        # Plot boxplot of package fullness changes\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        fullness_changes[['Pre-Fullness', 'Broken Fullness', 'Post-Fullness']].boxplot()\n",
    "        plt.title('Package Fullness Distribution Around Broken Yarn', fontsize=36)\n",
    "        plt.ylabel('Package Fullness (%)', fontsize=32)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Group statistics by device\n",
    "        print(\"\\nBroken Yarn and Package Fullness Statistics by Device:\")\n",
    "        device_stats = fullness_changes.groupby('Device').agg({\n",
    "            'Spindle': 'count',\n",
    "            'Duration (min)': 'mean',\n",
    "            'Fullness Decrease': 'mean',\n",
    "            'Fullness Recovery': 'mean'\n",
    "        }).rename(columns={'Spindle': 'Broken Count', 'Duration (min)': 'Avg Duration (min)'})\n",
    "        print(device_stats)\n",
    "\n",
    "        # 6. Analyze relationship between broken yarn and package fullness\n",
    "        print(\"\\nRelationship Between Broken Yarn and Package Fullness:\")\n",
    "        # Calculate distribution of pre-broken package fullness\n",
    "        print(\"\\nPre-Broken Package Fullness Distribution:\")\n",
    "        print(fullness_changes['Pre-Fullness'].describe())\n",
    "\n",
    "        # Calculate percentage of pre-broken fullness below intersection\n",
    "        if intersection:\n",
    "            low_fullness_ratio = (fullness_changes['Pre-Fullness'] < intersection).mean() * 100\n",
    "            print(f\"\\nPercentage of Pre-Broken Fullness Below Intersection ({intersection:.2f}%): {low_fullness_ratio:.2f}%\")\n",
    "\n",
    "        # Plot histogram of pre-broken package fullness without the red dashed line\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(fullness_changes['Pre-Fullness'], bins=20, edgecolor='black')\n",
    "\n",
    "        # Configure plot\n",
    "        # plt.title('Pre-Broken Package Fullness Distribution', fontsize=36)\n",
    "        plt.xlabel('Package Fullness (%)', fontweight='bold', labelpad=12)\n",
    "        plt.ylabel('Frequency', fontweight='bold', labelpad=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nNo qualified pre- and post-broken records found\")\n",
    "else:\n",
    "    print(\"\\nNo broken yarn spindle records found in the data\")"
   ],
   "id": "2a8a468ef95375cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置专业学术风格参数\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.dpi'] = 1200\n",
    "plt.rcParams['savefig.dpi'] = 1200\n",
    "plt.rcParams['font.weight'] = 'bold'  # 使用bold保持专业感\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['axes.linewidth'] = 2.5  # 适中轴线宽度\n",
    "plt.rcParams['font.size'] = 18  # 适度字体大小\n",
    "plt.rcParams['axes.titlesize'] = 16  # 标题大小\n",
    "plt.rcParams['axes.labelsize'] = 20  # 轴标签大小\n",
    "plt.rcParams['xtick.labelsize'] = 18  # 刻度标签\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.fontsize'] = 13\n",
    "plt.rcParams['grid.linewidth'] = 1.2  # 细网格线\n",
    "plt.rcParams['lines.linewidth'] = 3.5  # 数据线宽度\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['font.family'] = 'Arial'  # 使用更专业的英文字体\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "\n",
    "# 假设 all_broken_records 已经存在\n",
    "if all_broken_records:\n",
    "    fullness_changes = pd.DataFrame(all_broken_records)\n",
    "\n",
    "    # 调整下、左边框样式\n",
    "    ax.spines['right'].set_linewidth(3)  # 加粗边框线\n",
    "    ax.spines['top'].set_linewidth(3)    # 加粗边框线\n",
    "    ax.spines['right'].set_color('black')\n",
    "    ax.spines['top'].set_color('black')\n",
    "\n",
    "\n",
    "    ax.spines['bottom'].set_linewidth(3)  # 加粗边框线\n",
    "    ax.spines['left'].set_linewidth(3)    # 加粗边框线\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['left'].set_color('black')\n",
    "\n",
    "    # 绘制直方图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(fullness_changes['Pre-Fullness'], bins=20, edgecolor='black')\n",
    "\n",
    "    # 配置图表\n",
    "    plt.xlabel('Package Fullness (%)', fontweight='bold', labelpad=12)\n",
    "    plt.ylabel('Frequency', fontweight='bold', labelpad=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 调整刻度标签颜色和粗细\n",
    "    ax.tick_params(axis='x', colors='black', width=2, length=6, labelsize=24)\n",
    "    ax.tick_params(axis='y', colors='black', width=2, length=6, labelsize=24)\n",
    "\n",
    "    # 保存图像\n",
    "    plt.savefig('pre_broken_fullness_distribution.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "    # 显示图形\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nNo qualified pre- and post-broken records found\")"
   ],
   "id": "103dde7def457239",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置专业学术风格参数\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.weight'] = 'bold'  # 使用bold保持专业感\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['axes.linewidth'] = 2.5  # 适中轴线宽度\n",
    "plt.rcParams['font.size'] = 18  # 适度字体大小\n",
    "plt.rcParams['axes.titlesize'] = 16  # 标题大小\n",
    "plt.rcParams['axes.labelsize'] = 22  # 轴标签大小\n",
    "plt.rcParams['xtick.labelsize'] = 15  # 刻度标签\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "plt.rcParams['legend.fontsize'] = 13\n",
    "plt.rcParams['grid.linewidth'] = 1.2  # 细网格线\n",
    "plt.rcParams['lines.linewidth'] = 3.5  # 数据线宽度\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['font.family'] = 'Arial'  # 使用更专业的英文字体\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "\n",
    "# 假设 all_broken_records 已经存在\n",
    "if all_broken_records:\n",
    "    fullness_changes = pd.DataFrame(all_broken_records)\n",
    "\n",
    "    # 绘制直方图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(fullness_changes['Pre-Fullness'], bins=20, edgecolor='black')\n",
    "\n",
    "    # 配置图表\n",
    "    plt.xlabel('Package Fullness (%)', fontweight='bold', labelpad=17)\n",
    "    plt.ylabel('Frequency', fontweight='bold', labelpad=17)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 获取当前轴对象\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # 调整下、左边框样式\n",
    "    ax.spines['right'].set_linewidth(3)  # 加粗边框线\n",
    "    ax.spines['top'].set_linewidth(3)    # 加粗边框线\n",
    "    ax.spines['right'].set_color('black')\n",
    "    ax.spines['top'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(3)  # 加粗边框线\n",
    "    ax.spines['left'].set_linewidth(3)    # 加粗边框线\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['left'].set_color('black')\n",
    "\n",
    "    # 加粗 X 轴和 Y 轴的数据分割线（刻度线）并向外延伸\n",
    "    ax.tick_params(axis='x', colors='black', width=3, length=10, direction='out', labelsize=18)\n",
    "    ax.tick_params(axis='y', colors='black', width=3, length=10, direction='out', labelsize=18)\n",
    "\n",
    "    # 保存图像\n",
    "    plt.savefig('pre_broken_fullness_distribution.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "    # 显示图形\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nNo qualified pre- and post-broken records found\")"
   ],
   "id": "8a586434f27a3a8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Broken Yarn and Package Fullness Intersection Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "# Set higher DPI for better image quality\n",
    "plt.rcParams['figure.dpi'] = 1200\n",
    "plt.rcParams['savefig.dpi'] = 1200\n",
    "plt.rcParams['legend.fontsize'] = 28  # Increased legend font size\n",
    "plt.rcParams['axes.titlesize'] = 36\n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # Using HeiTi font\n",
    "plt.rcParams['axes.unicode_minus'] = False  # Correct minus sign display\n",
    "\n",
    "# Spindle status mapping\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Running\",\n",
    "    2: \"Full Package\",\n",
    "    3: \"Broken Yarn\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Speed Loss\",\n",
    "    7: \"Disconnected\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 1. Prepare spindle status, speed, and fullness columns (ensure one-to-one correspondence)\n",
    "spindle_cols = []\n",
    "for i in range(1, 101):\n",
    "    status_col = f\"D_dw{i}\"\n",
    "    speed_col = f\"V_dw{i}\"\n",
    "    fullness_col = f\"P_dw{i}\"\n",
    "    if status_col in df.columns and speed_col in df.columns and fullness_col in df.columns:\n",
    "        spindle_cols.append((status_col, speed_col, fullness_col))\n",
    "\n",
    "# 2. Find top 10 devices with most broken yarn occurrences\n",
    "device_broken_counts = defaultdict(int)\n",
    "\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    # Find all broken yarn records and count by device\n",
    "    broken_mask = df[status_col] == 3\n",
    "    if broken_mask.any():\n",
    "        broken_by_device = df[broken_mask].groupby('subsystem').size()\n",
    "        for device, count in broken_by_device.items():\n",
    "            device_broken_counts[device] += count\n",
    "\n",
    "# Convert to DataFrame and sort\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Count', ascending=False).head(10)\n",
    "    print(\"\\nTop 10 Devices with Most Broken Yarn Occurrences:\")\n",
    "    print(top_devices)\n",
    "\n",
    "    # 3. Analyze package fullness changes before and after broken yarn for these devices\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    all_broken_records = []\n",
    "\n",
    "    # Store package fullness for all devices in normal and broken states\n",
    "    all_normal_fullness = []\n",
    "    all_broken_fullness = []\n",
    "\n",
    "    for device_name in top_devices.index:\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        # Collect package fullness for this device in normal and broken states\n",
    "        device_normal_fullness = []\n",
    "        device_broken_fullness = []\n",
    "\n",
    "        for status_col, _, fullness_col in spindle_cols:\n",
    "            # Normal state (status 1: Running)\n",
    "            normal_mask = (device_data[status_col] == 1)\n",
    "            device_normal_fullness.extend(device_data[normal_mask][fullness_col].dropna().values)\n",
    "\n",
    "            # Broken state (status 3: Broken Yarn)\n",
    "            broken_mask = (device_data[status_col] == 3)\n",
    "            device_broken_fullness.extend(device_data[broken_mask][fullness_col].dropna().values)\n",
    "\n",
    "            # Find all broken yarn records for this spindle\n",
    "            broken_indices = device_data[broken_mask].index\n",
    "\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # Get start and end of all broken yarn events\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue  # Continuous broken state\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "\n",
    "            if current_start is None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # Analyze package fullness before and after each broken yarn event\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                # Get last non-broken record before broken yarn\n",
    "                prev_idx = device_data.index.get_loc(start_idx) - 1\n",
    "                while prev_idx >= 0 and device_data.iloc[prev_idx][status_col] == 3:\n",
    "                    prev_idx -= 1\n",
    "\n",
    "                # Get first non-broken record after broken yarn\n",
    "                next_idx = device_data.index.get_loc(end_idx) + 1\n",
    "                while next_idx < len(device_data) and device_data.iloc[next_idx][status_col] == 3:\n",
    "                    next_idx += 1\n",
    "\n",
    "                if prev_idx >= 0 and next_idx < len(device_data):\n",
    "                    record = {\n",
    "                        'Device': device_name,\n",
    "                        'Spindle': status_col.split('_')[1],\n",
    "                        'Broken Start Time': device_data.loc[start_idx, 'created_at'],\n",
    "                        'Broken End Time': device_data.loc[end_idx, 'created_at'],\n",
    "                        'Pre-Fullness': device_data.iloc[prev_idx][fullness_col],\n",
    "                        'Broken Fullness': device_data.loc[start_idx, fullness_col],\n",
    "                        'Post-Fullness': device_data.iloc[next_idx][fullness_col],\n",
    "                        'Duration (min)': (device_data.loc[end_idx, 'created_at'] - device_data.loc[start_idx, 'created_at']).total_seconds() / 60\n",
    "                    }\n",
    "                    device_records.append(record)\n",
    "                    all_broken_records.append(record)\n",
    "\n",
    "        # Plot typical broken yarn events for this device (max 5)\n",
    "        if device_records:\n",
    "            sample_records = pd.DataFrame(device_records).sample(min(5, len(device_records)), random_state=42)\n",
    "            for _, record in sample_records.iterrows():\n",
    "                plt.plot(['Pre-Broken', 'Broken', 'Post-Broken'],\n",
    "                         [record['Pre-Fullness'], record['Broken Fullness'], record['Post-Fullness']],\n",
    "                         marker='o',\n",
    "                         markersize=6,  # Increased marker size\n",
    "                         linewidth=2,   # Thicker lines\n",
    "                         label=f\"{record['Device']}-{record['Spindle']}@{record['Broken Start Time'].strftime('%H:%M')}\")\n",
    "\n",
    "        # Add to global collections\n",
    "        all_normal_fullness.extend(device_normal_fullness)\n",
    "        all_broken_fullness.extend(device_broken_fullness)\n",
    "\n",
    "    # Configure plot\n",
    "    plt.axhline(y=100, color='gray', linestyle='--', label='Target Fullness (100%)')\n",
    "    plt.title('Package Fullness Changes Around Broken Yarn Events\\n(Top 10 Devices)', fontsize=36)\n",
    "    plt.xlabel('Time Point', fontsize=32)\n",
    "    plt.ylabel('Package Fullness (%)', fontsize=32)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=28)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Calculate safe operation threshold\n",
    "if all_normal_fullness and all_broken_fullness:\n",
    "    # Convert to numpy arrays and remove invalid values\n",
    "    normal_data = np.array(all_normal_fullness)\n",
    "    normal_data = normal_data[~np.isnan(normal_data)]\n",
    "\n",
    "    broken_data = np.array(all_broken_fullness)\n",
    "    broken_data = broken_data[~np.isnan(broken_data)]\n",
    "\n",
    "    # Calculate kernel density estimation\n",
    "    kde_normal = gaussian_kde(normal_data)\n",
    "    kde_broken = gaussian_kde(broken_data)\n",
    "\n",
    "    # Define function to find intersection\n",
    "    def find_intersection(x):\n",
    "        return kde_normal(x) - kde_broken(x)\n",
    "\n",
    "    # Find intersection\n",
    "    min_val = min(min(normal_data), min(broken_data))\n",
    "    max_val = max(max(normal_data), max(broken_data))\n",
    "\n",
    "    try:\n",
    "        intersection = brentq(find_intersection, min_val, max_val)\n",
    "        print(f\"\\nSafe Operation Threshold (Normal vs Broken Distribution Intersection): {intersection:.2f}%\")\n",
    "\n",
    "        # Plot distribution curves and intersection\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        x = np.linspace(min_val, max_val, 1000)\n",
    "\n",
    "        # 设置绘图风格\n",
    "        plt.style.use('default')\n",
    "        plt.rcParams.update({\n",
    "            # 'font.family': 'Arial',\n",
    "            'font.size': 20,\n",
    "            'axes.labelsize': 20,\n",
    "            'axes.titlesize': 26,\n",
    "            'xtick.labelsize': 18,\n",
    "            'ytick.labelsize': 18,\n",
    "            'figure.dpi': 1200,\n",
    "            'savefig.dpi': 1200,\n",
    "            'figure.figsize': (12, 8),\n",
    "            'axes.grid': True,\n",
    "            'grid.linestyle': '--',\n",
    "            'grid.alpha': 0.3,\n",
    "            'grid.color': 'gray',\n",
    "        })\n",
    "\n",
    "        plt.rcParams['axes.labelweight'] = 'bold'\n",
    "        plt.rcParams['axes.titleweight'] = 'bold'\n",
    "\n",
    "        # 显式设置绘图区域（坐标轴内）的背景颜色为白色\n",
    "        ax = plt.gca()\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "        # 绘制分布曲线\n",
    "        plt.plot(x, kde_normal(x), linewidth=2, label='Normal State Distribution')\n",
    "        plt.plot(x, kde_broken(x), linewidth=2, label='Broken State Distribution')\n",
    "        plt.axvline(x=intersection, color='red', linestyle='--', linewidth=2,\n",
    "                    label=f'Intersection: {intersection:.2f}%')\n",
    "\n",
    "        # 设置标题和标签\n",
    "        plt.title('Package Fullness Distribution: Normal vs Broken States', fontsize=26, fontweight='bold', pad=30)\n",
    "        plt.xlabel('Package Fullness (%)', fontsize=20, fontweight='bold', labelpad=15)\n",
    "        plt.ylabel('Probability Density', fontsize=20, fontweight='bold', labelpad=15)\n",
    "\n",
    "        # 设置图例\n",
    "        plt.legend(fontsize=18, loc='upper right')\n",
    "\n",
    "        # 隐藏上、右边框\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "\n",
    "        # 调整下、左边框样式\n",
    "        ax.spines['bottom'].set_linewidth(3)  # 加粗边框线\n",
    "        ax.spines['left'].set_linewidth(3)    # 加粗边框线\n",
    "        ax.spines['bottom'].set_color('black')\n",
    "        ax.spines['left'].set_color('black')\n",
    "\n",
    "        # 调整刻度标签颜色和粗细\n",
    "        ax.tick_params(axis='x', colors='black', width=2, length=6)\n",
    "        ax.tick_params(axis='y', colors='black', width=2, length=6)\n",
    "\n",
    "        # 调整子图布局\n",
    "        plt.subplots_adjust(left=0.15, right=0.95, top=0.9, bottom=0.1)\n",
    "\n",
    "        # 调整布局\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # 保存图像\n",
    "        plt.savefig('package_fullness_distribution.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "        # 显示图形\n",
    "        plt.show()\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"\\nCannot find intersection between normal and broken state distributions\")\n",
    "        intersection = None\n",
    "\n",
    "    # 5. Calculate and display package fullness change statistics\n",
    "    if all_broken_records:\n",
    "        fullness_changes = pd.DataFrame(all_broken_records)\n",
    "        fullness_changes['Fullness Decrease'] = fullness_changes['Pre-Fullness'] - fullness_changes['Broken Fullness']\n",
    "        fullness_changes['Fullness Recovery'] = fullness_changes['Post-Fullness'] - fullness_changes['Broken Fullness']\n",
    "\n",
    "        print(\"\\nPackage Fullness Change Statistics Around Broken Yarn:\")\n",
    "        print(fullness_changes[['Device', 'Spindle', 'Broken Start Time', 'Duration (min)',\n",
    "                              'Pre-Fullness', 'Broken Fullness', 'Post-Fullness',\n",
    "                              'Fullness Decrease', 'Fullness Recovery']].describe())\n",
    "\n",
    "        # Plot boxplot of package fullness changes\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        fullness_changes[['Pre-Fullness', 'Broken Fullness', 'Post-Fullness']].boxplot()\n",
    "        plt.title('Package Fullness Distribution Around Broken Yarn', fontsize=36)\n",
    "        plt.ylabel('Package Fullness (%)', fontsize=32)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Group statistics by device\n",
    "        print(\"\\nBroken Yarn and Package Fullness Statistics by Device:\")\n",
    "        device_stats = fullness_changes.groupby ['Device'].agg({\n",
    "            'Spindle': 'count',\n",
    "            'Duration (min)': 'mean',\n",
    "            'Fullness Decrease': 'mean',\n",
    "            'Fullness Recovery': 'mean'\n",
    "        }).rename(columns={'Spindle': 'Broken Count', 'Duration (min)': 'Avg Duration (min)'})\n",
    "        print(device_stats)\n",
    "\n",
    "        # 6. Analyze relationship between broken yarn and package fullness\n",
    "        print(\"\\nRelationship Between Broken Yarn and Package Fullness:\")\n",
    "        # Calculate distribution of pre-broken package fullness\n",
    "        print(\"\\nPre-Broken Package Fullness Distribution:\")\n",
    "        print(fullness_changes['Pre-Fullness'].describe())\n",
    "\n",
    "        # Calculate percentage of pre-broken fullness below intersection\n",
    "        if intersection:\n",
    "            low_fullness_ratio = (fullness_changes['Pre-Fullness'] < intersection).mean() * 100\n",
    "            print(f\"\\nPercentage of Pre-Broken Fullness Below Intersection ({intersection:.2f}%): {low_fullness_ratio:.2f}%\")\n",
    "\n",
    "        # Plot histogram of pre-broken package fullness without the red dashed line\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(fullness_changes['Pre-Fullness'], bins=20, edgecolor='black')\n",
    "\n",
    "        # Configure plot\n",
    "        plt.title('Pre-Broken Package Fullness Distribution', fontsize=36)\n",
    "        plt.xlabel('Package Fullness (%)', fontweight='bold', labelpad=12)\n",
    "        plt.ylabel('Frequency', fontweight='bold', labelpad=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the histogram plot\n",
    "        plt.savefig('pre_broken_fullness_distribution.png', bbox_inches='tight', dpi=300)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nNo qualified pre- and post-broken records found\")\n",
    "else:\n",
    "    print(\"\\nNo broken yarn spindle records found in the data\")"
   ],
   "id": "ec6d3ba5da5b7d26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Broken Yarn and Package Fullness Intersection Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "# Set higher DPI for better image quality\n",
    "plt.rcParams['figure.dpi'] = 1200\n",
    "plt.rcParams['savefig.dpi'] = 1200\n",
    "plt.rcParams['legend.fontsize'] = 10  # Increased legend font size\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # Using HeiTi font\n",
    "plt.rcParams['axes.unicode_minus'] = False  # Correct minus sign display\n",
    "\n",
    "# Spindle status mapping\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Running\",\n",
    "    2: \"Full Package\",\n",
    "    3: \"Broken Yarn\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Speed Loss\",\n",
    "    7: \"Disconnected\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 1. Prepare spindle status, speed, and fullness columns (ensure one-to-one correspondence)\n",
    "spindle_cols = []\n",
    "for i in range(1, 101):\n",
    "    status_col = f\"D_dw{i}\"\n",
    "    speed_col = f\"V_dw{i}\"\n",
    "    fullness_col = f\"P_dw{i}\"\n",
    "    if status_col in df.columns and speed_col in df.columns and fullness_col in df.columns:\n",
    "        spindle_cols.append((status_col, speed_col, fullness_col))\n",
    "\n",
    "# 2. Find top 10 devices with most broken yarn occurrences\n",
    "device_broken_counts = defaultdict(int)\n",
    "\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    # Find all broken yarn records and count by device\n",
    "    broken_mask = df[status_col] == 3\n",
    "    if broken_mask.any():\n",
    "        broken_by_device = df[broken_mask].groupby('subsystem').size()\n",
    "        for device, count in broken_by_device.items():\n",
    "            device_broken_counts[device] += count\n",
    "\n",
    "# Convert to DataFrame and sort\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Count', ascending=False).head(10)\n",
    "    print(\"\\nTop 10 Devices with Most Broken Yarn Occurrences:\")\n",
    "    print(top_devices)\n",
    "\n",
    "    # 3. Analyze package fullness changes before and after broken yarn for these devices\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    all_broken_records = []\n",
    "\n",
    "    # Store package fullness for all devices in normal and broken states\n",
    "    all_normal_fullness = []\n",
    "    all_broken_fullness = []\n",
    "\n",
    "    for device_name in top_devices.index:\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        # Collect package fullness for this device in normal and broken states\n",
    "        device_normal_fullness = []\n",
    "        device_broken_fullness = []\n",
    "\n",
    "        for status_col, _, fullness_col in spindle_cols:\n",
    "            # Normal state (status 1: Running)\n",
    "            normal_mask = (device_data[status_col] == 1)\n",
    "            device_normal_fullness.extend(device_data[normal_mask][fullness_col].dropna().values)\n",
    "\n",
    "            # Broken state (status 3: Broken Yarn)\n",
    "            broken_mask = (device_data[status_col] == 3)\n",
    "            device_broken_fullness.extend(device_data[broken_mask][fullness_col].dropna().values)\n",
    "\n",
    "        # Add to global collections\n",
    "        all_normal_fullness.extend(device_normal_fullness)\n",
    "        all_broken_fullness.extend(device_broken_fullness)\n",
    "\n",
    "    # 4. Calculate safe operation threshold\n",
    "    if all_normal_fullness and all_broken_fullness:\n",
    "        # Convert to numpy arrays and remove invalid values\n",
    "        normal_data = np.array(all_normal_fullness)\n",
    "        normal_data = normal_data[~np.isnan(normal_data)]\n",
    "\n",
    "        broken_data = np.array(all_broken_fullness)\n",
    "        broken_data = broken_data[~np.isnan(broken_data)]\n",
    "\n",
    "        # Calculate kernel density estimation\n",
    "        kde_normal = gaussian_kde(normal_data)\n",
    "        kde_broken = gaussian_kde(broken_data)\n",
    "\n",
    "        # Define function to find intersection\n",
    "        def find_intersection(x):\n",
    "            return kde_normal(x) - kde_broken(x)\n",
    "\n",
    "        # Find intersection\n",
    "        min_val = min(min(normal_data), min(broken_data))\n",
    "        max_val = max(max(normal_data), max(broken_data))\n",
    "\n",
    "        try:\n",
    "            intersection = brentq(find_intersection, min_val, max_val)\n",
    "            print(f\"\\nSafe Operation Threshold (Normal vs Broken Distribution Intersection): {intersection:.2f}%\")\n",
    "\n",
    "            # Plot distribution curves and intersection\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            x = np.linspace(min_val, max_val, 1000)\n",
    "\n",
    "            plt.plot(x, kde_normal(x), linewidth=2, label='Normal State Distribution')\n",
    "            plt.plot(x, kde_broken(x), linewidth=2, label='Broken State Distribution')\n",
    "            plt.axvline(x=intersection, color='red', linestyle='--', linewidth=2,\n",
    "                       label=f'Intersection: {intersection:.2f}%')\n",
    "\n",
    "            plt.title('Package Fullness Distribution: Normal vs Broken States', fontsize=12)\n",
    "            plt.xlabel('Package Fullness (%)', fontsize=10)\n",
    "            plt.ylabel('Probability Density', fontsize=10)\n",
    "            plt.legend(fontsize=10)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Calculate safe range\n",
    "            safe_min = np.percentile(normal_data, 5)  # Lower 5% percentile of normal state as lower bound\n",
    "            safe_max = 100  # Upper bound is 100%\n",
    "            print(f\"\\nRecommended Safe Operation Range: {safe_min:.2f}% to {safe_max:.2f}%\")\n",
    "\n",
    "            # Calculate percentage of current data within safe range\n",
    "            in_safe_zone = np.sum((normal_data >= safe_min) & (normal_data <= safe_max)) / len(normal_data) * 100\n",
    "            print(f\"Percentage of Normal State Data in Safe Range: {in_safe_zone:.2f}%\")\n",
    "\n",
    "            # Calculate percentage of broken data outside safe range\n",
    "            broken_out_safe = np.sum((broken_data < safe_min) | (broken_data > safe_max)) / len(broken_data) * 100\n",
    "            print(f\"Percentage of Broken State Data Outside Safe Range: {broken_out_safe:.2f}%\")\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"\\nCannot find intersection between normal and broken state distributions\")\n",
    "            intersection = None\n",
    "\n",
    "    # 5. Analyze relationship between broken yarn and package fullness\n",
    "    print(\"\\nRelationship Between Broken Yarn and Package Fullness:\")\n",
    "    # Calculate distribution of pre-broken package fullness\n",
    "    print(\"\\nPre-Broken Package Fullness Distribution:\")\n",
    "    print(fullness_changes['Pre-Fullness'].describe())\n",
    "\n",
    "    # Calculate percentage of pre-broken fullness below intersection\n",
    "    if intersection:\n",
    "        low_fullness_ratio = (fullness_changes['Pre-Fullness'] < intersection).mean() * 100\n",
    "        print(f\"\\nPercentage of Pre-Broken Fullness Below Intersection ({intersection:.2f}%): {low_fullness_ratio:.2f}%\")\n",
    "\n",
    "    # Plot histogram of pre-broken package fullness\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(fullness_changes['Pre-Fullness'], bins=20, edgecolor='black')\n",
    "    if intersection:\n",
    "        plt.axvline(x=intersection, color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Intersection: {intersection:.2f}%')\n",
    "    plt.title('Pre-Broken Package Fullness Distribution', fontsize=12)\n",
    "    plt.xlabel('Package Fullness (%)', fontsize=10)\n",
    "    plt.ylabel('Frequency', fontsize=10)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nNo broken yarn spindle records found in the data\")"
   ],
   "id": "51f012407aad84e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import matplotlib as mpl\n",
    "\n",
    "# 设置超高质量参数\n",
    "plt.rcParams['figure.dpi'] = 600\n",
    "plt.rcParams['savefig.dpi'] = 1200\n",
    "plt.rcParams['font.size'] = 28\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 加载数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "\n",
    "# 状态映射\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Running\",\n",
    "    2: \"Full Package\",\n",
    "    3: \"Broken Yarn\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Speed Loss\",\n",
    "    7: \"Disconnected\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 准备锭子列\n",
    "spindle_cols = []\n",
    "for i in range(1, 101):\n",
    "    status_col = f\"D_dw{i}\"\n",
    "    speed_col = f\"V_dw{i}\"\n",
    "    fullness_col = f\"P_dw{i}\"\n",
    "    if status_col in df.columns and speed_col in df.columns and fullness_col in df.columns:\n",
    "        spindle_cols.append((status_col, speed_col, fullness_col))\n",
    "\n",
    "# 找出故障最多的设备\n",
    "device_broken_counts = defaultdict(int)\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    broken_mask = df[status_col] == 3\n",
    "    if broken_mask.any():\n",
    "        broken_by_device = df[broken_mask].groupby('subsystem').size()\n",
    "        for device, count in broken_by_device.items():\n",
    "            device_broken_counts[device] += count\n",
    "\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Count', ascending=False).head(6)\n",
    "    print(\"\\nTop 6 Devices with Most Broken Yarn Occurrences:\")\n",
    "    print(top_devices)\n",
    "\n",
    "    # 创建图形 - 使用更现代的布局\n",
    "    fig, ax = plt.subplots(figsize=(30, 18), facecolor='white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    # 设置背景和边框 - 更专业的配色方案\n",
    "    ax.set_facecolor('#FAFAFA')  # 更浅的灰色背景\n",
    "\n",
    "    # 边框设计 - 更精致的样式\n",
    "    for spine in ['top', 'right']:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "    for spine in ['bottom', 'left']:\n",
    "        ax.spines[spine].set_color('#2F2F2F')\n",
    "        ax.spines[spine].set_linewidth(4)\n",
    "        ax.spines[spine].set_linestyle('-')\n",
    "\n",
    "    # 网格线 - 更精细的设计\n",
    "    ax.grid(True, linestyle=':', linewidth=2, alpha=0.6, color='#CCCCCC')\n",
    "\n",
    "    # 刻度设置\n",
    "    ax.tick_params(axis='both', which='major',\n",
    "                   width=2, length=10,\n",
    "                   labelsize=32, pad=10,\n",
    "                   colors='#2F2F2F')\n",
    "\n",
    "    # 刻度标签加粗\n",
    "    for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "\n",
    "    # 轴标签 - 更专业的排版\n",
    "    ax.set_xlabel('Time Sequence',\n",
    "                 fontsize=42, weight='bold',\n",
    "                 labelpad=20, color='#2F2F2F')\n",
    "    ax.set_ylabel('Package Fullness (%)',\n",
    "                 fontsize=42, weight='bold',\n",
    "                 labelpad=22, color='#2F2F2F')\n",
    "\n",
    "    # 标题 - 更清晰的层次结构\n",
    "    ax.set_title('Package Fullness Changes Around Broken Yarn Events\\n(Top 6 Devices)',\n",
    "                fontsize=46, weight='bold', pad=24,\n",
    "                color='#2F2F2F')\n",
    "\n",
    "    # 转换时间戳\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "    # 收集所有故障记录\n",
    "    all_broken_records = []\n",
    "    line_handles = []\n",
    "\n",
    "    # 更专业的配色方案\n",
    "    colors = [\n",
    "        '#4E79A7', '#F28E2B', '#E15759', '#76B7B2',\n",
    "        '#59A14F', '#EDC948', '#B07AA1', '#FF9DA7',\n",
    "        '#9C755F', '#BAB0AC'\n",
    "    ]\n",
    "\n",
    "    # 为每个设备创建更清晰的视觉区分\n",
    "    for device_idx, (device_name, _) in enumerate(top_devices.iterrows()):\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        for status_col, _, fullness_col in spindle_cols:\n",
    "            broken_mask = (device_data[status_col] == 3)\n",
    "            broken_indices = device_data[broken_mask].index\n",
    "\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # 处理故障事件\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "            if current_start is not None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # 分析满度变化\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                prev_idx = device_data.index.get_loc(start_idx) - 1\n",
    "                while prev_idx >= 0 and device_data.iloc[prev_idx][status_col] == 3:\n",
    "                    prev_idx -= 1\n",
    "\n",
    "                next_idx = device_data.index.get_loc(end_idx) + 1\n",
    "                while next_idx < len(device_data) and device_data.iloc[next_idx][status_col] == 3:\n",
    "                    next_idx += 1\n",
    "\n",
    "                if prev_idx >= 0 and next_idx < len(device_data):\n",
    "                    record = {\n",
    "                        'Device': device_name,\n",
    "                        'Spindle': status_col.split('_')[1],\n",
    "                        'Broken Start Time': device_data.loc[start_idx, 'created_at'],\n",
    "                        'Broken End Time': device_data.loc[end_idx, 'created_at'],\n",
    "                        'Pre-Fullness': device_data.iloc[prev_idx][fullness_col],\n",
    "                        'Broken Fullness': device_data.loc[start_idx, fullness_col],\n",
    "                        'Post-Fullness': device_data.iloc[next_idx][fullness_col],\n",
    "                        'Duration (min)': (device_data.loc[end_idx, 'created_at'] - device_data.loc[start_idx, 'created_at']).total_seconds() / 60\n",
    "                    }\n",
    "                    device_records.append(record)\n",
    "                    all_broken_records.append(record)\n",
    "\n",
    "        # 绘制样本记录 - 更清晰的视觉表现\n",
    "        if device_records:\n",
    "            sample_records = pd.DataFrame(device_records).sample(min(5, len(device_records)), random_state=42)\n",
    "            for record_idx, (_, record) in enumerate(sample_records.iterrows()):\n",
    "                color = colors[device_idx % len(colors)]\n",
    "                line, = ax.plot(['Pre-Broken', 'Broken', 'Post-Broken'],\n",
    "                              [record['Pre-Fullness'], record['Broken Fullness'], record['Post-Fullness']],\n",
    "                              marker='o',\n",
    "                              markersize=18,\n",
    "                              markeredgewidth=2,\n",
    "                              linewidth=6,\n",
    "                              color=color,\n",
    "                              markeredgecolor='white',\n",
    "                              alpha=0.9,\n",
    "                              label=f\"{record['Device']}-{record['Spindle']} @ {record['Broken Start Time'].strftime('%H:%M')}\")\n",
    "                line_handles.append(line)\n",
    "\n",
    "    # 目标满度线 - 更清晰的视觉表现\n",
    "    target_line = ax.axhline(y=100, color='#E15759', linestyle='--',\n",
    "                            linewidth=4, alpha=0.8,\n",
    "                            label='Target Fullness (100%)')\n",
    "    line_handles.append(target_line)\n",
    "\n",
    "    # 图例 - 更专业的布局\n",
    "    legend = ax.legend(handles=line_handles,\n",
    "                     bbox_to_anchor=(1.02, 0.98),\n",
    "                     loc='upper left',\n",
    "                     fontsize=24,\n",
    "                     framealpha=0.95,\n",
    "                     facecolor='white',\n",
    "                     edgecolor='#DDDDDD',\n",
    "                     borderpad=0.8,\n",
    "                     labelspacing=0.5,\n",
    "                     handlelength=1.5,\n",
    "                     handletextpad=0.5,\n",
    "                     frameon=True,\n",
    "                     borderaxespad=0.5)\n",
    "\n",
    "    # 图例文本样式\n",
    "    for text in legend.get_texts():\n",
    "        text.set_fontweight('bold')\n",
    "        text.set_color('#2F2F2F')\n",
    "\n",
    "    # 调整布局 - 更合理的空间分配\n",
    "    plt.subplots_adjust(left=0.08, right=0.68, top=0.90, bottom=0.10)\n",
    "\n",
    "    # 图形边框 - 更精致的样式\n",
    "    fig.patch.set_edgecolor('#2F2F2F')\n",
    "    fig.patch.set_linewidth(4)\n",
    "\n",
    "    # 保存图形\n",
    "    plt.savefig('broken_yarn_analysis_professional_design.png',\n",
    "               bbox_inches='tight',\n",
    "               dpi=1200,\n",
    "               facecolor=fig.get_facecolor(),\n",
    "               edgecolor=fig.get_edgecolor(),\n",
    "               transparent=False)\n",
    "\n",
    "    plt.show()"
   ],
   "id": "6484370ac64271bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import matplotlib as mpl\n",
    "\n",
    "# 设置超高质量参数\n",
    "plt.rcParams['figure.dpi'] = 600\n",
    "plt.rcParams['savefig.dpi'] = 1200\n",
    "plt.rcParams['font.size'] = 28\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 加载数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "\n",
    "# 状态映射\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Running\",\n",
    "    2: \"Full Package\",\n",
    "    3: \"Broken Yarn\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Speed Loss\",\n",
    "    7: \"Disconnected\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 准备锭子列\n",
    "spindle_cols = []\n",
    "for i in range(1, 101):\n",
    "    status_col = f\"D_dw{i}\"\n",
    "    speed_col = f\"V_dw{i}\"\n",
    "    fullness_col = f\"P_dw{i}\"\n",
    "    if status_col in df.columns and speed_col in df.columns and fullness_col in df.columns:\n",
    "        spindle_cols.append((status_col, speed_col, fullness_col))\n",
    "\n",
    "# 找出故障最多的设备\n",
    "device_broken_counts = defaultdict(int)\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    broken_mask = df[status_col] == 3\n",
    "    if broken_mask.any():\n",
    "        broken_by_device = df[broken_mask].groupby('subsystem').size()\n",
    "        for device, count in broken_by_device.items():\n",
    "            device_broken_counts[device] += count\n",
    "\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Count', ascending=False).head(6)\n",
    "    print(\"\\nTop 6 Devices with Most Broken Yarn Occurrences:\")\n",
    "    print(top_devices)\n",
    "\n",
    "    # 创建图形 - 使用更现代的布局\n",
    "    fig, ax = plt.subplots(figsize=(30, 18), facecolor='white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    # 设置背景和边框 - 更专业的配色方案\n",
    "    ax.set_facecolor('#FAFAFA')  # 更浅的灰色背景\n",
    "\n",
    "    # 边框设计 - 更精致的样式\n",
    "    for spine in ['top', 'right']:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "    for spine in ['bottom', 'left']:\n",
    "        ax.spines[spine].set_color('#2F2F2F')\n",
    "        ax.spines[spine].set_linewidth(4)\n",
    "        ax.spines[spine].set_linestyle('-')\n",
    "\n",
    "    # 网格线 - 更精细的设计\n",
    "    ax.grid(True, linestyle=':', linewidth=2, alpha=0.6, color='#CCCCCC')\n",
    "\n",
    "    # 刻度设置\n",
    "    ax.tick_params(axis='both', which='major',\n",
    "                   width=2, length=10,\n",
    "                   labelsize=32, pad=10,\n",
    "                   colors='#2F2F2F')\n",
    "\n",
    "    # 刻度标签加粗\n",
    "    for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "        label.set_fontweight('bold')\n",
    "\n",
    "    # 轴标签 - 更专业的排版\n",
    "    ax.set_xlabel('Time Sequence',\n",
    "                 fontsize=42, weight='bold',\n",
    "                 labelpad=20, color='#2F2F2F')\n",
    "    ax.set_ylabel('Package Fullness (%)',\n",
    "                 fontsize=42, weight='bold',\n",
    "                 labelpad=22, color='#2F2F2F')\n",
    "\n",
    "    # 标题 - 更清晰的层次结构\n",
    "    ax.set_title('Package Fullness Changes Around Broken Yarn Events\\n(Top 6 Devices)',\n",
    "                fontsize=46, weight='bold', pad=24,\n",
    "                color='#2F2F2F')\n",
    "\n",
    "    # 转换时间戳\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "    # 收集所有故障记录\n",
    "    all_broken_records = []\n",
    "    line_handles = []\n",
    "\n",
    "    # 更专业的配色方案\n",
    "    colors = [\n",
    "        '#4E79A7', '#F28E2B', '#E15759', '#76B7B2',\n",
    "        '#59A14F', '#EDC948', '#B07AA1', '#FF9DA7',\n",
    "        '#9C755F', '#BAB0AC'\n",
    "    ]\n",
    "\n",
    "    # 为每个设备创建更清晰的视觉区分\n",
    "    for device_idx, (device_name, _) in enumerate(top_devices.iterrows()):\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        for status_col, _, fullness_col in spindle_cols:\n",
    "            broken_mask = (device_data[status_col] == 3)\n",
    "            broken_indices = device_data[broken_mask].index\n",
    "\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # 处理故障事件\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "            if current_start is not None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # 分析满度变化\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                prev_idx = device_data.index.get_loc(start_idx) - 1\n",
    "                while prev_idx >= 0 and device_data.iloc[prev_idx][status_col] == 3:\n",
    "                    prev_idx -= 1\n",
    "\n",
    "                next_idx = device_data.index.get_loc(end_idx) + 1\n",
    "                while next_idx < len(device_data) and device_data.iloc[next_idx][status_col] == 3:\n",
    "                    next_idx += 1\n",
    "\n",
    "                if prev_idx >= 0 and next_idx < len(device_data):\n",
    "                    record = {\n",
    "                        'Device': device_name,\n",
    "                        'Spindle': status_col.split('_')[1],\n",
    "                        'Broken Start Time': device_data.loc[start_idx, 'created_at'],\n",
    "                        'Broken End Time': device_data.loc[end_idx, 'created_at'],\n",
    "                        'Pre-Fullness': device_data.iloc[prev_idx][fullness_col],\n",
    "                        'Broken Fullness': device_data.loc[start_idx, fullness_col],\n",
    "                        'Post-Fullness': device_data.iloc[next_idx][fullness_col],\n",
    "                        'Duration (min)': (device_data.loc[end_idx, 'created_at'] - device_data.loc[start_idx, 'created_at']).total_seconds() / 60\n",
    "                    }\n",
    "                    device_records.append(record)\n",
    "                    all_broken_records.append(record)\n",
    "\n",
    "        # 绘制样本记录 - 更清晰的视觉表现\n",
    "        if device_records:\n",
    "            sample_records = pd.DataFrame(device_records).sample(min(5, len(device_records)), random_state=42)\n",
    "            for record_idx, (_, record) in enumerate(sample_records.iterrows()):\n",
    "                color = colors[device_idx % len(colors)]\n",
    "                line, = ax.plot(['Pre-Broken', 'Broken', 'Post-Broken'],\n",
    "                              [record['Pre-Fullness'], record['Broken Fullness'], record['Post-Fullness']],\n",
    "                              marker='o',\n",
    "                              markersize=18,\n",
    "                              markeredgewidth=2,\n",
    "                              linewidth=6,\n",
    "                              color=color,\n",
    "                              markeredgecolor='white',\n",
    "                              alpha=0.9,\n",
    "                              label=f\"{record['Device']}-{record['Spindle']} @ {record['Broken Start Time'].strftime('%H:%M')}\")\n",
    "                line_handles.append(line)\n",
    "\n",
    "    # 目标满度线 - 更清晰的视觉表现\n",
    "    target_line = ax.axhline(y=100, color='#E15759', linestyle='--',\n",
    "                            linewidth=4, alpha=0.8,\n",
    "                            label='Target Fullness (100%)')\n",
    "    line_handles.append(target_line)\n",
    "\n",
    "    # 图例 - 更专业的布局\n",
    "    legend = ax.legend(handles=line_handles,\n",
    "                     bbox_to_anchor=(1.02, 0.98),\n",
    "                     loc='upper left',\n",
    "                     fontsize=24,\n",
    "                     framealpha=0.95,\n",
    "                     facecolor='white',\n",
    "                     edgecolor='#DDDDDD',\n",
    "                     borderpad=0.8,\n",
    "                     labelspacing=0.5,\n",
    "                     handlelength=1.5,\n",
    "                     handletextpad=0.5,\n",
    "                     frameon=True,\n",
    "                     borderaxespad=0.5)\n",
    "\n",
    "    # 图例文本样式\n",
    "    for text in legend.get_texts():\n",
    "        text.set_fontweight('bold')\n",
    "        text.set_color('#2F2F2F')\n",
    "\n",
    "    # 调整布局 - 更合理的空间分配\n",
    "    plt.subplots_adjust(left=0.08, right=0.68, top=0.90, bottom=0.10)\n",
    "\n",
    "    # 图形边框 - 更精致的样式\n",
    "    fig.patch.set_edgecolor('#2F2F2F')\n",
    "    fig.patch.set_linewidth(4)\n",
    "\n",
    "    # 保存图形\n",
    "    plt.savefig('broken_yarn_analysis_professional_design.png',\n",
    "               bbox_inches='tight',\n",
    "               dpi=1200,\n",
    "               facecolor=fig.get_facecolor(),\n",
    "               edgecolor=fig.get_edgecolor(),\n",
    "               transparent=False)\n",
    "\n",
    "    plt.show()"
   ],
   "id": "72db2a6a098c29c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "# 设置专业学术风格参数\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.weight'] = 'bold'  # 使用bold而非extra bold保持专业感\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['axes.linewidth'] = 2.5  # 适中轴线宽度\n",
    "plt.rcParams['font.size'] = 16  # 适度字体大小\n",
    "plt.rcParams['axes.titlesize'] = 18  # 标题大小\n",
    "plt.rcParams['axes.labelsize'] = 24  # 轴标签大小\n",
    "plt.rcParams['xtick.labelsize'] = 26  # 刻度标签\n",
    "plt.rcParams['ytick.labelsize'] = 26\n",
    "plt.rcParams['legend.fontsize'] = 15\n",
    "plt.rcParams['grid.linewidth'] = 1.2  # 细网格线\n",
    "plt.rcParams['lines.linewidth'] = 3.5  # 数据线宽度\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['font.family'] = 'Arial'  # 使用更专业的英文字体\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# [数据加载和处理代码保持不变...]\n",
    "\n",
    "# 创建图形，优化布局\n",
    "fig, ax = plt.subplots(figsize=(12, 6), facecolor='white')  # 调整为更紧凑的比例\n",
    "\n",
    "# 设置边框线\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(2.5)\n",
    "    spine.set_color('black')\n",
    "\n",
    "# 优化刻度设置\n",
    "ax.tick_params(axis='both', which='major',\n",
    "               width=2, length=8,\n",
    "               labelsize=15, colors='black', pad=8)\n",
    "\n",
    "# 绘制分布图\n",
    "x = np.linspace(min_val, max_val, 1000)\n",
    "ax.plot(x, kde_normal(x), linewidth=3.5,\n",
    "        label='Normal State',\n",
    "        color='#1f77b4', alpha=0.9)\n",
    "ax.plot(x, kde_broken(x), linewidth=3.5,\n",
    "        label='Broken State',\n",
    "        color='#ff7f0e', alpha=0.9)\n",
    "\n",
    "# 添加阈值线\n",
    "ax.axvline(x=intersection, color='red',\n",
    "           linestyle='--', linewidth=3,\n",
    "           label=f'Intersection value : {intersection:.2f}%')\n",
    "\n",
    "# 优化标题和标签布局\n",
    "# ax.set_title('Package Fullness: Normal vs Broken States(Top 6 devices)',\n",
    "#              fontsize=22, pad=15, weight='bold')\n",
    "ax.set_xlabel('Package Fullness (%)',\n",
    "              fontsize=22, labelpad=16, weight='bold')\n",
    "ax.set_ylabel('Probability Density',\n",
    "              fontsize=22, labelpad=16, weight='bold')\n",
    "\n",
    "# 优化图例布局\n",
    "legend = ax.legend(fontsize=15, framealpha=1,\n",
    "                   frameon=True, edgecolor='black',\n",
    "                   facecolor='white', loc='upper right',\n",
    "                   borderpad=0.8)\n",
    "legend.get_frame().set_linewidth(2)\n",
    "legend.get_frame().set_boxstyle('round', pad=0.5, rounding_size=0.5)\n",
    "\n",
    "# 优化网格线\n",
    "ax.grid(True, linestyle=':', alpha=0.5, linewidth=1.2)\n",
    "\n",
    "# 调整文本紧凑性\n",
    "plt.tight_layout(pad=2.5)\n",
    "\n",
    "# 统一文本风格\n",
    "for item in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(18)\n",
    "    item.set_weight('bold')\n",
    "\n",
    "# 优化图例文本\n",
    "for text in legend.get_texts():\n",
    "    text.set_weight('bold')\n",
    "    text.set_fontsize(18)\n",
    "\n",
    "# 确保白色背景\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# 最终调整元素间距\n",
    "plt.subplots_adjust(left=0.12, right=0.95, top=0.9, bottom=0.15)\n",
    "\n",
    "# 保存高质量图片\n",
    "plt.savefig('package_fullness_distribution.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ],
   "id": "bf1c3445ba8739a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "# 设置全局字体大小为原来的两倍，并加粗\n",
    "plt.rcParams['font.weight'] = 'bold'  # 全局加粗\n",
    "plt.rcParams['axes.titlesize'] = 18 * 2  # 标题大小\n",
    "plt.rcParams['axes.labelsize'] = 14 * 2  # 坐标轴标签大小\n",
    "plt.rcParams['font.size'] = 12 * 2  # 字体大小\n",
    "plt.rcParams['legend.fontsize'] = 12 * 2  # 图例大小\n",
    "plt.rcParams['xtick.labelsize'] = 12 * 2  # x轴刻度标签大小\n",
    "plt.rcParams['ytick.labelsize'] = 12 * 2  # y轴刻度标签大小\n",
    "\n",
    "# 设置高DPI以提高图像质量\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "# 设置中文字体和正确的负号显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "\n",
    "# Spindle状态映射\n",
    "status_mapping = {\n",
    "    0: \"Stopped\",\n",
    "    1: \"Running\",\n",
    "    2: \"Full Package\",\n",
    "    3: \"Broken Yarn\",\n",
    "    5: \"Overheated\",\n",
    "    6: \"Speed Loss\",\n",
    "    7: \"Disconnected\",\n",
    "    11: \"Abnormal\"\n",
    "}\n",
    "\n",
    "# 1. 准备纺锤状态、速度和满度列名（确保一对一对应）\n",
    "spindle_cols = []\n",
    "for i in range(1, 101):\n",
    "    status_col = f\"D_dw{i}\"\n",
    "    speed_col = f\"V_dw{i}\"\n",
    "    fullness_col = f\"P_dw{i}\"\n",
    "    if status_col in df.columns and speed_col in df.columns and fullness_col in df.columns:\n",
    "        spindle_cols.append((status_col, speed_col, fullness_col))\n",
    "\n",
    "# 2. 找到断纱最多的前10台设备\n",
    "device_broken_counts = defaultdict(int)\n",
    "\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    # 找到所有断纱记录并按设备分组\n",
    "    broken_mask = df[status_col] == 3\n",
    "    if broken_mask.any():\n",
    "        broken_by_device = df[broken_mask].groupby('subsystem').size()\n",
    "        for device, count in broken_by_device.items():\n",
    "            device_broken_counts[device] += count\n",
    "\n",
    "# 转换为DataFrame并排序\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['Broken Count'])\n",
    "    top_devices = top_devices.sort_values('Broken Count', ascending=False).head(10)\n",
    "    print(\"\\nTop 10 Devices with Most Broken Yarn Occurrences:\")\n",
    "    print(top_devices)\n",
    "\n",
    "    # 3. 分析这些设备断纱前后满度变化\n",
    "    plt.figure(figsize=(12*2, 8*2))\n",
    "    all_broken_records = []\n",
    "\n",
    "    # 存储所有设备在正常和断纱状态下的满度\n",
    "    all_normal_fullness = []\n",
    "    all_broken_fullness = []\n",
    "\n",
    "    for device_name in top_devices.index:\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        # 收集该设备在正常和断纱状态下的满度\n",
    "        device_normal_fullness = []\n",
    "        device_broken_fullness = []\n",
    "\n",
    "        for status_col, _, fullness_col in spindle_cols:\n",
    "            # 正常状态（status 1: Running）\n",
    "            normal_mask = (device_data[status_col] == 1)\n",
    "            device_normal_fullness.extend(device_data[normal_mask][fullness_col].dropna().values)\n",
    "\n",
    "            # 断纱状态（status 3: Broken Yarn）\n",
    "            broken_mask = (device_data[status_col] == 3)\n",
    "            device_broken_fullness.extend(device_data[broken_mask][fullness_col].dropna().values)\n",
    "\n",
    "            # 找到所有断纱记录\n",
    "            broken_indices = device_data[broken_mask].index\n",
    "\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # 获取所有断纱事件的开始和结束\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue  # 连续的断纱状态\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "\n",
    "            if current_start is not None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # 分析每个断纱事件前后的满度变化\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                # 获取断纱前的最后一条正常记录\n",
    "                prev_idx = device_data.index.get_loc(start_idx) - 1\n",
    "                while prev_idx >= 0 and device_data.iloc[prev_idx][status_col] == 3:\n",
    "                    prev_idx -= 1\n",
    "\n",
    "                # 获取断纱后的第一条正常记录\n",
    "                next_idx = device_data.index.get_loc(end_idx) + 1\n",
    "                while next_idx < len(device_data) and device_data.iloc[next_idx][status_col] == 3:\n",
    "                    next_idx += 1\n",
    "\n",
    "                if prev_idx >= 0 and next_idx < len(device_data):\n",
    "                    record = {\n",
    "                        'Device': device_name,\n",
    "                        'Spindle': status_col.split('_')[1],\n",
    "                        'Broken Start Time': device_data.loc[start_idx, 'created_at'],\n",
    "                        'Broken End Time': device_data.loc[end_idx, 'created_at'],\n",
    "                        'Pre-Fullness': device_data.iloc[prev_idx][fullness_col],\n",
    "                        'Broken Fullness': device_data.loc[start_idx, fullness_col],\n",
    "                        'Post-Fullness': device_data.iloc[next_idx][fullness_col],\n",
    "                        'Duration (min)': (device_data.loc[end_idx, 'created_at'] - device_data.loc[start_idx, 'created_at']).total_seconds() / 60\n",
    "                    }\n",
    "                    device_records.append(record)\n",
    "                    all_broken_records.append(record)\n",
    "\n",
    "        # 绘制典型断纱事件（每个设备最多5个样本）\n",
    "        if device_records:\n",
    "            sample_records = pd.DataFrame(device_records).sample(min(5, len(device_records)), random_state=42)\n",
    "            for _, record in sample_records.iterrows():\n",
    "                plt.plot(['Pre-Broken', 'Broken', 'Post-Broken'],\n",
    "                         [record['Pre-Fullness'], record['Broken Fullness'], record['Post-Fullness']],\n",
    "                         marker='o',\n",
    "                         markersize=6 * 4,  # 增加标记大小\n",
    "                         linewidth=4 * 2 * 2 ,   # 再加粗线条，设置为32\n",
    "                         label=f\"{record['Device']}-{record['Spindle']}@{record['Broken Start Time'].strftime('%H:%M')}\")\n",
    "\n",
    "        # 添加到全局集合\n",
    "        all_normal_fullness.extend(device_normal_fullness)\n",
    "        all_broken_fullness.extend(device_broken_fullness)\n",
    "\n",
    "    # 设置绘图设置\n",
    "    plt.axhline(y=100, color='gray', linestyle='--', label='Target Fullness (100%)', linewidth=4*2*2)  # 再增粗线条\n",
    "    plt.title('Package Fullness Changes Around Broken Yarn Events\\n(Top 10 Devices)', fontsize=12*2, fontweight='bold')\n",
    "    plt.xlabel('Time Point', fontsize=12*2, fontweight='bold')\n",
    "    plt.ylabel('Package Fullness (%)', fontsize=12*2, fontweight='bold')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12*2, title_fontsize=12*2)  # 保持图例大小不变\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout(rect=[0, 0, 0.8, 1])  # 调整布局以适应图例放置\n",
    "    plt.show()\n",
    "\n",
    "    # 4. 计算安全操作阈值\n",
    "    if all_normal_fullness and all_broken_fullness:\n",
    "        # 转换为numpy数组并移除无效值\n",
    "        normal_data = np.array(all_normal_fullness)\n",
    "        normal_data = normal_data[~np.isnan(normal_data)]\n",
    "\n",
    "        broken_data = np.array(all_broken_fullness)\n",
    "        broken_data = broken_data[~np.isnan(broken_data)]\n",
    "\n",
    "        # 计算核密度估计\n",
    "        kde_normal = gaussian_kde(normal_data)\n",
    "        kde_broken = gaussian_kde(broken_data)\n",
    "\n",
    "        # 定义函数以查找交点\n",
    "        def find_intersection(x):\n",
    "            return kde_normal(x) - kde_broken(x)\n",
    "\n",
    "        # 查找交点\n",
    "        min_val = min(min(normal_data), min(broken_data))\n",
    "        max_val = max(max(normal_data), max(broken_data))\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            intersection = brentq(find_intersection, min_val, max_val)\n",
    "            print(f\"\\nSafe Operation Threshold (Normal vs Broken Distribution Intersection): {intersection:.2f}%\")\n",
    "\n",
    "            # Create figure with white background and adjusted layout\n",
    "            fig, ax = plt.subplots(figsize=(14, 8), facecolor='white')\n",
    "\n",
    "            # Make left side wider by adjusting subplot position\n",
    "            fig.subplots_adjust(left=0.15, right=0.9, top=0.9, bottom=0.15)\n",
    "\n",
    "            # Set background color\n",
    "            ax.set_facecolor('white')\n",
    "\n",
    "            # Thicker spines (borders)\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_linewidth(3)\n",
    "\n",
    "            # Plot distributions with thicker lines\n",
    "            x = np.linspace(min_val, max_val, 1000)\n",
    "            ax.plot(x, kde_normal(x), linewidth=6, label='Normal State Distribution')\n",
    "            ax.plot(x, kde_broken(x), linewidth=6, label='Broken State Distribution')\n",
    "            ax.axvline(x=intersection, color='red', linestyle='--', linewidth=6,\n",
    "                      label=f'Intersection: {intersection:.2f}%')\n",
    "\n",
    "            # Set titles and labels with increased weight\n",
    "            ax.set_title('Package Fullness Distribution: Normal vs Broken States',\n",
    "                        fontsize=24, weight='bold', pad=20)\n",
    "            ax.set_xlabel('Package Fullness (%)', fontsize=21, weight='bold', labelpad=15)\n",
    "            ax.set_ylabel('Probability Density', fontsize=21, weight='bold', labelpad=15)\n",
    "\n",
    "            # Customize legend with thicker border\n",
    "            legend = ax.legend(fontsize=18, framealpha=1, facecolor='white')\n",
    "            legend.get_frame().set_linewidth(3)\n",
    "            legend.get_frame().set_edgecolor('black')\n",
    "\n",
    "            # Thicker grid lines\n",
    "            ax.grid(True, linestyle='--', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "            # Save with tight layout and white background\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('distribution_plot.png', bbox_inches='tight', facecolor='white')\n",
    "            plt.show()\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"\\nCannot find intersection between normal and broken state distributions\")\n",
    "            intersection = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 5. 计算并显示包装满度变化统计\n",
    "    if all_broken_records:\n",
    "        fullness_changes = pd.DataFrame(all_broken_records)\n",
    "        fullness_changes['Fullness Decrease'] = fullness_changes['Pre-Fullness'] - fullness_changes['Broken Fullness']\n",
    "        fullness_changes['Fullness Recovery'] = fullness_changes['Post-Fullness'] - fullness_changes['Broken Fullness']\n",
    "\n",
    "        print(\"\\nPackage Fullness Change Statistics Around Broken Yarn:\")\n",
    "        print(fullness_changes[['Device', 'Spindle', 'Broken Start Time', 'Duration (min)',\n",
    "                              'Pre-Fullness', 'Broken Fullness', 'Post-Fullness',\n",
    "                              'Fullness Decrease', 'Fullness Recovery']].describe())\n",
    "\n",
    "        # 绘制包装满度变化的箱线图\n",
    "        plt.figure(figsize=(12*2, 6*2))  # 保持图的大小不变\n",
    "        fullness_changes[['Pre-Fullness', 'Broken Fullness', 'Post-Fullness']].boxplot(\n",
    "            patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightblue', edgecolor='black', lw=6*2*2, alpha=0.8),  # 再加粗线条，设置为12\n",
    "            whiskerprops=dict(color='black', lw=6*2*2),  # 再加粗线条，设置为12\n",
    "            capprops=dict(color='black', lw=6*2*2),  # 再加粗线条，设置为12\n",
    "            medianprops=dict(color='black', lw=6*2*2),  # 再加粗线条，设置为12\n",
    "            flierprops=dict(color='red', markeredgecolor='red', markerfacecolor='red', markeredgewidth=6, markersize=10*2))  # 填充颜色和标记大小\n",
    "        plt.title('Package Fullness Distribution Around Broken Yarn', fontsize=12*2, fontweight='bold')\n",
    "        plt.ylabel('Package Fullness (%)', fontsize=12*2, fontweight='bold')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 按设备分组统计\n",
    "        print(\"\\nBroken Yarn and Package Fullness Statistics by Device:\")\n",
    "        device_stats = fullness_changes.groupby('Device').agg({\n",
    "            'Spindle': 'count',\n",
    "            'Duration (min)': 'mean',\n",
    "            'Fullness Decrease': 'mean',\n",
    "            'Fullness Recovery': 'mean'\n",
    "        }).rename(columns={'Spindle': 'Broken Count', 'Duration (min)': 'Avg Duration (min)'})\n",
    "        print(device_stats)\n",
    "\n",
    "        # 6. 分析断纱和包装满度之间的关系\n",
    "        print(\"\\nRelationship Between Broken Yarn and Package Fullness:\")\n",
    "        # 计算断纱前包装满度的分布\n",
    "        print(\"\\nPre-Broken Package Fullness Distribution:\")\n",
    "        print(fullness_changes['Pre-Fullness'].describe())\n",
    "\n",
    "        # 计算断纱前满度低于交点的百分比\n",
    "        if intersection:\n",
    "            low_fullness_ratio = (fullness_changes['Pre-Fullness'] < intersection).mean() * 100\n",
    "            print(f\"\\nPercentage of Pre-Broken Fullness Below Intersection ({intersection:.2f}%): {low_fullness_ratio:.2f}%\")\n",
    "\n",
    "        # 绘制断纱前包装满度的直方图\n",
    "        plt.figure(figsize=(12*2, 6*2))\n",
    "        plt.hist(fullness_changes['Pre-Fullness'], bins=20, edgecolor='black')\n",
    "        if intersection:\n",
    "            plt.axvline(x=intersection, color='red', linestyle='--', linewidth=2*2,\n",
    "                       label=f'Intersection: {intersection:.2f}%')\n",
    "        plt.title('Pre-Broken Package Fullness Distribution', fontsize=12*2, fontweight='bold')\n",
    "        plt.xlabel('Package Fullness (%)', fontsize=12*2, fontweight='bold')\n",
    "        plt.ylabel('Frequency', fontsize=12*2, fontweight='bold')\n",
    "        plt.legend(fontsize=12*2)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nNo qualified pre- and post-broken records found\")\n",
    "else:\n",
    "    print(\"\\nNo broken yarn spindle records found in the data\")"
   ],
   "id": "e6deafcefcf48f07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#断纱卷装程度分析以及安全区间估计\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "# 锭位状态映射\n",
    "status_mapping = {\n",
    "    0: \"停锭\",\n",
    "    1: \"启动\",\n",
    "    2: \"满管\",\n",
    "    3: \"断纱\",\n",
    "    5: \"过热\",\n",
    "    6: \"失速\",\n",
    "    7: \"掉线\",\n",
    "    11: \"异常\"\n",
    "}\n",
    "\n",
    "# 1. 准备锭位状态、速度和卷装程度的列名（确保一一对应）\n",
    "spindle_cols = []\n",
    "for i in range(1, 101):\n",
    "    status_col = f\"D_dw{i}\"\n",
    "    speed_col = f\"V_dw{i}\"\n",
    "    fullness_col = f\"P_dw{i}\"\n",
    "    if status_col in df.columns and speed_col in df.columns and fullness_col in df.columns:\n",
    "        spindle_cols.append((status_col, speed_col, fullness_col))\n",
    "\n",
    "# 2. 找出断纱最多的前十台设备\n",
    "device_broken_counts = defaultdict(int)\n",
    "\n",
    "for status_col, _, _ in spindle_cols:\n",
    "    # 找出所有断纱记录并按设备分组计数\n",
    "    broken_mask = df[status_col] == 3\n",
    "    if broken_mask.any():\n",
    "        broken_by_device = df[broken_mask].groupby('subsystem').size()\n",
    "        for device, count in broken_by_device.items():\n",
    "            device_broken_counts[device] += count\n",
    "\n",
    "# 转换为DataFrame并排序\n",
    "if device_broken_counts:\n",
    "    top_devices = pd.DataFrame.from_dict(device_broken_counts, orient='index', columns=['断纱次数'])\n",
    "    top_devices = top_devices.sort_values('断纱次数', ascending=False).head(10)\n",
    "    print(\"\\n断纱次数最多的前十台设备:\")\n",
    "    print(top_devices)\n",
    "\n",
    "    # 3. 分析这些设备的断纱前后卷装程度变化（确保使用同一行数据）\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    all_broken_records = []\n",
    "    all_normal_records = []  # 用于存储正常状态的数据\n",
    "\n",
    "    for device_name in top_devices.index:\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        device_data = df[df['subsystem'] == device_name].sort_values('created_at')\n",
    "        device_records = []\n",
    "\n",
    "        # 收集正常状态的数据（用于对比分析）\n",
    "        for _, row in device_data.iterrows():\n",
    "            for status_col, _, fullness_col in spindle_cols:\n",
    "                if row[status_col] != 3:  # 非断纱状态\n",
    "                    all_normal_records.append({\n",
    "                        '设备': device_name,\n",
    "                        '锭位': status_col.split('_')[1],\n",
    "                        '时间': row['created_at'],\n",
    "                        '状态': row[status_col],\n",
    "                        '卷装程度': row[fullness_col],\n",
    "                        '是否断纱': 0  # 0表示正常\n",
    "                    })\n",
    "\n",
    "        for status_col, _, fullness_col in spindle_cols:\n",
    "            # 找出该设备该锭位的所有断纱记录\n",
    "            broken_mask = (device_data[status_col] == 3)\n",
    "            broken_indices = device_data[broken_mask].index\n",
    "\n",
    "            if len(broken_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # 获取所有断纱事件的开始和结束\n",
    "            broken_events = []\n",
    "            current_start = None\n",
    "\n",
    "            for idx in broken_indices:\n",
    "                if current_start is None:\n",
    "                    current_start = idx\n",
    "                elif idx == broken_indices[broken_indices.get_loc(current_start) + 1]:\n",
    "                    continue  # 连续断纱状态\n",
    "                else:\n",
    "                    broken_events.append((current_start, broken_indices[broken_indices.get_loc(current_start) + 1 - 1]))\n",
    "                    current_start = idx\n",
    "\n",
    "            if current_start is not None:\n",
    "                broken_events.append((current_start, broken_indices[-1]))\n",
    "\n",
    "            # 对每个断纱事件分析\n",
    "            for start_idx, end_idx in broken_events:\n",
    "                record = {\n",
    "                    '设备': device_name,\n",
    "                    '锭位': status_col.split('_')[1],\n",
    "                    '断纱开始时间': device_data.loc[start_idx, 'created_at'],\n",
    "                    '断纱结束时间': device_data.loc[end_idx, 'created_at'],\n",
    "                    '断纱时卷装程度': device_data.loc[start_idx, fullness_col],\n",
    "                    '断纱持续时间': (device_data.loc[end_idx, 'created_at'] - device_data.loc[start_idx, 'created_at']).total_seconds() / 60,\n",
    "                    '是否断纱': 1  # 1表示断纱\n",
    "                }\n",
    "\n",
    "                # 获取断纱前的最后一个非断纱记录（同一锭位）\n",
    "                prev_idx = device_data.index.get_loc(start_idx) - 1\n",
    "                while prev_idx >= 0 and device_data.iloc[prev_idx][status_col] == 3:\n",
    "                    prev_idx -= 1\n",
    "\n",
    "                # 获取断纱后的第一个非断纱记录（同一锭位）\n",
    "                next_idx = device_data.index.get_loc(end_idx) + 1\n",
    "                while next_idx < len(device_data) and device_data.iloc[next_idx][status_col] == 3:\n",
    "                    next_idx += 1\n",
    "\n",
    "                if prev_idx >= 0:\n",
    "                    record['前卷装程度'] = device_data.iloc[prev_idx][fullness_col]\n",
    "                else:\n",
    "                    record['前卷装程度'] = np.nan\n",
    "\n",
    "                if next_idx < len(device_data):\n",
    "                    record['后卷装程度'] = device_data.iloc[next_idx][fullness_col]\n",
    "                else:\n",
    "                    record['后卷装程度'] = np.nan\n",
    "\n",
    "                device_records.append(record)\n",
    "                all_broken_records.append(record)\n",
    "\n",
    "                # 将断纱记录也加入all_normal_records用于对比\n",
    "                all_normal_records.append({\n",
    "                    '设备': device_name,\n",
    "                    '锭位': status_col.split('_')[1],\n",
    "                    '时间': device_data.loc[start_idx, 'created_at'],\n",
    "                    '状态': 3,  # 断纱状态\n",
    "                    '卷装程度': device_data.loc[start_idx, fullness_col],\n",
    "                    '是否断纱': 1\n",
    "                })\n",
    "\n",
    "        # 绘制该设备的典型断纱事件（最多5个）\n",
    "        if device_records:\n",
    "            device_records_df = pd.DataFrame(device_records)\n",
    "            # 只选择有前后卷装程度记录的事件\n",
    "            valid_records = device_records_df.dropna(subset=['前卷装程度', '后卷装程度'])\n",
    "            if len(valid_records) > 0:\n",
    "                sample_records = valid_records.sample(min(5, len(valid_records)), random_state=42)\n",
    "                for _, record in sample_records.iterrows():\n",
    "                    plt.plot(['断纱前', '断纱时', '断纱后'],\n",
    "                             [record['前卷装程度'], record['断纱时卷装程度'], record['后卷装程度']],\n",
    "                             marker='o',\n",
    "                             label=f\"{record['设备']}-{record['锭位']}@{record['断纱开始时间'].strftime('%H:%M')}\")\n",
    "\n",
    "    # 设置图表\n",
    "    plt.axhline(y=100, color='gray', linestyle='--', label='目标卷装程度(100%)')\n",
    "    plt.title('前十台设备典型断纱事件前后卷装程度变化')\n",
    "    plt.xlabel('时间点')\n",
    "    plt.ylabel('卷装程度(%)')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 4. 计算并显示卷装程度变化统计信息\n",
    "    if all_broken_records:\n",
    "        fullness_changes = pd.DataFrame(all_broken_records)\n",
    "        fullness_changes['卷装程度下降量'] = fullness_changes['前卷装程度'] - fullness_changes['断纱时卷装程度']\n",
    "        fullness_changes['卷装程度恢复量'] = fullness_changes['后卷装程度'] - fullness_changes['断纱时卷装程度']\n",
    "\n",
    "        print(\"\\n断纱前后卷装程度变化统计:\")\n",
    "        print(fullness_changes[['设备', '锭位', '断纱开始时间', '断纱持续时间',\n",
    "                              '前卷装程度', '断纱时卷装程度', '后卷装程度',\n",
    "                              '卷装程度下降量', '卷装程度恢复量']].describe())\n",
    "\n",
    "        # 绘制卷装程度变化箱线图\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        fullness_changes[['前卷装程度', '断纱时卷装程度', '后卷装程度']].boxplot()\n",
    "        plt.title('断纱前后卷装程度分布')\n",
    "        plt.ylabel('卷装程度(%)')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "        # 按设备分组统计\n",
    "        print(\"\\n按设备分组的断纱与卷装程度统计:\")\n",
    "        device_stats = fullness_changes.groupby('设备').agg({\n",
    "            '锭位': 'count',\n",
    "            '断纱持续时间': 'mean',\n",
    "            '卷装程度下降量': 'mean',\n",
    "            '卷装程度恢复量': 'mean'\n",
    "        }).rename(columns={'锭位': '断纱次数', '断纱持续时间': '平均持续时间(分钟)'})\n",
    "        print(device_stats)\n",
    "\n",
    "        # 5. 分析断纱与卷装程度的关联性，并推测安全操作区间\n",
    "        print(\"\\n断纱与卷装程度的关联性分析:\")\n",
    "\n",
    "        # 创建包含所有记录的DataFrame（正常和断纱）\n",
    "        all_records_df = pd.DataFrame(all_normal_records)\n",
    "\n",
    "        # 计算断纱和非断纱状态的卷装程度统计\n",
    "        print(\"\\n不同状态的卷装程度统计:\")\n",
    "        print(all_records_df.groupby('是否断纱')['卷装程度'].describe())\n",
    "\n",
    "        # 绘制卷装程度分布对比图\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # 正常状态的卷装程度分布\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(all_records_df[all_records_df['是否断纱'] == 0]['卷装程度'],\n",
    "                 bins=20, color='green', alpha=0.7, edgecolor='black')\n",
    "        plt.title('正常状态卷装程度分布')\n",
    "        plt.xlabel('卷装程度(%)')\n",
    "        plt.ylabel('频次')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        # 断纱状态的卷装程度分布\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(all_records_df[all_records_df['是否断纱'] == 1]['卷装程度'],\n",
    "                 bins=20, color='red', alpha=0.7, edgecolor='black')\n",
    "        plt.title('断纱状态卷装程度分布')\n",
    "        plt.xlabel('卷装程度(%)')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 使用核密度估计找出安全区间\n",
    "        normal_fullness = all_records_df[all_records_df['是否断纱'] == 0]['卷装程度']\n",
    "        broken_fullness = all_records_df[all_records_df['是否断纱'] == 1]['卷装程度']\n",
    "\n",
    "        # 计算正常和断纱状态的卷装程度分布差异\n",
    "        kde_normal = stats.gaussian_kde(normal_fullness.dropna())\n",
    "        kde_broken = stats.gaussian_kde(broken_fullness.dropna())\n",
    "\n",
    "        x = np.linspace(0, 100, 500)\n",
    "        y_normal = kde_normal(x)\n",
    "        y_broken = kde_broken(x)\n",
    "\n",
    "        # 找出断纱概率显著低于正常状态的区间\n",
    "        safety_threshold = 0.2  # 断纱概率低于20%的区间\n",
    "        safe_intervals = []\n",
    "        current_start = None\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            ratio = y_broken[i] / (y_normal[i] + y_broken[i] + 1e-6)  # 断纱占比\n",
    "            if ratio < safety_threshold:\n",
    "                if current_start is None:\n",
    "                    current_start = x[i]\n",
    "            else:\n",
    "                if current_start is not None:\n",
    "                    safe_intervals.append((current_start, x[i-1]))\n",
    "                    current_start = None\n",
    "\n",
    "        if current_start is not None:\n",
    "            safe_intervals.append((current_start, x[-1]))\n",
    "\n",
    "        # 合并相邻区间\n",
    "        merged_intervals = []\n",
    "        for interval in safe_intervals:\n",
    "            if not merged_intervals:\n",
    "                merged_intervals.append(list(interval))\n",
    "            else:\n",
    "                last = merged_intervals[-1]\n",
    "                if interval[0] - last[1] < 5:  # 如果间隔小于5%，合并\n",
    "                    last[1] = interval[1]\n",
    "                else:\n",
    "                    merged_intervals.append(list(interval))\n",
    "\n",
    "        # 显示安全操作区间\n",
    "        print(\"\\n推荐的卷装程度安全操作区间（断纱概率<20%）：\")\n",
    "        for interval in merged_intervals:\n",
    "            print(f\"{interval[0]:.1f}%-{interval[1]:.1f}%\")\n",
    "\n",
    "        # 绘制安全区间可视化\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(x, y_normal, label='正常状态分布', color='green')\n",
    "        plt.plot(x, y_broken, label='断纱状态分布', color='red')\n",
    "\n",
    "        # 标记安全区间\n",
    "        for interval in merged_intervals:\n",
    "            plt.axvspan(interval[0], interval[1], color='green', alpha=0.1)\n",
    "            plt.text(np.mean(interval), np.max(y_normal)*0.9,\n",
    "                    f\"安全区间\\n{interval[0]:.0f}-{interval[1]:.0f}%\",\n",
    "                    ha='center', va='center', bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "        plt.title('卷装程度安全操作区间分析')\n",
    "        plt.xlabel('卷装程度(%)')\n",
    "        plt.ylabel('概率密度')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "        # 6. 分析断纱前卷装程度特征\n",
    "        print(\"\\n断纱前卷装程度分析:\")\n",
    "        # 计算断纱前卷装程度的分布\n",
    "        print(\"\\n断纱前卷装程度分布:\")\n",
    "        print(fullness_changes['前卷装程度'].describe())\n",
    "\n",
    "        # 计算断纱前卷装程度低于50%的比例\n",
    "        low_fullness_ratio = (fullness_changes['前卷装程度'] < 50).mean() * 100\n",
    "        print(f\"\\n断纱前卷装程度低于50%的比例: {low_fullness_ratio:.2f}%\")\n",
    "\n",
    "        # 绘制断纱前卷装程度直方图\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(fullness_changes['前卷装程度'].dropna(), bins=20, edgecolor='black')\n",
    "        plt.title('断纱前卷装程度分布')\n",
    "        plt.xlabel('卷装程度(%)')\n",
    "        plt.ylabel('频次')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\n没有找到符合条件的断纱前后记录\")\n",
    "else:\n",
    "    print(\"\\n数据中没有发现断纱锭位记录\")"
   ],
   "id": "b4c0ced8c1dd993",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from scipy.stats import linregress\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "# 加载数据\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "\n",
    "# 锭位状态映射\n",
    "status_mapping = {\n",
    "    0: \"停锭\",\n",
    "    1: \"启动\",\n",
    "    2: \"满管\",\n",
    "    3: \"断纱\",\n",
    "    5: \"过热\",\n",
    "    6: \"失速\",\n",
    "    7: \"掉线\",\n",
    "    11: \"异常\"\n",
    "}\n",
    "\n",
    "# 1. 准备锭位状态、速度和卷装程度的列名\n",
    "spindle_cols = []\n",
    "for i in range(1, 101):\n",
    "    status_col = f\"D_dw{i}\"\n",
    "    speed_col = f\"V_dw{i}\"\n",
    "    fullness_col = f\"P_dw{i}\"\n",
    "    target_speed_col = f\"R_dw{i}\"\n",
    "    if all(col in df.columns for col in [status_col, speed_col, fullness_col, target_speed_col]):\n",
    "        spindle_cols.append((status_col, speed_col, fullness_col, target_speed_col))\n",
    "\n",
    "# 2. 分析卷装程度、速度偏离和断纱率的耦合关系\n",
    "analysis_results = []\n",
    "\n",
    "for status_col, speed_col, fullness_col, target_speed_col in spindle_cols:\n",
    "    # 计算速度偏离\n",
    "    df['speed_deviation'] = df[speed_col] - df[target_speed_col]\n",
    "    \n",
    "    # 计算断纱率（断纱记录占总记录的比例）\n",
    "    broken_rate = (df[status_col] == 3).mean()\n",
    "    \n",
    "    # 计算平均卷装程度和平均速度偏离\n",
    "    avg_fullness = df[fullness_col].mean()\n",
    "    avg_speed_deviation = df['speed_deviation'].mean()\n",
    "    \n",
    "    # 计算不同卷装程度区间的断纱率\n",
    "    fullness_bins = pd.cut(df[fullness_col], bins=np.linspace(0, 100, 11))\n",
    "    fullness_group = df.groupby(fullness_bins)\n",
    "    \n",
    "    for interval, group in fullness_group:\n",
    "        interval_broken_rate = (group[status_col] == 3).mean()\n",
    "        interval_avg_speed_deviation = (group[speed_col] - group[target_speed_col]).mean()\n",
    "        \n",
    "        analysis_results.append({\n",
    "            '锭位': status_col.split('_')[1],\n",
    "            '卷装程度区间': interval,\n",
    "            '断纱率': interval_broken_rate,\n",
    "            '平均速度偏离': interval_avg_speed_deviation,\n",
    "            '记录数': len(group)\n",
    "        })\n",
    "\n",
    "# 转换为DataFrame\n",
    "coupling_df = pd.DataFrame(analysis_results)\n",
    "\n",
    "# 3. 可视化分析结果\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# 3.1 卷装程度与断纱率的关系\n",
    "plt.subplot(2, 2, 1)\n",
    "fullness_vs_broken = coupling_df.groupby('卷装程度区间')['断纱率'].mean()\n",
    "fullness_vs_broken.plot(kind='bar', color='skyblue')\n",
    "plt.title('不同卷装程度区间的平均断纱率')\n",
    "plt.xlabel('卷装程度区间')\n",
    "plt.ylabel('断纱率')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# 3.2 速度偏离与断纱率的关系\n",
    "plt.subplot(2, 2, 2)\n",
    "deviation_vs_broken = coupling_df.groupby(pd.cut(coupling_df['平均速度偏离'], bins=20))['断纱率'].mean()\n",
    "deviation_vs_broken.plot(kind='bar', color='salmon')\n",
    "plt.title('不同速度偏离区间的平均断纱率')\n",
    "plt.xlabel('速度偏离区间')\n",
    "plt.ylabel('断纱率')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# 3.3 卷装程度×速度偏离×断纱率的热力图\n",
    "plt.subplot(2, 2, 3)\n",
    "heatmap_data = coupling_df.pivot_table(index='卷装程度区间', \n",
    "                                      columns=pd.cut(coupling_df['平均速度偏离'], bins=5), \n",
    "                                      values='断纱率', \n",
    "                                      aggfunc='mean')\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".3f\", cmap=\"YlOrRd\")\n",
    "plt.title('卷装程度×速度偏离×断纱率热力图')\n",
    "plt.xlabel('速度偏离区间')\n",
    "plt.ylabel('卷装程度区间')\n",
    "\n",
    "# 3.4 断纱率与卷装程度、速度偏离的回归分析\n",
    "plt.subplot(2, 2, 4)\n",
    "x = coupling_df['卷装程度区间'].apply(lambda x: x.mid)\n",
    "y = coupling_df['断纱率']\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "plt.scatter(x, y, alpha=0.5)\n",
    "plt.plot(x, intercept + slope * x, 'r', label=f'线性拟合 (R²={r_value**2:.2f})')\n",
    "plt.title('卷装程度与断纱率的线性回归')\n",
    "plt.xlabel('卷装程度区间中值')\n",
    "plt.ylabel('断纱率')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. 高级耦合分析\n",
    "# 4.1 按卷装程度和速度偏离分组统计断纱率\n",
    "coupling_df['卷装程度类别'] = pd.cut(coupling_df['卷装程度区间'].apply(lambda x: x.mid), \n",
    "                                 bins=[0, 30, 70, 100], \n",
    "                                 labels=['低卷装程度', '中卷装程度', '高卷装程度'])\n",
    "coupling_df['速度偏离类别'] = pd.cut(coupling_df['平均速度偏离'], \n",
    "                                  bins=[-np.inf, -5, 5, np.inf], \n",
    "                                  labels=['负偏离大', '正常范围', '正偏离大'])\n",
    "\n",
    "coupling_stats = coupling_df.groupby(['卷装程度类别', '速度偏离类别']).agg({\n",
    "    '断纱率': ['mean', 'count'],\n",
    "    '平均速度偏离': 'mean'\n",
    "}).unstack()\n",
    "\n",
    "print(\"\\n卷装程度×速度偏离×断纱率耦合分析:\")\n",
    "print(coupling_stats)\n",
    "\n",
    "# 4.2 多因素方差分析（需要statsmodels）\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.formula.api import ols\n",
    "    \n",
    "    # 准备数据\n",
    "    anova_df = coupling_df.copy()\n",
    "    anova_df['卷装程度中值'] = anova_df['卷装程度区间'].apply(lambda x: x.mid)\n",
    "    anova_df = anova_df.dropna(subset=['断纱率', '卷装程度中值', '平均速度偏离'])\n",
    "    \n",
    "    # 执行双因素方差分析\n",
    "    model = ols('断纱率 ~ C(卷装程度类别) + C(速度偏离类别) + C(卷装程度类别):C(速度偏离类别)', data=anova_df).fit()\n",
    "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "    \n",
    "    print(\"\\n双因素方差分析结果:\")\n",
    "    print(anova_table)\n",
    "except ImportError:\n",
    "    print(\"\\n无法执行方差分析，请安装statsmodels库: pip install statsmodels\")\n",
    "\n",
    "# 5. 找出高风险组合\n",
    "high_risk_combinations = coupling_df.groupby(['卷装程度类别', '速度偏离类别'])['断纱率'].mean().unstack()\n",
    "high_risk_combinations = high_risk_combinations.stack().reset_index()\n",
    "high_risk_combinations.columns = ['卷装程度类别', '速度偏离类别', '平均断纱率']\n",
    "high_risk_combinations = high_risk_combinations.sort_values('平均断纱率', ascending=False)\n",
    "\n",
    "print(\"\\n高风险组合排序:\")\n",
    "print(high_risk_combinations.head(10))\n",
    "\n",
    "# 6. 建议的安全操作区间\n",
    "safe_operating_ranges = coupling_df.groupby('卷装程度类别').agg({\n",
    "    '平均速度偏离': ['mean', 'std'],\n",
    "    '断纱率': 'mean'\n",
    "})\n",
    "\n",
    "safe_operating_ranges['建议速度偏离下限'] = safe_operating_ranges[('平均速度偏离', 'mean')] - 2 * safe_operating_ranges[('平均速度偏离', 'std')]\n",
    "safe_operating_ranges['建议速度偏离上限'] = safe_operating_ranges[('平均速度偏离', 'mean')] + 2 * safe_operating_ranges[('平均速度偏离', 'std')]\n",
    "\n",
    "print(\"\\n建议的安全操作区间(基于±2σ):\")\n",
    "print(safe_operating_ranges)"
   ],
   "id": "7d7970c54fc30cfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# #7能耗分析\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from termcolor import colored\n",
    "# plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "# plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "# # 读取数据\n",
    "# file_path = 'D:/code/junma/600000/0424/final2.csv'\n",
    "# df = pd.read_csv(file_path)\n",
    "#\n",
    "# # 确保时间列是datetime类型并按时间排序\n",
    "# df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "# df = df.sort_values(['name', 'created_at'])\n",
    "#\n",
    "# # 选择特定设备（例如NX48）\n",
    "# device_name = 'NX48'\n",
    "# df_device = df[df['name'] == device_name]\n",
    "#\n",
    "# # 计算基础能耗指标\n",
    "# energy_metrics = df_device[['E_dw13', 'E_dw14']].describe()\n",
    "#\n",
    "# print(\"\\n基础能耗指标分析:\")\n",
    "# print(energy_metrics)\n",
    "#\n",
    "# # 可视化功率变化\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(df_device['created_at'], df_device['E_dw13'], label='总功率', color='blue')\n",
    "# plt.title(f'{device_name} 总功率随时间变化趋势')\n",
    "# plt.xlabel('时间')\n",
    "# plt.ylabel('功率(kW)')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "#\n",
    "# # 计算电流和电压的稳定性\n",
    "# current_cols = ['E_dw1', 'E_dw2', 'E_dw3']\n",
    "# voltage_cols = ['E_dw4', 'E_dw5', 'E_dw6', 'E_dw7', 'E_dw8', 'E_dw9', 'E_dw10', 'E_dw11', 'E_dw12']\n",
    "#\n",
    "# current_std = df_device[current_cols].std().mean()\n",
    "# voltage_std = df_device[voltage_cols].std().mean()\n",
    "#\n",
    "# print(colored(\"\\n电流稳定性分析:\", 'green'))\n",
    "# print(f\"所有电流通道的平均标准差: {current_std}\")\n",
    "#\n",
    "# print(colored(\"\\n电压稳定性分析:\", 'green'))\n",
    "# print(f\"所有电压通道的平均标准差: {voltage_std}\")\n",
    "#\n",
    "# # 可视化电流和电压的稳定性\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.plot(df_device['created_at'], df_device['E_dw1'], label='电流A', color='orange')\n",
    "# plt.plot(df_device['created_at'], df_device['E_dw2'], label='电流B', color='green')\n",
    "# plt.plot(df_device['created_at'], df_device['E_dw3'], label='电流C', color='red')\n",
    "# plt.title(f'{device_name} 电流随时间变化趋势')\n",
    "# plt.xlabel('时间')\n",
    "# plt.ylabel('电流(A)')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "#\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(df_device['created_at'], df_device['E_dw4'], label='电压A', color='purple')\n",
    "# plt.plot(df_device['created_at'], df_device['E_dw5'], label='电压B', color='pink')\n",
    "# plt.plot(df_device['created_at'], df_device['E_dw6'], label='电压C', color='brown')\n",
    "# plt.plot(df_device['created_at'], df_device['E_dw7'], label='电压AB', color='cyan')\n",
    "# plt.plot(df_device['created_at'], df_device['E_dw8'], label='电压BC', color='magenta')\n",
    "# plt.plot(df_device['created_at'], df_device['E_dw9'], label='电压AC', color='lime')\n",
    "# plt.title(f'{device_name} 电压随时间变化趋势')\n",
    "# plt.xlabel('时间')\n",
    "# plt.ylabel('电压(V)')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "#\n",
    "# # 分析电流和电压对断纱的影响\n",
    "# # 假设断纱事件在D_dw1-D_dw100中表示为特定值（例如1）\n",
    "# breakage_cols = [f'D_dw{i}' for i in range(1, 101)]\n",
    "# df_device['breakage'] = df_device[breakage_cols].sum(axis=1) > 0\n",
    "#\n",
    "# # 计算断纱事件发生的概率\n",
    "# breakage_prob = df_device['breakage'].mean()\n",
    "# print(colored(\"\\n断纱事件分析:\", 'green'))\n",
    "# print(f\"断纱事件发生的概率: {breakage_prob:.2%}\")\n",
    "#\n",
    "# # 可视化电流和电压在断纱事件发生时的变化\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(df_device['created_at'], df_device['E_dw1'], label='电流A', color='orange')\n",
    "# plt.plot(df_device['created_at'], df_device['E_dw2'], label='电流B', color='green')\n",
    "# plt.plot(df_device['created_at'], df_device['E_dw3'], label='电流C', color='red')\n",
    "# plt.scatter(df_device[df_device['breakage']]['created_at'], df_device[df_device['breakage']]['E_dw1'], color='orange', label='断纱 (电流A)', marker='x')\n",
    "# plt.scatter(df_device[df_device['breakage']]['created_at'], df_device[df_device['breakage']]['E_dw2'], color='green', label='断纱 (电流B)', marker='x')\n",
    "# plt.scatter(df_device[df_device['breakage']]['created_at'], df_device[df_device['breakage']]['E_dw3'], color='red', label='断纱 (电流C)', marker='x')\n",
    "# plt.title(f'{device_name} 电流变化（断纱事件）')\n",
    "# plt.xlabel('时间')\n",
    "# plt.ylabel('电流(A)')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "#\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(df_device['created_at'], df_device['E_dw4'], label='电压A', color='purple')\n",
    "# plt.plot(df_device['created_at'], df_device['E_dw5'], label='电压B', color='pink')\n",
    "# plt.scatter(df_device[df_device['breakage']]['created_at'], df_device[df_device['breakage']]['E_dw4'], color='purple', label='断纱 (电压A)', marker='x')\n",
    "# plt.scatter(df_device[df_device['breakage']]['created_at'], df_device[df_device['breakage']]['E_dw5'], color='pink', label='断纱 (电压B)', marker='x')\n",
    "# plt.title(f'{device_name} 电压变化（断纱事件）')\n",
    "# plt.xlabel('时间')\n",
    "# plt.ylabel('电压(V)')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "#\n",
    "# # 分析电流和电压对锭速的影响\n",
    "# speed_cols = [f'P_dw{i}' for i in range(1, 101)]\n",
    "# df_device['speed'] = df_device[speed_cols].mean(axis=1)\n",
    "#\n",
    "# # 可视化电流和电压与锭速的关系\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.scatter(df_device['E_dw1'], df_device['speed'], label='电流A vs 锭速', color='orange', alpha=0.5)\n",
    "# plt.scatter(df_device['E_dw2'], df_device['speed'], label='电流B vs 锭速', color='green', alpha=0.5)\n",
    "# plt.scatter(df_device['E_dw3'], df_device['speed'], label='电流C vs 锭速', color='red', alpha=0.5)\n",
    "# plt.title(f'{device_name} 电流与锭速的关系')\n",
    "# plt.xlabel('电流(A)')\n",
    "# plt.ylabel('锭速')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "#\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.scatter(df_device['E_dw4'], df_device['speed'], label='电压A vs 锭速', color='purple', alpha=0.5)\n",
    "# plt.scatter(df_device['E_dw5'], df_device['speed'], label='电压B vs 锭速', color='pink', alpha=0.5)\n",
    "# plt.scatter(df_device['E_dw6'], df_device['speed'], label='电压C vs 锭速', color='brown', alpha=0.5)\n",
    "# plt.title(f'{device_name} 电压与锭速的关系')\n",
    "# plt.xlabel('电压(V)')\n",
    "# plt.ylabel('锭速')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "id": "dd639da1311efd7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "# 读取数据\n",
    "file_path = 'D:/code/junma/600000/0424/final2.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 确保时间列是datetime类型并按时间排序\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "df = df.sort_values(['name', 'created_at'])\n",
    "\n",
    "# 选择特定设备（例如NX48）\n",
    "device_name = 'NX48'\n",
    "df_device = df[df['name'] == device_name]\n",
    "\n",
    "# 计算基础能耗指标\n",
    "energy_metrics = df_device[['E_dw13', 'E_dw14']].describe()\n",
    "\n",
    "print(\"\\n基础能耗指标分析:\")\n",
    "print(energy_metrics)\n",
    "\n",
    "# 可视化功率变化\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_device['created_at'], df_device['E_dw13'], label='总功率', color='blue')\n",
    "plt.title(f'{device_name} 总功率随时间变化趋势')\n",
    "plt.xlabel('时间')\n",
    "plt.ylabel('功率(kW)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 计算电流和电压的稳定性\n",
    "current_cols = ['E_dw7', 'E_dw8', 'E_dw9']  # A/B/C项电流\n",
    "voltage_cols = ['E_dw1', 'E_dw2', 'E_dw3', 'E_dw4', 'E_dw5', 'E_dw6']  # 各相电压\n",
    "\n",
    "current_std = df_device[current_cols].std().mean()\n",
    "voltage_std = df_device[voltage_cols].std().mean()\n",
    "\n",
    "print(colored(\"\\n电流稳定性分析:\", 'green'))\n",
    "print(f\"所有电流通道的平均标准差: {current_std:.2f}\")\n",
    "\n",
    "print(colored(\"\\n电压稳定性分析:\", 'green'))\n",
    "print(f\"所有电压通道的平均标准差: {voltage_std:.2f}\")\n",
    "\n",
    "# 可视化电流和电压的稳定性\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(df_device['created_at'], df_device['E_dw7'], label='A项电流', color='orange')\n",
    "plt.plot(df_device['created_at'], df_device['E_dw8'], label='B项电流', color='green')\n",
    "plt.plot(df_device['created_at'], df_device['E_dw9'], label='C项电流', color='red')\n",
    "plt.title(f'{device_name} 电流随时间变化趋势')\n",
    "plt.xlabel('时间')\n",
    "plt.ylabel('电流(A)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(df_device['created_at'], df_device['E_dw1'], label='A项电压', color='purple')\n",
    "plt.plot(df_device['created_at'], df_device['E_dw2'], label='B项电压', color='pink')\n",
    "plt.plot(df_device['created_at'], df_device['E_dw3'], label='C项电压', color='brown')\n",
    "plt.plot(df_device['created_at'], df_device['E_dw4'], label='AB项电压', color='cyan')\n",
    "plt.plot(df_device['created_at'], df_device['E_dw5'], label='BC项电压', color='magenta')\n",
    "plt.plot(df_device['created_at'], df_device['E_dw6'], label='AC项电压', color='lime')\n",
    "plt.title(f'{device_name} 电压随时间变化趋势')\n",
    "plt.xlabel('时间')\n",
    "plt.ylabel('电压(V)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 分析电流和电压对断纱的影响\n",
    "# 假设断纱事件在D_dw1-D_dw100中表示为特定值（例如1）\n",
    "breakage_cols = [f'D_dw{i}' for i in range(1, 101)]\n",
    "df_device['breakage'] = df_device[breakage_cols].sum(axis=1) > 0\n",
    "\n",
    "# 计算断纱事件发生的概率\n",
    "breakage_prob = df_device['breakage'].mean()\n",
    "print(colored(\"\\n断纱事件分析:\", 'green'))\n",
    "print(f\"断纱事件发生的概率: {breakage_prob:.2%}\")\n",
    "\n",
    "# 可视化电流和电压在断纱事件发生时的变化\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_device['created_at'], df_device['E_dw7'], label='A项电流', color='orange')\n",
    "plt.plot(df_device['created_at'], df_device['E_dw8'], label='B项电流', color='green')\n",
    "plt.plot(df_device['created_at'], df_device['E_dw9'], label='C项电流', color='red')\n",
    "plt.scatter(df_device[df_device['breakage']]['created_at'], df_device[df_device['breakage']]['E_dw7'],\n",
    "            color='orange', label='断纱 (A项电流)', marker='x')\n",
    "plt.scatter(df_device[df_device['breakage']]['created_at'], df_device[df_device['breakage']]['E_dw8'],\n",
    "            color='green', label='断纱 (B项电流)', marker='x')\n",
    "plt.scatter(df_device[df_device['breakage']]['created_at'], df_device[df_device['breakage']]['E_dw9'],\n",
    "            color='red', label='断纱 (C项电流)', marker='x')\n",
    "plt.title(f'{device_name} 电流变化（断纱事件）')\n",
    "plt.xlabel('时间')\n",
    "plt.ylabel('电流(A)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_device['created_at'], df_device['E_dw1'], label='A项电压', color='purple')\n",
    "plt.plot(df_device['created_at'], df_device['E_dw2'], label='B项电压', color='pink')\n",
    "plt.scatter(df_device[df_device['breakage']]['created_at'], df_device[df_device['breakage']]['E_dw1'],\n",
    "            color='purple', label='断纱 (A项电压)', marker='x')\n",
    "plt.scatter(df_device[df_device['breakage']]['created_at'], df_device[df_device['breakage']]['E_dw2'],\n",
    "            color='pink', label='断纱 (B项电压)', marker='x')\n",
    "plt.title(f'{device_name} 电压变化（断纱事件）')\n",
    "plt.xlabel('时间')\n",
    "plt.ylabel('电压(V)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 分析电流和电压对锭速的影响\n",
    "speed_cols = [f'P_dw{i}' for i in range(1, 101)]\n",
    "df_device['speed'] = df_device[speed_cols].mean(axis=1)\n",
    "\n",
    "# 可视化电流和电压与锭速的关系\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(df_device['E_dw7'], df_device['speed'], label='A项电流 vs 锭速', color='orange', alpha=0.5)\n",
    "plt.scatter(df_device['E_dw8'], df_device['speed'], label='B项电流 vs 锭速', color='green', alpha=0.5)\n",
    "plt.scatter(df_device['E_dw9'], df_device['speed'], label='C项电流 vs 锭速', color='red', alpha=0.5)\n",
    "plt.title(f'{device_name} 电流与锭速的关系')\n",
    "plt.xlabel('电流(A)')\n",
    "plt.ylabel('锭速')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(df_device['E_dw1'], df_device['speed'], label='A项电压 vs 锭速', color='purple', alpha=0.5)\n",
    "plt.scatter(df_device['E_dw2'], df_device['speed'], label='B项电压 vs 锭速', color='pink', alpha=0.5)\n",
    "plt.scatter(df_device['E_dw3'], df_device['speed'], label='C项电压 vs 锭速', color='brown', alpha=0.5)\n",
    "plt.title(f'{device_name} 电压与锭速的关系')\n",
    "plt.xlabel('电压(V)')\n",
    "plt.ylabel('锭速')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c1ddf0d9489a02b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from termcolor import colored  # 确保安装了 termcolor 库\n",
    "\n",
    "# 读取数据\n",
    "file_path = 'D:/code/junma/600000/0424/final2.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 转换时间列并排序\n",
    "df['time'] = pd.to_datetime(df['created_at'])\n",
    "df = df.sort_values(['name', 'time'])  # 按设备名称和时间排序\n",
    "\n",
    "# 控制图生成函数（修正版）\n",
    "def create_control_chart(df, column_name, device_name):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # 计算统计量\n",
    "    mean = df[column_name].mean()\n",
    "    std = df[column_name].std()\n",
    "    UCL = mean + 3 * std\n",
    "    LCL = mean - 3 * std\n",
    "\n",
    "    # 计算Z分数和异常值\n",
    "    z_scores = zscore(df[column_name])\n",
    "    anomalies = df[np.abs(z_scores) > 3]\n",
    "\n",
    "    # 关键修正：使用 matplotlib 兼容的时间格式\n",
    "    # 方法1：直接使用 datetime 对象（推荐）\n",
    "    plt.plot(df['time'], df[column_name], label=f'{column_name} 值', color='blue')\n",
    "\n",
    "    # 方法2：转换为字符串格式（如果需要特定格式）\n",
    "    # plt.plot(df['time'].dt.strftime('%Y-%m-%d %H:%M'), df[column_name], ...)\n",
    "\n",
    "    # 添加控制线\n",
    "    plt.axhline(mean, color='black', linestyle='--', label='平均值')\n",
    "    plt.axhline(UCL, color='red', linestyle='--', label='UCL (均值+3σ)')\n",
    "    plt.axhline(LCL, color='green', linestyle='--', label='LCL (均值-3σ)')\n",
    "\n",
    "    # 标记异常值\n",
    "    if not anomalies.empty:\n",
    "        plt.scatter(anomalies['time'], anomalies[column_name],\n",
    "                   color='red', s=100, zorder=5, label='异常值 (|Z|>3)')\n",
    "\n",
    "    # 图表装饰\n",
    "    plt.title(f'{device_name} - {column_name} 控制图\\n(均值={mean:.2f}, σ={std:.2f})')\n",
    "    plt.xlabel('时间')\n",
    "    plt.ylabel(column_name)\n",
    "\n",
    "    # 优化X轴标签显示\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.gcf().autofmt_xdate()  # 自动调整日期格式\n",
    "\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 分析所有设备\n",
    "columns_of_interest = ['R_dw22', 'R_dw23', 'R_dw24', 'R_dw25', 'R_dw26',\n",
    "                      'R_dw29', 'R_dw30', 'R_dw31', 'R_dw32', 'R_dw33']\n",
    "\n",
    "print(\"\\n=== 全局分析 ===\")\n",
    "for column in columns_of_interest:\n",
    "    print(colored(f\"正在处理 {column} 的控制图...\", 'green'))\n",
    "    create_control_chart(df, column, '所有设备')\n",
    "\n",
    "# 分析特定设备（示例：NX48）\n",
    "device_name = 'NX48'\n",
    "df_device = df[df['name'] == device_name]\n",
    "\n",
    "if not df_device.empty:\n",
    "    print(f\"\\n=== 设备 {device_name} 分析 ===\")\n",
    "    for column in columns_of_interest:\n",
    "        print(colored(f\"正在处理 {column} 的控制图...\", 'cyan'))\n",
    "        create_control_chart(df_device, column, device_name)\n",
    "else:\n",
    "    print(colored(f\"\\n警告：未找到设备 {device_name} 的数据\", 'yellow'))"
   ],
   "id": "60b814eb034de77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from termcolor import colored  # 确保安装了 termcolor 库\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "file_path = 'D:/code/junma/600000/0424/final2.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 转换时间列并排序\n",
    "df['time'] = pd.to_datetime(df['created_at'])\n",
    "df = df.sort_values(['name', 'time'])  # 按设备名称和时间排序\n",
    "\n",
    "# 控制图生成函数\n",
    "def create_control_chart(df, column_name, device_name):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # 计算统计量\n",
    "    mean = df[column_name].mean()\n",
    "    std = df[column_name].std()\n",
    "    UCL = mean + 3 * std\n",
    "    LCL = mean - 3 * std\n",
    "\n",
    "    # 计算Z分数和异常值\n",
    "    z_scores = zscore(df[column_name])\n",
    "    anomalies = df[np.abs(z_scores) > 3]\n",
    "\n",
    "    # 绘制控制图\n",
    "    plt.plot(df['time'], df[column_name], label=f'{column_name} 值', color='blue')\n",
    "\n",
    "    # 添加控制线\n",
    "    plt.axhline(mean, color='black', linestyle='--', label='平均值')\n",
    "    plt.axhline(UCL, color='red', linestyle='--', label='UCL (均值+3σ)')\n",
    "    plt.axhline(LCL, color='green', linestyle='--', label='LCL (均值-3σ)')\n",
    "\n",
    "    # 标记异常值\n",
    "    if not anomalies.empty:\n",
    "        plt.scatter(anomalies['time'], anomalies[column_name],\n",
    "                   color='red', s=100, zorder=5, label='异常值 (|Z|>3)')\n",
    "\n",
    "    # 图表装饰\n",
    "    plt.title(f'{device_name} - {column_name} 控制图\\n(均值={mean:.2f}, σ={std:.2f})')\n",
    "    plt.xlabel('时间')\n",
    "    plt.ylabel(column_name)\n",
    "\n",
    "    # 优化X轴标签显示\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.gcf().autofmt_xdate()  # 自动调整日期格式\n",
    "\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 找出断纱次数最多的设备\n",
    "# 假设断纱次数存储在某个列中，这里用'break_count'作为示例列名\n",
    "# 如果没有这个列，可以用其他相关列代替，如'R_dw22'等\n",
    "break_column = 'R_dw22'  # 请替换为实际的断纱次数列名\n",
    "\n",
    "# 计算每台设备的总断纱次数\n",
    "device_break_totals = df.groupby('name')[break_column].sum().sort_values(ascending=False)\n",
    "\n",
    "if not device_break_totals.empty:\n",
    "    # 获取断纱次数最多的设备名称\n",
    "    top_device = device_break_totals.index[0]\n",
    "    total_breaks = device_break_totals.iloc[0]\n",
    "\n",
    "    print(colored(f\"\\n断纱次数最多的设备是: {top_device} (总断纱次数: {total_breaks})\", 'red', attrs=['bold']))\n",
    "\n",
    "    # 筛选该设备的数据\n",
    "    df_top_device = df[df['name'] == top_device]\n",
    "\n",
    "    # 分析该设备的各项指标\n",
    "    columns_of_interest = ['R_dw22', 'R_dw23', 'R_dw24', 'R_dw25', 'R_dw26',\n",
    "                          'R_dw29', 'R_dw30', 'R_dw31', 'R_dw32', 'R_dw33']\n",
    "\n",
    "    print(f\"\\n=== 设备 {top_device} 详细分析 ===\")\n",
    "    for column in columns_of_interest:\n",
    "        print(colored(f\"正在处理 {column} 的控制图...\", 'cyan'))\n",
    "        create_control_chart(df_top_device, column, top_device)\n",
    "\n",
    "        # 计算并显示基本统计信息\n",
    "        col_mean = df_top_device[column].mean()\n",
    "        col_std = df_top_device[column].std()\n",
    "        col_min = df_top_device[column].min()\n",
    "        col_max = df_top_device[column].max()\n",
    "\n",
    "        print(f\"{column} 统计: 均值={col_mean:.2f}, 标准差={col_std:.2f}, 范围=[{col_min:.2f}, {col_max:.2f}]\")\n",
    "\n",
    "        # 计算异常值比例\n",
    "        z_scores = zscore(df_top_device[column])\n",
    "        anomaly_ratio = (np.abs(z_scores) > 3).mean() * 100\n",
    "        print(f\"异常值比例: {anomaly_ratio:.2f}%\\n\")\n",
    "else:\n",
    "    print(colored(\"\\n警告：未找到任何设备数据或断纱次数列为空\", 'yellow'))"
   ],
   "id": "fe4d2ef09f0278b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7452274b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:13.269190Z",
     "iopub.status.busy": "2023-09-11T21:34:13.268804Z",
     "iopub.status.idle": "2023-09-11T21:34:13.275093Z",
     "shell.execute_reply": "2023-09-11T21:34:13.273732Z"
    },
    "papermill": {
     "duration": 0.046868,
     "end_time": "2023-09-11T21:34:13.277510",
     "exception": false,
     "start_time": "2023-09-11T21:34:13.230642",
     "status": "completed"
    },
    "tags": []
   },
   "source": "#df.drop(drop_columns, axis =1, inplace=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # 假设df中有'timestamp'列记录时间\n",
    "# if 'created_at' in df.columns and broken_data:\n",
    "#     # 将时间列转换为datetime类型（如果尚未转换）\n",
    "#     df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "#\n",
    "#     # 按时间分析断纱情况\n",
    "#     print(\"\\n=== 时间趋势分析 ===\")\n",
    "#\n",
    "#     # 按小时统计断纱次数\n",
    "#     hourly_broken = df.set_index('created_at')[spindle_status_cols].apply(lambda x: x == 3).sum(axis=1)\n",
    "#     hourly_broken = hourly_broken.resample('H').sum()\n",
    "#\n",
    "#     plt.figure(figsize=(15, 5))\n",
    "#     hourly_broken.plot(title='每小时断纱锭位总数趋势', color='red')\n",
    "#     plt.xlabel('时间')\n",
    "#     plt.ylabel('断纱锭位数量')\n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "#\n",
    "#     # 按设备+时间分析\n",
    "#     top5_devices = device_stats_df.sort_values('断纱锭位总数', ascending=False)['设备名称'].head(5)\n",
    "#     plt.figure(figsize=(15, 8))\n",
    "#     for i, device in enumerate(top5_devices, 1):\n",
    "#         device_data = df[df['name'] == device]\n",
    "#         device_broken = device_data.set_index('created_at')[spindle_status_cols].apply(lambda x: x == 3).sum(axis=1)\n",
    "#         device_broken = device_broken.resample('H').sum()\n",
    "#\n",
    "#         plt.subplot(len(top5_devices), 1, i)\n",
    "#         device_broken.plot(title=f'设备 {device} 断纱趋势', color='blue')\n",
    "#         plt.xlabel('')\n",
    "#         plt.ylabel('断纱数')\n",
    "#         plt.grid(True)\n",
    "#\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ],
   "id": "d2a729a74b3d8e48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 状态转换分析\n",
    "\n",
    "df = pd.read_csv(\"D:/code/junma/600000/0424/second_final.csv\")\n",
    "if len(df) > 1 and 'created_at' in df.columns:\n",
    "    print(\"\\n=== 状态转换分析 ===\")\n",
    "\n",
    "    # 按时间排序\n",
    "    df = df.sort_values(['subsystem', 'created_at'])\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "    # 选择几个示例锭位进行分析\n",
    "    example_spindles = ['D_dw1', 'D_dw25', 'D_dw50']\n",
    "    spindle_status_cols=[f'D_dw{i}' for i in range(1, 101)]\n",
    "\n",
    "    for spindle in example_spindles:\n",
    "        if spindle in spindle_status_cols:\n",
    "            speed_col = spindle.replace('D_', 'V_')\n",
    "\n",
    "            # 获取状态变化点\n",
    "            status_changes = df.groupby('subsystem')[spindle].diff().ne(0)\n",
    "            broken_events = df[(df[spindle] == 3) & status_changes]\n",
    "\n",
    "            if not broken_events.empty:\n",
    "                print(f\"\\n锭位 {spindle} 断纱事件分析:\")\n",
    "\n",
    "                # 分析断纱前状态\n",
    "                prev_status = []\n",
    "                for _, row in broken_events.iterrows():\n",
    "                    prev = df[(df['subsystem'] == row['subsystem']) &\n",
    "                            (df['created_at'] < row['created_at'])].tail(1)\n",
    "                    if not prev.empty:\n",
    "                        prev_status.append(prev[spindle].values[0])\n",
    "\n",
    "                if prev_status:\n",
    "                    prev_status_counts = pd.Series(prev_status).value_counts()\n",
    "                    print(\"断纱前状态分布:\")\n",
    "                    print(prev_status_counts.rename(index=status_mapping))\n",
    "\n",
    "                # 分析断纱前后速度变化\n",
    "                for _, row in broken_events.head(3).iterrows():  # 分析前3个事件\n",
    "                    time_window = pd.Timedelta(minutes=30)\n",
    "                    window_data = df[(df['subsystem'] == row['subsystem']) &\n",
    "                                   (df['created_at'] >= row['created_at'] - time_window) &\n",
    "                                   (df['created_at'] <= row['created_at'] + time_window)]\n",
    "\n",
    "                    if not window_data.empty:\n",
    "                        plt.figure(figsize=(12, 4))\n",
    "                        plt.plot(window_data['created_at'], window_data[speed_col],\n",
    "                                label='锭位速度', color='blue')\n",
    "                        plt.axvline(x=row['created_at'], color='red', linestyle='--',\n",
    "                                   label='断纱发生时间')\n",
    "                        plt.title(f'锭位 {spindle} 断纱前后速度变化 (设备 {row[\"name\"]})')\n",
    "                        plt.xlabel('时间')\n",
    "                        plt.ylabel('速度')\n",
    "                        plt.legend()\n",
    "                        plt.grid(True)\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()"
   ],
   "id": "65f2ebd7f3881335",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # 异常检测\n",
    "# if broken_data:\n",
    "#     print(\"\\n=== 异常检测 ===\")\n",
    "#\n",
    "#     # 检测速度异常高的断纱锭位\n",
    "#     speed_threshold_high = broken_df['锭位速度'].quantile(0.99)  # 取99%分位数作为高阈值\n",
    "#     high_speed_broken = broken_df[broken_df['锭位速度'] > speed_threshold_high]\n",
    "#\n",
    "#     print(f\"\\n检测到 {len(high_speed_broken)} 个速度异常高(>{speed_threshold_high:.1f})的断纱锭位:\")\n",
    "#     print(high_speed_broken.describe())\n",
    "#\n",
    "#     # 检测速度异常低的非零断纱锭位\n",
    "#     non_zero_broken = broken_df[broken_df['锭位速度'] > 0]\n",
    "#     if len(non_zero_broken) > 0:\n",
    "#         speed_threshold_low = non_zero_broken['锭位速度'].quantile(0.01)  # 取1%分位数作为低阈值\n",
    "#         low_speed_broken = non_zero_broken[non_zero_broken['锭位速度'] < speed_threshold_low]\n",
    "#\n",
    "#         print(f\"\\n检测到 {len(low_speed_broken)} 个速度异常低(<{speed_threshold_low:.1f})的非零断纱锭位:\")\n",
    "#         print(low_speed_broken.describe())\n",
    "#\n",
    "#     # 可视化异常点\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.scatter(range(len(broken_df)), broken_df['锭位速度'], alpha=0.6,\n",
    "#                label='正常速度', color='blue')\n",
    "#     if not high_speed_broken.empty:\n",
    "#         plt.scatter(high_speed_broken.index, high_speed_broken['锭位速度'],\n",
    "#                    color='red', label=f'高速异常(>{speed_threshold_high:.1f})')\n",
    "#     if not low_speed_broken.empty:\n",
    "#         plt.scatter(low_speed_broken.index, low_speed_broken['锭位速度'],\n",
    "#                    color='orange', label=f'低速异常(<{speed_threshold_low:.1f})')\n",
    "#\n",
    "#     plt.axhline(y=speed_threshold_high, color='red', linestyle='--', alpha=0.5)\n",
    "#     if len(non_zero_broken) > 0:\n",
    "#         plt.axhline(y=speed_threshold_low, color='orange', linestyle='--', alpha=0.5)\n",
    "#\n",
    "#     plt.title('断纱锭位速度异常检测')\n",
    "#     plt.xlabel('样本索引')\n",
    "#     plt.ylabel('速度')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ],
   "id": "24158edcf4ad5b01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 机器学习预测\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if len(df) > 1000:  # 确保有足够的数据\n",
    "    print(\"\\n=== 机器学习预测 ===\")\n",
    "\n",
    "    # 准备特征和目标变量\n",
    "    features = []\n",
    "    targets = []\n",
    "\n",
    "    # 为每个锭位创建样本\n",
    "    for spindle in spindle_status_cols:\n",
    "        speed_col = spindle.replace('D_', 'V_')\n",
    "\n",
    "        # 获取当前锭位的所有记录\n",
    "        spindle_data = df[[spindle, speed_col, 'name']].copy()\n",
    "        spindle_data['锭位ID'] = spindle\n",
    "\n",
    "        # 添加滞后特征\n",
    "        for lag in [1, 2, 3]:  # 添加前3个时间点的特征\n",
    "            spindle_data[f'速度滞后{lag}'] = spindle_data.groupby(['name', '锭位ID'])[speed_col].shift(lag)\n",
    "            spindle_data[f'状态滞后{lag}'] = spindle_data.groupby(['name', '锭位ID'])[spindle].shift(lag)\n",
    "\n",
    "        # 移除缺失值\n",
    "        spindle_data = spindle_data.dropna()\n",
    "\n",
    "        # 定义目标：下个时间点是否会断纱\n",
    "        spindle_data['目标'] = (spindle_data.groupby(['name', '锭位ID'])[spindle].shift(-1) == 3).astype(int)\n",
    "\n",
    "        if not spindle_data.empty:\n",
    "            features.append(spindle_data.drop(columns=[spindle, '目标']))\n",
    "            targets.append(spindle_data['目标'])\n",
    "\n",
    "    if features and targets:\n",
    "        X = pd.concat(features)\n",
    "        y = pd.concat(targets)\n",
    "\n",
    "        # 特征工程\n",
    "        X = pd.get_dummies(X, columns=['锭位ID', 'name', '状态滞后1', '状态滞后2', '状态滞后3'])\n",
    "\n",
    "        # 划分训练测试集\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        num_cols = [col for col in X_train.columns if col.startswith('速度')]\n",
    "        X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "        X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "        # 训练模型\n",
    "        print(\"\\n训练随机森林模型...\")\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # 评估模型\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(\"\\n分类报告:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        # 特征重要性\n",
    "        importance = pd.DataFrame({\n",
    "            '特征': X.columns,\n",
    "            '重要性': model.feature_importances_\n",
    "        }).sort_values('重要性', ascending=False).head(20)\n",
    "\n",
    "        print(\"\\n最重要的20个特征:\")\n",
    "        print(importance)\n",
    "\n",
    "        # 可视化混淆矩阵\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['正常', '将断纱'])\n",
    "        disp.plot(cmap='Blues')\n",
    "        plt.title('断纱预测混淆矩阵')\n",
    "        plt.show()\n",
    "\n",
    "        # 保存模型供后续使用\n",
    "        import joblib\n",
    "        joblib.dump(model, 'broken_spindle_predictor.pkl')\n",
    "        print(\"\\n模型已保存为 'broken_spindle_predictor.pkl'\")"
   ],
   "id": "996038747f60a2aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "126640fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:13.354878Z",
     "iopub.status.busy": "2023-09-11T21:34:13.354065Z",
     "iopub.status.idle": "2023-09-11T21:34:13.360133Z",
     "shell.execute_reply": "2023-09-11T21:34:13.359078Z"
    },
    "papermill": {
     "duration": 0.047406,
     "end_time": "2023-09-11T21:34:13.362540",
     "exception": false,
     "start_time": "2023-09-11T21:34:13.315134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# We replaced the categorical expression in the variable Electricity Generated (kWh/yr) with \"Project Name\", NaN.\n",
    "\n",
    "#df['Electricity Generated (kWh/yr)'].replace('Project Name', np.nan, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed45ab03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:13.438552Z",
     "iopub.status.busy": "2023-09-11T21:34:13.438124Z",
     "iopub.status.idle": "2023-09-11T21:34:13.444049Z",
     "shell.execute_reply": "2023-09-11T21:34:13.442840Z"
    },
    "papermill": {
     "duration": 0.047252,
     "end_time": "2023-09-11T21:34:13.446317",
     "exception": false,
     "start_time": "2023-09-11T21:34:13.399065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# We changed the type of the variable Electricity Generated (kWh/yr) from categorical to numeric.\n",
    "\n",
    "#df['Electricity Generated (kWh/yr)'] = pd.to_numeric(df['Electricity Generated (kWh/yr)'], errors='coerce')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "798f7d78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:13.522034Z",
     "iopub.status.busy": "2023-09-11T21:34:13.521635Z",
     "iopub.status.idle": "2023-09-11T21:34:13.537305Z",
     "shell.execute_reply": "2023-09-11T21:34:13.536149Z"
    },
    "papermill": {
     "duration": 0.056475,
     "end_time": "2023-09-11T21:34:13.539908",
     "exception": false,
     "start_time": "2023-09-11T21:34:13.483433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NaN observations on farms without livestock were replaced by 0.\n",
    "\n",
    "df[\"A面1罗拉运行值_转\"] = df[\"A面1罗拉运行值_转\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"A面2罗拉运行值_转\"] = df[\"A面2罗拉运行值_转\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"A面3罗拉运行值_转\"] = df[\"A面3罗拉运行值_转\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"A面黑辊1运行值_转\"] = df[\"A面黑辊1运行值_转\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"A面黑辊2运行值_转.1\"] = df[\"A面黑辊2运行值_转.1\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"A面油罗拉运行值_转\"] = df[\"A面油罗拉运行值_转\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"A面龙带运行值_转\"] = df[\"A面龙带运行值_转\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"A面4罗拉运行值_转\"] = df[\"A面4罗拉运行值_转\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"A面5罗拉设定值_转\"] = df[\"A面5罗拉设定值_转\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"B面1罗拉运行值_转\"] = df[\"B面1罗拉运行值_转\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"B面2罗拉运行值_转\"] = df[\"B面2罗拉运行值_转\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"B面3罗拉运行值_转\"] = df[\"B面3罗拉运行值_转\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"B面黑辊1运行值_转\"] = df[\"B面黑辊1运行值_转\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"B面黑辊2运行值_转\"] = df[\"B面黑辊2运行值_转\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"B面黑辊2运行值_转.1\"] = df[\"B面黑辊2运行值_转.1\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"B面油罗拉运行值_转\"] = df[\"B面油罗拉运行值_转\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"第1锭在锭重量\"] = df[\"第1锭在锭重量\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"第31锭在锭重量\"] = df[\"第31锭在锭重量\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"第61锭在锭重量\"] = df[\"第61锭在锭重量\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"第91锭在锭重量\"] = df[\"第91锭在锭重量\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"第121锭在锭重量\"] = df[\"第121锭在锭重量\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"第151锭在锭重量\"] = df[\"第151锭在锭重量\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"第181锭在锭重量\"] = df[\"第181锭在锭重量\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"第211锭在锭重量\"] = df[\"第211锭在锭重量\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"第241锭在锭重量\"] = df[\"第241锭在锭重量\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"第271锭在锭重量\"] = df[\"第271锭在锭重量\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"第301锭在锭重量\"] = df[\"第301锭在锭重量\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"第331锭在锭重量\"] = df[\"第331锭在锭重量\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "#f[\"第361锭在锭重量\"] = df[\"第361锭在锭重量\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"A面第1节上热箱控制区温度\"] = df[\"A面第1节上热箱控制区温度\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"A面第1节上热箱显示区温度\"] = df[\"A面第1节上热箱显示区温度\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"B面第1节上热箱控制区温度\"] = df[\"B面第1节上热箱控制区温度\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"B面第1节上热箱显示区温度\"] = df[\"B面第1节上热箱显示区温度\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"A面第1节下热箱控制区温度\"] = df[\"A面第1节下热箱控制区温度\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"A面第1节下热箱显示区温度\"] = df[\"A面第1节下热箱显示区温度\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"B面第1节下热箱控制区温度\"] = df[\"B面第1节下热箱控制区温度\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "df[\"B面第1节下热箱显示区温度\"] = df[\"B面第1节下热箱显示区温度\"].apply(lambda x: x if pd.notna(x) and np.isreal(x) else '0')\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numeric_cols = df.select_dtypes(include='number').columns\n",
    "all_zero_rows = df[numeric_cols].eq(0).all(axis=1)\n",
    "df_cleaned = df[~all_zero_rows]\n",
    "# 选择需要检查的列（排除'name'和'time'）\n",
    "other_cols = df.drop(['name', 'created_at'], axis=1)\n",
    "\n",
    "# 选择除了'name'和'time'之外的所有列\n",
    "cols_to_check = [col for col in df.columns if col not in ['name', 'created_atime']]\n",
    "\n",
    "# 删除这些列中所有值都为0的行\n",
    "df = df[~(df[cols_to_check] == 0).all(axis=1)]\n",
    "#df = df.drop(columns=['A面4罗拉运行值_转'])\n",
    "cols_to_check = [col for col in df.columns if col not in ['name', 'tcreated_at']]\n",
    "df = df[~(df[cols_to_check] == 0).any(axis=1)]\n",
    "\n",
    "df.to_csv(\"D:/code/junma/processed_merged_data.csv\", index=False)"
   ],
   "id": "8cbc33852d8c4ed5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0bd30330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:13.616500Z",
     "iopub.status.busy": "2023-09-11T21:34:13.615618Z",
     "iopub.status.idle": "2023-09-11T21:34:13.624221Z",
     "shell.execute_reply": "2023-09-11T21:34:13.623398Z"
    },
    "papermill": {
     "duration": 0.049098,
     "end_time": "2023-09-11T21:34:13.626803",
     "exception": false,
     "start_time": "2023-09-11T21:34:13.577705",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# If the observations in these variables are filled with NaN if there is no information, we replaced those observations with the No-Info variable.\n",
    "\n",
    "columns_to_fill = ['Awarded USDA Funding?', 'Digester Type', 'LCFS Pathway?', \"Biogas End Use(s)\"]\n",
    "\n",
    "df[columns_to_fill] = df[columns_to_fill].fillna('No-Info')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c3c64df0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:13.702502Z",
     "iopub.status.busy": "2023-09-11T21:34:13.702078Z",
     "iopub.status.idle": "2023-09-11T21:34:13.711595Z",
     "shell.execute_reply": "2023-09-11T21:34:13.710311Z"
    },
    "papermill": {
     "duration": 0.051015,
     "end_time": "2023-09-11T21:34:13.714061",
     "exception": false,
     "start_time": "2023-09-11T21:34:13.663046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# We have replaced the observations in the following variables with Yes if there is information in them and No-Info otherwise.\n",
    "\n",
    "df[\"Co-Digestion\"] = df[\"Co-Digestion\"].apply(lambda x: 'Yes' if pd.notna(x) else 'No-Info')\n",
    "\n",
    "df[\"Receiving Utility\"] = df[\"Receiving Utility\"].apply(lambda x: 'Yes' if pd.notna(x) else 'No-Info')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8157b530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:13.789287Z",
     "iopub.status.busy": "2023-09-11T21:34:13.788887Z",
     "iopub.status.idle": "2023-09-11T21:34:13.797034Z",
     "shell.execute_reply": "2023-09-11T21:34:13.796113Z"
    },
    "papermill": {
     "duration": 0.048562,
     "end_time": "2023-09-11T21:34:13.799264",
     "exception": false,
     "start_time": "2023-09-11T21:34:13.750702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# We discarded rows whose 'City' and 'County' columns are NaN at the same time.\n",
    "\n",
    "df.dropna(subset=['City', 'County'], how='all', inplace= True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b63b6da5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:13.874711Z",
     "iopub.status.busy": "2023-09-11T21:34:13.874008Z",
     "iopub.status.idle": "2023-09-11T21:34:13.894290Z",
     "shell.execute_reply": "2023-09-11T21:34:13.892985Z"
    },
    "papermill": {
     "duration": 0.061417,
     "end_time": "2023-09-11T21:34:13.897104",
     "exception": false,
     "start_time": "2023-09-11T21:34:13.835687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# We have completed the missing County information.\n",
    "\n",
    "def fill_county(row):\n",
    "    city = row['City']\n",
    "    state = row['State']\n",
    "\n",
    "    county_dict = {\n",
    "        'Thompson,Connecticut': 'Windham County',\n",
    "        'Coventry,Connecticut': 'Tolland County',\n",
    "        'Pocomoke City,Maryland': 'Worcester County',\n",
    "        'Columbia,Missouri': 'Boone County',\n",
    "        'Browning,Missouri': 'Linn County',\n",
    "        'Sumter,South Carolina': 'Sumter County',\n",
    "        'Dalhart,Texas': 'Dallam County',\n",
    "        'Milford,Utah': 'Beaver County'\n",
    "    }\n",
    "\n",
    "    return county_dict.get(f\"{city},{state}\", row['County'])\n",
    "\n",
    "df['County'] = df.apply(fill_county, axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6bdb897b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:13.972231Z",
     "iopub.status.busy": "2023-09-11T21:34:13.971805Z",
     "iopub.status.idle": "2023-09-11T21:34:13.977560Z",
     "shell.execute_reply": "2023-09-11T21:34:13.976647Z"
    },
    "papermill": {
     "duration": 0.046415,
     "end_time": "2023-09-11T21:34:13.979672",
     "exception": false,
     "start_time": "2023-09-11T21:34:13.933257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# We have completed the location information for the farm at index 192.\n",
    "\n",
    "df.loc[192, 'County'] = 'Kings County'\n",
    "df.loc[192, 'State'] = 'California'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "894d227a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:14.055041Z",
     "iopub.status.busy": "2023-09-11T21:34:14.054168Z",
     "iopub.status.idle": "2023-09-11T21:34:14.060167Z",
     "shell.execute_reply": "2023-09-11T21:34:14.059388Z"
    },
    "papermill": {
     "duration": 0.045983,
     "end_time": "2023-09-11T21:34:14.062427",
     "exception": false,
     "start_time": "2023-09-11T21:34:14.016444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# We discarded the 489th index of the Year Operational variable because it was empty. The plant here is both shutdown and the year of shutdown is missing. so this plant does not carry useful information for analysis.\n",
    "\n",
    "df.drop(index=489, inplace= True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c34e094e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:14.137091Z",
     "iopub.status.busy": "2023-09-11T21:34:14.136497Z",
     "iopub.status.idle": "2023-09-11T21:34:14.148442Z",
     "shell.execute_reply": "2023-09-11T21:34:14.146994Z"
    },
    "papermill": {
     "duration": 0.052369,
     "end_time": "2023-09-11T21:34:14.151115",
     "exception": false,
     "start_time": "2023-09-11T21:34:14.098746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "df.isnull().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f906b780",
   "metadata": {
    "papermill": {
     "duration": 0.037198,
     "end_time": "2023-09-11T21:34:14.226415",
     "exception": false,
     "start_time": "2023-09-11T21:34:14.189217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>5 |</span></b> <b>Feature Extraction</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "704d4f65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:14.302582Z",
     "iopub.status.busy": "2023-09-11T21:34:14.302181Z",
     "iopub.status.idle": "2023-09-11T21:34:14.306810Z",
     "shell.execute_reply": "2023-09-11T21:34:14.305542Z"
    },
    "papermill": {
     "duration": 0.045673,
     "end_time": "2023-09-11T21:34:14.309183",
     "exception": false,
     "start_time": "2023-09-11T21:34:14.263510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Present Day\n",
    "\n",
    "current_year = 2023"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20646d1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:14.385439Z",
     "iopub.status.busy": "2023-09-11T21:34:14.384124Z",
     "iopub.status.idle": "2023-09-11T21:34:14.391087Z",
     "shell.execute_reply": "2023-09-11T21:34:14.390083Z"
    },
    "papermill": {
     "duration": 0.047776,
     "end_time": "2023-09-11T21:34:14.393694",
     "exception": false,
     "start_time": "2023-09-11T21:34:14.345918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# We calculated how many years the farm has been running and added it to the dataset as a new variable.\n",
    "\n",
    "df[\"Operational Years\"] = current_year - df[\"Year Operational\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ead8ce8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:14.469785Z",
     "iopub.status.busy": "2023-09-11T21:34:14.469298Z",
     "iopub.status.idle": "2023-09-11T21:34:14.476498Z",
     "shell.execute_reply": "2023-09-11T21:34:14.475236Z"
    },
    "papermill": {
     "duration": 0.048017,
     "end_time": "2023-09-11T21:34:14.478845",
     "exception": false,
     "start_time": "2023-09-11T21:34:14.430828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Here, if the year of operation of the farm is less than 0, this indicates that the plant has not been commissioned yet, so we need to replace these negative values with 0.\n",
    "# In the 'Operational Years' column, we replaced the negative values with 0. Our aim is to emphasise that this plant has not been commissioned yet, i.e. it is not running.\n",
    "\n",
    "df.loc[df['Operational Years'] < 0, 'Operational Years'] = 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65120369",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:14.553612Z",
     "iopub.status.busy": "2023-09-11T21:34:14.553175Z",
     "iopub.status.idle": "2023-09-11T21:34:14.561266Z",
     "shell.execute_reply": "2023-09-11T21:34:14.560129Z"
    },
    "papermill": {
     "duration": 0.048472,
     "end_time": "2023-09-11T21:34:14.563741",
     "exception": false,
     "start_time": "2023-09-11T21:34:14.515269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# We have used the information of this variable and it is no longer needed. We dropped it from the data set.\n",
    "\n",
    "df.drop(\"Year Shutdown\", axis=1, inplace = True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5f30dd51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:14.640795Z",
     "iopub.status.busy": "2023-09-11T21:34:14.640070Z",
     "iopub.status.idle": "2023-09-11T21:34:14.660840Z",
     "shell.execute_reply": "2023-09-11T21:34:14.659937Z"
    },
    "papermill": {
     "duration": 0.061354,
     "end_time": "2023-09-11T21:34:14.663280",
     "exception": false,
     "start_time": "2023-09-11T21:34:14.601926",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Filter conditions based on \"Biogas End Use(s)\"\n",
    "conditions = [\n",
    "    df[\"Biogas End Use(s)\"] == \"Flared Full-time\",\n",
    "    df[\"Biogas End Use(s)\"] == \"CNG\",\n",
    "    df[\"Biogas End Use(s)\"] == \"Boiler/Furnace fuel\",\n",
    "    df[\"Biogas End Use(s)\"] == \"Boiler/Furnace fuel; CNG\",\n",
    "]\n",
    "\n",
    "# Combining the conditions\n",
    "combined_conditions = np.logical_or.reduce(conditions)\n",
    "\n",
    "# Replacing NaN values with 0 for the specified rows in 'Electricity Generated (kWh/yr)'\n",
    "df.loc[combined_conditions, 'Electricity Generated (kWh/yr)'] = df.loc[combined_conditions, 'Electricity Generated (kWh/yr)'].fillna(0)\n",
    "\n",
    "# Displaying the rows where changes were made\n",
    "df[combined_conditions][['Biogas End Use(s)', 'Electricity Generated (kWh/yr)']].head()\n",
    "\n",
    "# Our aim is to say that farms with this information do not convert gas into electricity."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65ebfba6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:14.740333Z",
     "iopub.status.busy": "2023-09-11T21:34:14.739679Z",
     "iopub.status.idle": "2023-09-11T21:34:15.307776Z",
     "shell.execute_reply": "2023-09-11T21:34:15.306539Z"
    },
    "papermill": {
     "duration": 0.610795,
     "end_time": "2023-09-11T21:34:15.311871",
     "exception": false,
     "start_time": "2023-09-11T21:34:14.701076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column].astype(str))\n",
    "    label_encoders[column] = le\n",
    "\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "\n",
    "for column, encoder in label_encoders.items():\n",
    "    df[column] = encoder.inverse_transform(df[column].astype(int))\n",
    "\n",
    "\n",
    "missing_values_after = df.isnull().sum()\n",
    "missing_values_after[missing_values_after > 0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb76133d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:15.418601Z",
     "iopub.status.busy": "2023-09-11T21:34:15.417329Z",
     "iopub.status.idle": "2023-09-11T21:34:15.430111Z",
     "shell.execute_reply": "2023-09-11T21:34:15.428963Z"
    },
    "papermill": {
     "duration": 0.053673,
     "end_time": "2023-09-11T21:34:15.432706",
     "exception": false,
     "start_time": "2023-09-11T21:34:15.379033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "df.isnull().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1ce0574",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:15.508842Z",
     "iopub.status.busy": "2023-09-11T21:34:15.508428Z",
     "iopub.status.idle": "2023-09-11T21:34:15.522599Z",
     "shell.execute_reply": "2023-09-11T21:34:15.521414Z"
    },
    "papermill": {
     "duration": 0.055018,
     "end_time": "2023-09-11T21:34:15.524998",
     "exception": false,
     "start_time": "2023-09-11T21:34:15.469980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "df['Cattle'] = df['Cattle'].astype('float64')\n",
    "df['Dairy'] = df['Dairy'].astype('float64')\n",
    "df['Poultry'] = df['Poultry'].astype('float64')\n",
    "df['Swine'] = df['Swine'].astype('float64')\n",
    "\n",
    "df[['Cattle', 'Dairy', 'Poultry', 'Swine']].dtypes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "313b59a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:15.602173Z",
     "iopub.status.busy": "2023-09-11T21:34:15.601739Z",
     "iopub.status.idle": "2023-09-11T21:34:15.608699Z",
     "shell.execute_reply": "2023-09-11T21:34:15.607544Z"
    },
    "papermill": {
     "duration": 0.049047,
     "end_time": "2023-09-11T21:34:15.611986",
     "exception": false,
     "start_time": "2023-09-11T21:34:15.562939",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Total Number of Animals\n",
    "\n",
    "df['Total_Animals'] = df['Cattle'] + df['Dairy'] + df['Poultry'] + df['Swine']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8139c1b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:15.690152Z",
     "iopub.status.busy": "2023-09-11T21:34:15.689744Z",
     "iopub.status.idle": "2023-09-11T21:34:15.703375Z",
     "shell.execute_reply": "2023-09-11T21:34:15.702354Z"
    },
    "papermill": {
     "duration": 0.055738,
     "end_time": "2023-09-11T21:34:15.705532",
     "exception": false,
     "start_time": "2023-09-11T21:34:15.649794",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Calculation of the amount of Biogas Production per Animal of each plant\n",
    "\n",
    "df['Biogas_per_Animal (cu-ft/day)'] = df['Biogas Generation Estimate (cu-ft/day)'] / df['Total_Animals']\n",
    "\n",
    "\n",
    "df[['Biogas Generation Estimate (cu-ft/day)', 'Total_Animals', 'Biogas_per_Animal (cu-ft/day)']].head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3afeff37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:15.782530Z",
     "iopub.status.busy": "2023-09-11T21:34:15.782022Z",
     "iopub.status.idle": "2023-09-11T21:34:15.789128Z",
     "shell.execute_reply": "2023-09-11T21:34:15.787939Z"
    },
    "papermill": {
     "duration": 0.048321,
     "end_time": "2023-09-11T21:34:15.791518",
     "exception": false,
     "start_time": "2023-09-11T21:34:15.743197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Emission Reduction by Operational Years\n",
    "\n",
    "df['Emission_Reduction_per_Year'] = df['Total Emission Reductions (MTCO2e/yr)'] / df['Operational Years']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ae578059",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:15.870496Z",
     "iopub.status.busy": "2023-09-11T21:34:15.870078Z",
     "iopub.status.idle": "2023-09-11T21:34:15.876459Z",
     "shell.execute_reply": "2023-09-11T21:34:15.875205Z"
    },
    "papermill": {
     "duration": 0.048851,
     "end_time": "2023-09-11T21:34:15.879156",
     "exception": false,
     "start_time": "2023-09-11T21:34:15.830305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# We will assume that facilities whose aim is inf have not made Emission_Reduction.\n",
    "\n",
    "df['Emission_Reduction_per_Year'].replace([np.inf, -np.inf], 0, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b31abaaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:15.958817Z",
     "iopub.status.busy": "2023-09-11T21:34:15.957984Z",
     "iopub.status.idle": "2023-09-11T21:34:15.964467Z",
     "shell.execute_reply": "2023-09-11T21:34:15.963443Z"
    },
    "papermill": {
     "duration": 0.048941,
     "end_time": "2023-09-11T21:34:15.966794",
     "exception": false,
     "start_time": "2023-09-11T21:34:15.917853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Biogas to Electricity Conversion Efficiency Analysis of Plants.\n",
    "\n",
    "df['Electricity_to_Biogas_Ratio'] = df['Electricity Generated (kWh/yr)'] / df['Biogas Generation Estimate (cu-ft/day)']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a937ad01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:16.049437Z",
     "iopub.status.busy": "2023-09-11T21:34:16.048029Z",
     "iopub.status.idle": "2023-09-11T21:34:16.077094Z",
     "shell.execute_reply": "2023-09-11T21:34:16.075856Z"
    },
    "papermill": {
     "duration": 0.073616,
     "end_time": "2023-09-11T21:34:16.079871",
     "exception": false,
     "start_time": "2023-09-11T21:34:16.006255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c118117b",
   "metadata": {
    "papermill": {
     "duration": 0.039415,
     "end_time": "2023-09-11T21:34:16.157670",
     "exception": false,
     "start_time": "2023-09-11T21:34:16.118255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color: #f8f5fc; padding: 50px; border-radius: 25px; border: 10px double #AD1457; box-shadow: 0 5px 20px 0 rgba(0, 0, 0, 0.2);\">\n",
    "    <h2 style=\"font-size: 40px; font-weight: bold; color: #880E4F; text-align: center; font-family: 'Arial'; padding: 35px 0; margin-top: 25px; border-top: 8px solid #FFC107; border-bottom: 8px solid #FFC107; text-shadow: 1px 1px 4px #b39ddb;\">🐄 Manure Production in US Livestock 🐓</h2>\n",
    "    <ul style=\"font-size: 24px; font-family: 'Arial'; padding: 30px 50px; color: #212121; line-height: 2.8; background-color: #f3e5f5; border-radius: 20px; box-shadow: 0 4px 10px 0 rgba(0, 0, 0, 0.1);\">\n",
    "        <li>🐄 <strong style=\"color: #6A1B9A; font-size: 26px; text-decoration: underline;\">Cattle (Dry Cow):</strong> Averages 1,000 lbs (453.6 kg) with a Manure Quantity of 81.4 lbs (36.9 kg).</li>\n",
    "        <li>🐄 <strong style=\"color: #6A1B9A; font-size: 26px; text-decoration: underline;\">Dairy (Holstein Cow):</strong> Averages 1,500 lbs (680 kg) with a Manure Quantity of 150 lbs (68 kg).</li>\n",
    "        <li>🐓 <strong style=\"color: #6A1B9A; font-size: 26px; text-decoration: underline;\">Poultry (Mixed Egg and Meat Chicken):</strong> Averages 7.7 lbs (3.5 kg) with a Manure Quantity of 0.62 lbs (0.28 kg).</li>\n",
    "        <li>🐖 <strong style=\"color: #6A1B9A; font-size: 26px; text-decoration: underline;\">Swine (Finisher Pig):</strong> Averages 150 lbs (68 kg) with a Manure Quantity of 12.6 lbs (5.7 kg).</li>\n",
    "    </ul>\n",
    "    <p style=\"font-size: 28px; color: #333; text-align: center; font-family: 'Arial'; padding: 30px; border-top: 5px dashed #AD1457; margin-top: 35px; background-color: #ede7f6; border-radius: 20px; box-shadow: 0 4px 10px 0 rgba(0, 0, 0, 0.1);\">📊 <strong style=\"color: #6A1B9A; text-decoration: underline;\">Total Waste (kg/day)</strong> = C × Number of Cattle + D × Number of Dairy + P × Number of Poultry + S × Number of Swine</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "17f0c1ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:16.246197Z",
     "iopub.status.busy": "2023-09-11T21:34:16.245807Z",
     "iopub.status.idle": "2023-09-11T21:34:16.265360Z",
     "shell.execute_reply": "2023-09-11T21:34:16.263783Z"
    },
    "papermill": {
     "duration": 0.071633,
     "end_time": "2023-09-11T21:34:16.269678",
     "exception": false,
     "start_time": "2023-09-11T21:34:16.198045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Using the amount of waste per animal, we calculated the total amount of waste for each facility.\n",
    "\n",
    "df['Total_Waste_Ibs/day'] = (df['Cattle'] * 81.4) + \\\n",
    "                           (df['Dairy'] * 150) + \\\n",
    "                           (df['Poultry'] * 0.62) + \\\n",
    "                           (df['Swine'] * 12.6)\n",
    "\n",
    "# İlk 5 satırı gösterelim\n",
    "df[['Cattle', 'Dairy', 'Poultry', 'Swine', 'Total_Waste_Ibs/day']].head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee05629d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:16.360641Z",
     "iopub.status.busy": "2023-09-11T21:34:16.359496Z",
     "iopub.status.idle": "2023-09-11T21:34:16.366006Z",
     "shell.execute_reply": "2023-09-11T21:34:16.365166Z"
    },
    "papermill": {
     "duration": 0.048686,
     "end_time": "2023-09-11T21:34:16.368342",
     "exception": false,
     "start_time": "2023-09-11T21:34:16.319656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# We create the Waste Efficiency variable.  It shows how much biogas is produced per kilogram of waste for each plant.\n",
    "\n",
    "df['Waste_Efficiency'] = df['Biogas Generation Estimate (cu-ft/day)'] / df['Total_Waste_Ibs/day']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ca16491",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:16.446942Z",
     "iopub.status.busy": "2023-09-11T21:34:16.446555Z",
     "iopub.status.idle": "2023-09-11T21:34:16.452759Z",
     "shell.execute_reply": "2023-09-11T21:34:16.451317Z"
    },
    "papermill": {
     "duration": 0.048698,
     "end_time": "2023-09-11T21:34:16.455120",
     "exception": false,
     "start_time": "2023-09-11T21:34:16.406422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# We create the Electricity Efficiency variable. it shows how much electricity each facility generates per kilogram of waste.\n",
    "\n",
    "df['Electricity_Efficiency'] = df['Electricity Generated (kWh/yr)'] / df['Total_Waste_Ibs/day']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c73cd8d9",
   "metadata": {
    "papermill": {
     "duration": 0.038723,
     "end_time": "2023-09-11T21:34:16.532366",
     "exception": false,
     "start_time": "2023-09-11T21:34:16.493643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>6 |</span></b> <b>Project Count by State</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "3c826aeb",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:16.611567Z",
     "iopub.status.busy": "2023-09-11T21:34:16.610929Z",
     "iopub.status.idle": "2023-09-11T21:34:18.040755Z",
     "shell.execute_reply": "2023-09-11T21:34:18.039795Z"
    },
    "papermill": {
     "duration": 1.472924,
     "end_time": "2023-09-11T21:34:18.044014",
     "exception": false,
     "start_time": "2023-09-11T21:34:16.571090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "df_state_projects = df['dw5'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "df_state_projects.sort_values(ascending=False).plot(kind='bar', color='skyblue', alpha=0.8)\n",
    "\n",
    "plt.title('Project Count by dw5', weight='bold', fontsize=18)\n",
    "plt.ylabel('Number of Projects', weight='bold', fontsize=14)\n",
    "plt.xlabel('States', weight='bold', fontsize=14)\n",
    "plt.xticks(rotation=90, weight='bold', fontsize=12)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "\n",
    "for i, value in enumerate(df_state_projects.sort_values(ascending=False)):\n",
    "    plt.text(i, value + 1, str(value), ha='center', va='bottom', weight='bold', fontsize=12)\n",
    "\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Project Count by dw5.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "19a8e58d",
   "metadata": {
    "papermill": {
     "duration": 0.040339,
     "end_time": "2023-09-11T21:34:18.125182",
     "exception": false,
     "start_time": "2023-09-11T21:34:18.084843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>7 |</span></b> <b>Top 10 Project Count by County</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "8fc53bae",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:18.211342Z",
     "iopub.status.busy": "2023-09-11T21:34:18.210625Z",
     "iopub.status.idle": "2023-09-11T21:34:19.094854Z",
     "shell.execute_reply": "2023-09-11T21:34:19.093743Z"
    },
    "papermill": {
     "duration": 0.931565,
     "end_time": "2023-09-11T21:34:19.097766",
     "exception": false,
     "start_time": "2023-09-11T21:34:18.166201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "df_county_projects = df['dw6'].value_counts().head(10)\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "df_county_projects.sort_values(ascending=False).plot(kind='bar', color='lightcoral', alpha=0.8)\n",
    "\n",
    "plt.title('Top 10 Project Count by dw6', weight='bold', fontsize=18)\n",
    "plt.ylabel('Number of Projects', weight='bold', fontsize=14)\n",
    "plt.xlabel('Counties', weight='bold', fontsize=14)\n",
    "plt.xticks(rotation=90, weight='bold', fontsize=12)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "\n",
    "for i, value in enumerate(df_county_projects.sort_values(ascending=False)):\n",
    "    plt.text(i, value + 0.2, str(value), ha='center', va='bottom', weight='bold', fontsize=12)\n",
    "\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Top 10 Project Count by dw6.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c2598658",
   "metadata": {
    "papermill": {
     "duration": 0.041244,
     "end_time": "2023-09-11T21:34:19.180527",
     "exception": false,
     "start_time": "2023-09-11T21:34:19.139283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>8 |</span></b> <b>Top 10 Project Count by City</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c632c2b",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:19.266836Z",
     "iopub.status.busy": "2023-09-11T21:34:19.265902Z",
     "iopub.status.idle": "2023-09-11T21:34:20.183754Z",
     "shell.execute_reply": "2023-09-11T21:34:20.182553Z"
    },
    "papermill": {
     "duration": 0.964021,
     "end_time": "2023-09-11T21:34:20.186143",
     "exception": false,
     "start_time": "2023-09-11T21:34:19.222122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "df_city_projects = df['B相有功功率'].value_counts().head(10)\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "df_city_projects.sort_values(ascending=False).plot(kind='bar', color='lightgreen', alpha=0.8)\n",
    "\n",
    "plt.title('Top 10 Project Count by City', weight='bold', fontsize=18)\n",
    "plt.ylabel('Number of Projects', weight='bold', fontsize=14)\n",
    "plt.xlabel('Cities', weight='bold', fontsize=14)\n",
    "plt.xticks(rotation=90, weight='bold', fontsize=12)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "\n",
    "for i, value in enumerate(df_city_projects.sort_values(ascending=False)):\n",
    "    plt.text(i, value + 0.2, str(value), ha='center', va='bottom', weight='bold', fontsize=12)\n",
    "\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Top 10 Project Count by City.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fe78f421",
   "metadata": {
    "papermill": {
     "duration": 0.043761,
     "end_time": "2023-09-11T21:34:20.274636",
     "exception": false,
     "start_time": "2023-09-11T21:34:20.230875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>9 |</span></b> <b>Distribution by Project Type</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "fd54eb45",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:20.365436Z",
     "iopub.status.busy": "2023-09-11T21:34:20.365020Z",
     "iopub.status.idle": "2023-09-11T21:34:20.774695Z",
     "shell.execute_reply": "2023-09-11T21:34:20.773561Z"
    },
    "papermill": {
     "duration": 0.457934,
     "end_time": "2023-09-11T21:34:20.777825",
     "exception": false,
     "start_time": "2023-09-11T21:34:20.319891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "df_project_type = df['C相有功功率'].value_counts()\n",
    "\n",
    "explode = [0.05 for _ in range(len(df_project_type))]\n",
    "colors = sns.color_palette(\"pastel\")\n",
    "\n",
    "df_project_type.plot(kind='pie', autopct='%1.1f%%', startangle=140, colors=colors, explode=explode, textprops={'fontsize': 14, 'weight': 'bold'})\n",
    "\n",
    "plt.title('Distribution by Project Type', weight='bold', fontsize=18)\n",
    "plt.ylabel('')\n",
    "plt.savefig('Distribution by Project Type.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1b405719",
   "metadata": {
    "papermill": {
     "duration": 0.044885,
     "end_time": "2023-09-11T21:34:20.867745",
     "exception": false,
     "start_time": "2023-09-11T21:34:20.822860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>10 |</span></b> <b>Distribution by Project Status</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "de096b43",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:20.960303Z",
     "iopub.status.busy": "2023-09-11T21:34:20.959645Z",
     "iopub.status.idle": "2023-09-11T21:34:21.327267Z",
     "shell.execute_reply": "2023-09-11T21:34:21.326114Z"
    },
    "papermill": {
     "duration": 0.416935,
     "end_time": "2023-09-11T21:34:21.329752",
     "exception": false,
     "start_time": "2023-09-11T21:34:20.912817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "df_project_status = df['B面第1节下热箱显示区温度'].value_counts()\n",
    "\n",
    "explode = [0.05 for _ in range(len(df_project_status))]\n",
    "colors = sns.color_palette(\"Set3\")\n",
    "\n",
    "df_project_status.plot(kind='pie', autopct='%1.1f%%', startangle=140, colors=colors, explode=explode, textprops={'fontsize': 14, 'weight': 'bold'})\n",
    "\n",
    "plt.title('Distribution by Project Status', weight='bold', fontsize=18)\n",
    "plt.ylabel('')\n",
    "plt.savefig('Distribution by Project Status.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "89a4634d",
   "metadata": {
    "papermill": {
     "duration": 0.046249,
     "end_time": "2023-09-11T21:34:21.423142",
     "exception": false,
     "start_time": "2023-09-11T21:34:21.376893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>11 |</span></b> <b>Distribution of Projects Based on USDA Funding</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "ae41949b",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:21.522220Z",
     "iopub.status.busy": "2023-09-11T21:34:21.521592Z",
     "iopub.status.idle": "2023-09-11T21:34:21.883624Z",
     "shell.execute_reply": "2023-09-11T21:34:21.882426Z"
    },
    "papermill": {
     "duration": 0.413405,
     "end_time": "2023-09-11T21:34:21.886250",
     "exception": false,
     "start_time": "2023-09-11T21:34:21.472845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "df_usda_funding = df['Awarded USDA Funding?'].value_counts()\n",
    "\n",
    "explode = [0.05 for _ in range(len(df_usda_funding))]\n",
    "colors = sns.color_palette(\"husl\")\n",
    "\n",
    "df_usda_funding.plot(kind='pie', autopct='%1.1f%%', startangle=140, colors=colors, explode=explode, textprops={'fontsize': 14, 'weight': 'bold'})\n",
    "\n",
    "# Title\n",
    "plt.title('Distribution of Projects Based on USDA Funding', weight='bold', fontsize=18)\n",
    "plt.ylabel('')\n",
    "plt.savefig('Distribution of Projects Based on USDA Funding.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "905d852e",
   "metadata": {
    "papermill": {
     "duration": 0.048478,
     "end_time": "2023-09-11T21:34:21.983727",
     "exception": false,
     "start_time": "2023-09-11T21:34:21.935249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>12 |</span></b> <b>Total Electricity Production by Project Type</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "774e8b6e",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:22.083792Z",
     "iopub.status.busy": "2023-09-11T21:34:22.083303Z",
     "iopub.status.idle": "2023-09-11T21:34:22.568606Z",
     "shell.execute_reply": "2023-09-11T21:34:22.567140Z"
    },
    "papermill": {
     "duration": 0.538214,
     "end_time": "2023-09-11T21:34:22.571233",
     "exception": false,
     "start_time": "2023-09-11T21:34:22.033019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "project_type_electricity = df.groupby('Project Type')['Electricity Generated (kWh/yr)'].sum()\n",
    "explode = [0.1] * len(project_type_electricity)\n",
    "colors = sns.color_palette(\"pastel\", len(project_type_electricity))\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "project_type_electricity.plot(kind='pie', explode=explode, colors=colors, autopct='%1.1f%%', startangle=140, \n",
    "                              textprops={'fontsize': 14, 'weight': 'bold'})\n",
    "plt.title('Total Electricity Production by Project Type', fontsize=16, weight='bold')\n",
    "plt.ylabel('')  \n",
    "plt.tight_layout()\n",
    "plt.savefig('Total Electricity Production by Project Type.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "92dc3904",
   "metadata": {
    "papermill": {
     "duration": 0.050133,
     "end_time": "2023-09-11T21:34:22.672274",
     "exception": false,
     "start_time": "2023-09-11T21:34:22.622141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>13 |</span></b> <b>Total Animal Count by State</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1760746",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:22.775244Z",
     "iopub.status.busy": "2023-09-11T21:34:22.774176Z",
     "iopub.status.idle": "2023-09-11T21:34:23.987711Z",
     "shell.execute_reply": "2023-09-11T21:34:23.986551Z"
    },
    "papermill": {
     "duration": 1.268281,
     "end_time": "2023-09-11T21:34:23.990421",
     "exception": false,
     "start_time": "2023-09-11T21:34:22.722140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "state_animals = df.groupby('State')['Total_Animals'].sum()\n",
    "\n",
    "sorted_state_animals = state_animals.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(25, 8))\n",
    "sorted_state_animals.plot(kind='bar', color='lightgreen')\n",
    "plt.title('Total Animal Count by State', fontsize=16, weight='bold')\n",
    "plt.ylabel('Number of Animals', fontsize=14, weight='bold')\n",
    "plt.xlabel('State', fontsize=14, weight='bold')\n",
    "plt.xticks(rotation=90, weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Total Animal Count by State.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "de3e8825",
   "metadata": {
    "papermill": {
     "duration": 0.052306,
     "end_time": "2023-09-11T21:34:24.095345",
     "exception": false,
     "start_time": "2023-09-11T21:34:24.043039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>14 |</span></b> <b>Biogas Production by State</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "28003e74",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:24.201949Z",
     "iopub.status.busy": "2023-09-11T21:34:24.201513Z",
     "iopub.status.idle": "2023-09-11T21:34:25.545341Z",
     "shell.execute_reply": "2023-09-11T21:34:25.544534Z"
    },
    "papermill": {
     "duration": 1.401395,
     "end_time": "2023-09-11T21:34:25.548587",
     "exception": false,
     "start_time": "2023-09-11T21:34:24.147192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "df_biogas_production = df.groupby('State')['Biogas Generation Estimate (cu-ft/day)'].sum().sort_values(ascending=False)\n",
    "plt.figure(figsize=(25,8))\n",
    "\n",
    "df_biogas_production.plot(kind='bar', color='lightcoral', alpha=0.8)\n",
    "\n",
    "plt.title('Biogas Production by State', weight='bold', fontsize=18)\n",
    "plt.ylabel('Biogas Production (cu-ft/day)', weight='bold', fontsize=14)\n",
    "plt.xlabel('States', weight='bold', fontsize=14)\n",
    "plt.xticks(rotation=90, weight='bold', fontsize=12)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Biogas Production by State.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0550e4b4",
   "metadata": {
    "papermill": {
     "duration": 0.054389,
     "end_time": "2023-09-11T21:34:25.657689",
     "exception": false,
     "start_time": "2023-09-11T21:34:25.603300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>15 |</span></b> <b>Electricity Production by State</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "3f97c255",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:25.770503Z",
     "iopub.status.busy": "2023-09-11T21:34:25.769798Z",
     "iopub.status.idle": "2023-09-11T21:34:26.981946Z",
     "shell.execute_reply": "2023-09-11T21:34:26.981013Z"
    },
    "papermill": {
     "duration": 1.271219,
     "end_time": "2023-09-11T21:34:26.984720",
     "exception": false,
     "start_time": "2023-09-11T21:34:25.713501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "plt.figure(figsize=(25,8))\n",
    "df_electricity_production = df.groupby('State')['Electricity Generated (kWh/yr)'].sum().sort_values(ascending=False)\n",
    "\n",
    "df_electricity_production.plot(kind='bar', color='lightgreen', alpha=0.8)\n",
    "\n",
    "plt.title('Electricity Production by State', weight='bold', fontsize=18)\n",
    "plt.ylabel('Electricity Production (kWh/yr)', weight='bold', fontsize=14)\n",
    "plt.xlabel('States', weight='bold', fontsize=14)\n",
    "plt.xticks(rotation=90, weight='bold', fontsize=12)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Electricity Production by State.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a3475cc5",
   "metadata": {
    "papermill": {
     "duration": 0.056646,
     "end_time": "2023-09-11T21:34:27.098535",
     "exception": false,
     "start_time": "2023-09-11T21:34:27.041889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>16 |</span></b> <b>Biogas Production by Project Type</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "c26937b5",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:27.214940Z",
     "iopub.status.busy": "2023-09-11T21:34:27.214136Z",
     "iopub.status.idle": "2023-09-11T21:34:27.789712Z",
     "shell.execute_reply": "2023-09-11T21:34:27.788384Z"
    },
    "papermill": {
     "duration": 0.637063,
     "end_time": "2023-09-11T21:34:27.792180",
     "exception": false,
     "start_time": "2023-09-11T21:34:27.155117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "animal_types = df['A相有功功率'].unique()\n",
    "biogas_by_animal = df.groupby('A相有功功率')['B相有功功率'].sum().loc[animal_types]\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "biogas_by_animal.sort_values().plot(kind='barh', color=sns.color_palette(\"muted\"), alpha=0.8)\n",
    "\n",
    "plt.title('Biogas Production by Project Type', weight='bold', fontsize=18)\n",
    "plt.xlabel('Biogas Production (cu-ft/day)', weight='bold', fontsize=14)\n",
    "plt.ylabel('Project Types', weight='bold', fontsize=14)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "plt.xticks(weight='bold', fontsize=12)\n",
    "\n",
    "plt.grid(axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Biogas Production by Project Type.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b63c0134",
   "metadata": {
    "papermill": {
     "duration": 0.057358,
     "end_time": "2023-09-11T21:34:27.906936",
     "exception": false,
     "start_time": "2023-09-11T21:34:27.849578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>17 |</span></b> <b>Biogas Production by Digester Type</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "2cc9ca40",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:28.025863Z",
     "iopub.status.busy": "2023-09-11T21:34:28.024923Z",
     "iopub.status.idle": "2023-09-11T21:34:28.828981Z",
     "shell.execute_reply": "2023-09-11T21:34:28.827908Z"
    },
    "papermill": {
     "duration": 0.866416,
     "end_time": "2023-09-11T21:34:28.831996",
     "exception": false,
     "start_time": "2023-09-11T21:34:27.965580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "biogas_by_digester = df.groupby('Digester Type')['Biogas Generation Estimate (cu-ft/day)'].sum()\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "biogas_by_digester.sort_values().plot(kind='barh', color='mediumpurple', alpha=0.8)\n",
    "\n",
    "plt.title('Biogas Production by Digester Type', weight='bold', fontsize=18)\n",
    "plt.xlabel('Biogas Production (cu-ft/day)', weight='bold', fontsize=14)\n",
    "plt.ylabel('Digester Types', weight='bold', fontsize=14)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "plt.xticks(weight='bold', fontsize=12)\n",
    "\n",
    "plt.grid(axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Biogas Production by Digester Type.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "81b64470",
   "metadata": {
    "papermill": {
     "duration": 0.058286,
     "end_time": "2023-09-11T21:34:28.949225",
     "exception": false,
     "start_time": "2023-09-11T21:34:28.890939",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>18 |</span></b> <b>Number of Projects Operational by Year</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "b8092185",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:29.071639Z",
     "iopub.status.busy": "2023-09-11T21:34:29.070456Z",
     "iopub.status.idle": "2023-09-11T21:34:29.833818Z",
     "shell.execute_reply": "2023-09-11T21:34:29.832560Z"
    },
    "papermill": {
     "duration": 0.827987,
     "end_time": "2023-09-11T21:34:29.836679",
     "exception": false,
     "start_time": "2023-09-11T21:34:29.008692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "df_yearly_projects = df['C相有功功率'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "df_yearly_projects.plot(kind='line', marker='o', color='orange', linewidth=2.5)\n",
    "\n",
    "plt.title('Number of Projects Operational by Year', weight='bold', fontsize=20)\n",
    "plt.ylabel('Number of Projects', weight='bold', fontsize=16)\n",
    "plt.xlabel('Year', weight='bold', fontsize=16)\n",
    "plt.xticks(rotation=45, weight='bold', fontsize=14)\n",
    "plt.yticks(weight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Number of Projects Operational by Year.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "673a1a93",
   "metadata": {
    "papermill": {
     "duration": 0.062144,
     "end_time": "2023-09-11T21:34:29.961711",
     "exception": false,
     "start_time": "2023-09-11T21:34:29.899567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>19 |</span></b> <b>Total Biogas Production Over Years</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "205543a7",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:30.089123Z",
     "iopub.status.busy": "2023-09-11T21:34:30.088625Z",
     "iopub.status.idle": "2023-09-11T21:34:30.950739Z",
     "shell.execute_reply": "2023-09-11T21:34:30.949611Z"
    },
    "papermill": {
     "duration": 0.928822,
     "end_time": "2023-09-11T21:34:30.953119",
     "exception": false,
     "start_time": "2023-09-11T21:34:30.024297",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "df_total_biogas_yearly = df.groupby('Year Operational')['Biogas Generation Estimate (cu-ft/day)'].sum()\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "df_total_biogas_yearly.plot(kind='area', color='skyblue', alpha=0.7)\n",
    "\n",
    "plt.title('Total Biogas Production Over Years', weight='bold', fontsize=18)\n",
    "plt.ylabel('Total Biogas Production (cu-ft/day)', weight='bold', fontsize=14)\n",
    "plt.xlabel('Year', weight='bold', fontsize=14)\n",
    "plt.xticks(weight='bold', fontsize=12)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Total Biogas Production Over Years.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1256ab1f",
   "metadata": {
    "papermill": {
     "duration": 0.065048,
     "end_time": "2023-09-11T21:34:31.083800",
     "exception": false,
     "start_time": "2023-09-11T21:34:31.018752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>20 |</span></b> <b>Yearly Biogas Production by State</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "7637ebc4",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:31.215397Z",
     "iopub.status.busy": "2023-09-11T21:34:31.214964Z",
     "iopub.status.idle": "2023-09-11T21:34:36.731055Z",
     "shell.execute_reply": "2023-09-11T21:34:36.729890Z"
    },
    "papermill": {
     "duration": 5.586319,
     "end_time": "2023-09-11T21:34:36.734570",
     "exception": false,
     "start_time": "2023-09-11T21:34:31.148251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "df_biogas_stacked = df.groupby(['A相有功功率', 'B相有功功率'])['C相有功功率'].sum().unstack().fillna(0)\n",
    "\n",
    "sorted_columns = df_biogas_stacked.sum(axis=0).sort_values(ascending=False).index\n",
    "df_biogas_stacked_sorted = df_biogas_stacked[sorted_columns]\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "df_biogas_stacked_sorted.plot(kind='bar', stacked=True, colormap='tab20', figsize=(20,10))\n",
    "\n",
    "plt.title('Yearly Biogas Production by State', weight='bold', fontsize=18)\n",
    "plt.ylabel('Total Biogas Production (cu-ft/day)', weight='bold', fontsize=14)\n",
    "plt.xlabel('Year', weight='bold', fontsize=14)\n",
    "plt.legend(title='States', fontsize=10, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xticks(weight='bold', fontsize=12, rotation=90)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Yearly Biogas Production by State.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f3e6c57",
   "metadata": {
    "papermill": {
     "duration": 0.065965,
     "end_time": "2023-09-11T21:34:36.868338",
     "exception": false,
     "start_time": "2023-09-11T21:34:36.802373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>21 |</span></b> <b>Cattle Count by State</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "e7a67da3",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:37.005407Z",
     "iopub.status.busy": "2023-09-11T21:34:37.004766Z",
     "iopub.status.idle": "2023-09-11T21:34:38.497224Z",
     "shell.execute_reply": "2023-09-11T21:34:38.496117Z"
    },
    "papermill": {
     "duration": 1.56414,
     "end_time": "2023-09-11T21:34:38.500351",
     "exception": false,
     "start_time": "2023-09-11T21:34:36.936211",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "cattle_by_state = df.groupby('State')['Cattle'].sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "cattle_by_state.plot(kind='bar', color='lightblue')\n",
    "\n",
    "plt.title('Cattle Count by State', weight='bold', fontsize=18)\n",
    "plt.ylabel('Number of Cattle', weight='bold', fontsize=14)\n",
    "plt.xlabel('States', weight='bold', fontsize=14)\n",
    "plt.xticks(weight='bold', fontsize=12, rotation=90)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Cattle Count by State.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1fc8cd76",
   "metadata": {
    "papermill": {
     "duration": 0.068972,
     "end_time": "2023-09-11T21:34:38.638927",
     "exception": false,
     "start_time": "2023-09-11T21:34:38.569955",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>22 |</span></b> <b>Dairy Count by State</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "c00c9a4f",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:38.780645Z",
     "iopub.status.busy": "2023-09-11T21:34:38.779813Z",
     "iopub.status.idle": "2023-09-11T21:34:40.019175Z",
     "shell.execute_reply": "2023-09-11T21:34:40.018036Z"
    },
    "papermill": {
     "duration": 1.314382,
     "end_time": "2023-09-11T21:34:40.022908",
     "exception": false,
     "start_time": "2023-09-11T21:34:38.708526",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "dairy_by_state = df.groupby('State')['Dairy'].sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "dairy_by_state.plot(kind='bar', color='lightcoral')\n",
    "\n",
    "plt.title('Dairy Count by State', weight='bold', fontsize=18)\n",
    "plt.ylabel('Number of Dairy Cattle', weight='bold', fontsize=14)\n",
    "plt.xlabel('States', weight='bold', fontsize=14)\n",
    "plt.xticks(weight='bold', fontsize=12, rotation=90)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Dairy Count by State.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "746f06b7",
   "metadata": {
    "papermill": {
     "duration": 0.075339,
     "end_time": "2023-09-11T21:34:40.171252",
     "exception": false,
     "start_time": "2023-09-11T21:34:40.095913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>23 |</span></b> <b>Poultry Count by State</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "5be39ee7",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:40.323809Z",
     "iopub.status.busy": "2023-09-11T21:34:40.323399Z",
     "iopub.status.idle": "2023-09-11T21:34:41.563599Z",
     "shell.execute_reply": "2023-09-11T21:34:41.562700Z"
    },
    "papermill": {
     "duration": 1.320531,
     "end_time": "2023-09-11T21:34:41.566133",
     "exception": false,
     "start_time": "2023-09-11T21:34:40.245602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "poultry_by_state = df.groupby('State')['Poultry'].sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "poultry_by_state.plot(kind='bar', color='lightgreen')\n",
    "\n",
    "plt.title('Poultry Count by State', weight='bold', fontsize=18)\n",
    "plt.ylabel('Number of Poultry', weight='bold', fontsize=14)\n",
    "plt.xlabel('States', weight='bold', fontsize=14)\n",
    "plt.xticks(weight='bold', fontsize=12, rotation=90)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Poultry Count by State.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc675c52",
   "metadata": {
    "papermill": {
     "duration": 0.074118,
     "end_time": "2023-09-11T21:34:41.715868",
     "exception": false,
     "start_time": "2023-09-11T21:34:41.641750",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>24 |</span></b> <b>Swine Count by State</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "bdf71abb",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:41.874417Z",
     "iopub.status.busy": "2023-09-11T21:34:41.873673Z",
     "iopub.status.idle": "2023-09-11T21:34:43.057830Z",
     "shell.execute_reply": "2023-09-11T21:34:43.056933Z"
    },
    "papermill": {
     "duration": 1.264495,
     "end_time": "2023-09-11T21:34:43.060381",
     "exception": false,
     "start_time": "2023-09-11T21:34:41.795886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "swine_by_state = df.groupby('State')['Swine'].sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "swine_by_state.plot(kind='bar', color='lightpink')\n",
    "\n",
    "plt.title('Swine Count by State', weight='bold', fontsize=18)\n",
    "plt.ylabel('Number of Swine', weight='bold', fontsize=14)\n",
    "plt.xlabel('States', weight='bold', fontsize=14)\n",
    "plt.xticks(weight='bold', fontsize=12, rotation=90)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Swine Count by State.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6db72f17",
   "metadata": {
    "papermill": {
     "duration": 0.077627,
     "end_time": "2023-09-11T21:34:43.216152",
     "exception": false,
     "start_time": "2023-09-11T21:34:43.138525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>25 |</span></b> <b>Distribution of Biogas End Uses</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "22fabb1f",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:43.373851Z",
     "iopub.status.busy": "2023-09-11T21:34:43.373380Z",
     "iopub.status.idle": "2023-09-11T21:34:44.457656Z",
     "shell.execute_reply": "2023-09-11T21:34:44.456429Z"
    },
    "papermill": {
     "duration": 1.167827,
     "end_time": "2023-09-11T21:34:44.461547",
     "exception": false,
     "start_time": "2023-09-11T21:34:43.293720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "biogas_end_uses = df['Biogas End Use(s)'].str.split(',', expand=True).stack().str.strip().value_counts()\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "biogas_end_uses.plot(kind='bar', color='lightblue')\n",
    "\n",
    "plt.title('Distribution of Biogas End Uses', weight='bold', fontsize=18)\n",
    "plt.ylabel('Count', weight='bold', fontsize=14)\n",
    "plt.xlabel('Biogas End Uses', weight='bold', fontsize=14)\n",
    "plt.xticks(weight='bold', fontsize=12, rotation=90)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Distribution of Biogas End Uses.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "13d1cf33",
   "metadata": {
    "papermill": {
     "duration": 0.079431,
     "end_time": "2023-09-11T21:34:44.622903",
     "exception": false,
     "start_time": "2023-09-11T21:34:44.543472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>26 |</span></b> <b>Distribution of Projects by LCFS Pathway</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "c04196d0",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:44.787024Z",
     "iopub.status.busy": "2023-09-11T21:34:44.786577Z",
     "iopub.status.idle": "2023-09-11T21:34:45.275729Z",
     "shell.execute_reply": "2023-09-11T21:34:45.274329Z"
    },
    "papermill": {
     "duration": 0.573951,
     "end_time": "2023-09-11T21:34:45.278624",
     "exception": false,
     "start_time": "2023-09-11T21:34:44.704673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "lcfs_pathway_counts = df['LCFS Pathway?'].value_counts()\n",
    "\n",
    "explode = (0.1, 0)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "lcfs_pathway_counts.plot(kind='pie', autopct=lambda p: '{:.1f}% ({:,.0f})'.format(p, (p/100)*lcfs_pathway_counts.sum()), \n",
    "                         startangle=90, colors=sns.color_palette(\"pastel\"), explode=explode, textprops={'weight': 'bold'})\n",
    "\n",
    "plt.title('Distribution of Projects by LCFS Pathway', weight='bold', fontsize=16)\n",
    "plt.ylabel('') \n",
    "plt.tight_layout()\n",
    "plt.savefig('Distribution of Projects by LCFS Pathway.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3cb2cc1e",
   "metadata": {
    "papermill": {
     "duration": 0.081039,
     "end_time": "2023-09-11T21:34:45.444198",
     "exception": false,
     "start_time": "2023-09-11T21:34:45.363159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>27 |</span></b> <b>Biogas Production Comparison Based on Co-Digestion</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "070f1584",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:45.610612Z",
     "iopub.status.busy": "2023-09-11T21:34:45.609602Z",
     "iopub.status.idle": "2023-09-11T21:34:46.045727Z",
     "shell.execute_reply": "2023-09-11T21:34:46.044776Z"
    },
    "papermill": {
     "duration": 0.522771,
     "end_time": "2023-09-11T21:34:46.049009",
     "exception": false,
     "start_time": "2023-09-11T21:34:45.526238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "biogas_by_codigestion = df.groupby('Co-Digestion')['Biogas Generation Estimate (cu-ft/day)'].mean()\n",
    "\n",
    "explode = (0.1, 0)  \n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "biogas_by_codigestion.plot(kind='pie', autopct=lambda p: '{:.1f}% ({:,.0f} cu-ft/day)'.format(p, (p/100)*biogas_by_codigestion.sum()), \n",
    "                           startangle=90, colors=sns.color_palette(\"pastel\"), explode=explode, textprops={'weight': 'bold'})\n",
    "\n",
    "plt.title('Biogas Production Comparison Based on Co-Digestion', weight='bold', fontsize=16)\n",
    "plt.ylabel('') \n",
    "plt.tight_layout()\n",
    "plt.savefig('Biogas Production Comparison Based on Co-Digestion.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d1024b2b",
   "metadata": {
    "papermill": {
     "duration": 0.080781,
     "end_time": "2023-09-11T21:34:46.211345",
     "exception": false,
     "start_time": "2023-09-11T21:34:46.130564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>28 |</span></b> <b>Project Counts by Receiving Utility</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "fd846794",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:46.400367Z",
     "iopub.status.busy": "2023-09-11T21:34:46.399933Z",
     "iopub.status.idle": "2023-09-11T21:34:46.823314Z",
     "shell.execute_reply": "2023-09-11T21:34:46.822081Z"
    },
    "papermill": {
     "duration": 0.511656,
     "end_time": "2023-09-11T21:34:46.825699",
     "exception": false,
     "start_time": "2023-09-11T21:34:46.314043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "receiving_utility_counts = df['Receiving Utility'].value_counts()\n",
    "\n",
    "explode = [0.1] * len(receiving_utility_counts)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "receiving_utility_counts.plot(kind='pie', autopct=lambda p: '{:.1f}% ({:,.0f} projects)'.format(p, (p/100)*receiving_utility_counts.sum()), \n",
    "                              startangle=90, colors=sns.color_palette(\"pastel\", len(receiving_utility_counts)), explode=explode, textprops={'weight': 'bold'})\n",
    "\n",
    "plt.title('Project Counts by Receiving Utility', weight='bold', fontsize=16)\n",
    "plt.ylabel('') \n",
    "plt.tight_layout()\n",
    "plt.savefig('Project Counts by Receiving Utility.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6f7a9076",
   "metadata": {
    "papermill": {
     "duration": 0.083121,
     "end_time": "2023-09-11T21:34:46.993057",
     "exception": false,
     "start_time": "2023-09-11T21:34:46.909936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>29 |</span></b> <b>Distribution of Projects by Number of Operational Years</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "8c9f6cc4",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:47.163042Z",
     "iopub.status.busy": "2023-09-11T21:34:47.162586Z",
     "iopub.status.idle": "2023-09-11T21:34:48.452229Z",
     "shell.execute_reply": "2023-09-11T21:34:48.450842Z"
    },
    "papermill": {
     "duration": 1.37687,
     "end_time": "2023-09-11T21:34:48.455102",
     "exception": false,
     "start_time": "2023-09-11T21:34:47.078232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "operational_years = (df['Year Operational'].max() - df['Year Operational']) + 1\n",
    "operational_years_distribution = operational_years.value_counts()\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "operational_years_distribution.sort_values(ascending=False).plot(kind='bar', color='lightgreen')\n",
    "\n",
    "plt.title('Distribution of Projects by Number of Operational Years', weight='bold', fontsize=16)\n",
    "plt.ylabel('Number of Projects', weight='bold', fontsize=14)\n",
    "plt.xlabel('Number of Operational Years', weight='bold', fontsize=14)\n",
    "plt.xticks(weight='bold', fontsize=12, rotation=0)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Distribution of Projects by Number of Operational Years.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d7dca67a",
   "metadata": {
    "papermill": {
     "duration": 0.083965,
     "end_time": "2023-09-11T21:34:48.624971",
     "exception": false,
     "start_time": "2023-09-11T21:34:48.541006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>30 |</span></b> <b>USDA Funding vs. Biogas Production</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "82ad4b56",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:48.795650Z",
     "iopub.status.busy": "2023-09-11T21:34:48.794940Z",
     "iopub.status.idle": "2023-09-11T21:34:49.214738Z",
     "shell.execute_reply": "2023-09-11T21:34:49.213277Z"
    },
    "papermill": {
     "duration": 0.508836,
     "end_time": "2023-09-11T21:34:49.217399",
     "exception": false,
     "start_time": "2023-09-11T21:34:48.708563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "biogas_total_by_usda_funding = df.groupby('Awarded USDA Funding?')['Biogas Generation Estimate (cu-ft/day)'].sum()\n",
    "\n",
    "colors = ['lightcoral', 'lightblue']\n",
    "explode = (0.1, 0) \n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "biogas_total_by_usda_funding.plot(kind='pie', explode=explode, colors=colors, autopct='%1.1f%%', \n",
    "                                 startangle=140, textprops={'fontsize': 12, 'weight': 'bold'})\n",
    "plt.title('USDA Funding vs. Biogas Production', weight='bold', fontsize=16)\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('USDA Funding vs. Biogas Production.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "83e7a9ff",
   "metadata": {
    "papermill": {
     "duration": 0.087155,
     "end_time": "2023-09-11T21:34:49.393243",
     "exception": false,
     "start_time": "2023-09-11T21:34:49.306088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>31 |</span></b> <b>Annual Emission Reduction Comparison by State</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "736ae819",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:49.563923Z",
     "iopub.status.busy": "2023-09-11T21:34:49.563144Z",
     "iopub.status.idle": "2023-09-11T21:34:50.911784Z",
     "shell.execute_reply": "2023-09-11T21:34:50.910750Z"
    },
    "papermill": {
     "duration": 1.439375,
     "end_time": "2023-09-11T21:34:50.915436",
     "exception": false,
     "start_time": "2023-09-11T21:34:49.476061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "emission_by_state = df.groupby('State')['Total Emission Reductions (MTCO2e/yr)'].sum()\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "emission_by_state.sort_values(ascending=False).plot(kind='bar', color='lightsalmon', grid=True)\n",
    "\n",
    "plt.title('Annual Emission Reduction Comparison by State', weight='bold', fontsize=18)\n",
    "plt.ylabel('Emission Reduction (MTCO2e/yr)', weight='bold', fontsize=14)\n",
    "plt.xlabel('States', weight='bold', fontsize=14)\n",
    "plt.xticks(weight='bold', fontsize=12, rotation=90)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Annual Emission Reduction Comparison by State.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3af3f21d",
   "metadata": {
    "papermill": {
     "duration": 0.089168,
     "end_time": "2023-09-11T21:34:51.095949",
     "exception": false,
     "start_time": "2023-09-11T21:34:51.006781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>32 |</span></b> <b>Distribution of Electricity to Biogas Ratio by State</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "0edbd1dc",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:51.277775Z",
     "iopub.status.busy": "2023-09-11T21:34:51.276958Z",
     "iopub.status.idle": "2023-09-11T21:34:52.744837Z",
     "shell.execute_reply": "2023-09-11T21:34:52.743651Z"
    },
    "papermill": {
     "duration": 1.563313,
     "end_time": "2023-09-11T21:34:52.748466",
     "exception": false,
     "start_time": "2023-09-11T21:34:51.185153",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "electricity_biogas_ratio_by_state = df.groupby('State')['Electricity_to_Biogas_Ratio'].mean()\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "electricity_biogas_ratio_by_state.sort_values(ascending=False).plot(kind='bar', color='lightblue')\n",
    "\n",
    "plt.title('Distribution of Electricity to Biogas Ratio by State', weight='bold', fontsize=18)\n",
    "plt.ylabel('Electricity to Biogas Ratio', weight='bold', fontsize=14)\n",
    "plt.xlabel('States', weight='bold', fontsize=14)\n",
    "plt.xticks(weight='bold', fontsize=12, rotation=90)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Distribution of Electricity to Biogas Ratio by State.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5b072bd3",
   "metadata": {
    "papermill": {
     "duration": 0.091498,
     "end_time": "2023-09-11T21:34:52.934867",
     "exception": false,
     "start_time": "2023-09-11T21:34:52.843369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>33 |</span></b> <b>Total Daily Waste by State</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d173d17",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:53.119541Z",
     "iopub.status.busy": "2023-09-11T21:34:53.118680Z",
     "iopub.status.idle": "2023-09-11T21:34:54.385877Z",
     "shell.execute_reply": "2023-09-11T21:34:54.384643Z"
    },
    "papermill": {
     "duration": 1.36133,
     "end_time": "2023-09-11T21:34:54.389535",
     "exception": false,
     "start_time": "2023-09-11T21:34:53.028205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "waste_by_state = df.groupby('State')['Total_Waste_Ibs/day'].sum()\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "waste_by_state.sort_values(ascending=False).plot(kind='bar', color='lightgreen')\n",
    "\n",
    "plt.title('Total Daily Waste by State', weight='bold', fontsize=18)\n",
    "plt.ylabel('Total_Waste_Ibs/day', weight='bold', fontsize=14)\n",
    "plt.xlabel('States', weight='bold', fontsize=14)\n",
    "plt.xticks(rotation=90, weight='bold', fontsize=12)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Total Daily Waste by State.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d9dc97ef",
   "metadata": {
    "papermill": {
     "duration": 0.094143,
     "end_time": "2023-09-11T21:34:54.577603",
     "exception": false,
     "start_time": "2023-09-11T21:34:54.483460",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>34 |</span></b> <b>Correlation Analysis Results</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "f78cbe64",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:54.773234Z",
     "iopub.status.busy": "2023-09-11T21:34:54.772254Z",
     "iopub.status.idle": "2023-09-11T21:34:55.425705Z",
     "shell.execute_reply": "2023-09-11T21:34:55.423845Z"
    },
    "papermill": {
     "duration": 0.757451,
     "end_time": "2023-09-11T21:34:55.429028",
     "exception": false,
     "start_time": "2023-09-11T21:34:54.671577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "correlation_biogas_animals = df[['Biogas Generation Estimate (cu-ft/day)', 'Total_Animals']].corr().iloc[0, 1]\n",
    "\n",
    "correlation_electricity_biogas = df[['Electricity Generated (kWh/yr)', 'Biogas Generation Estimate (cu-ft/day)']].corr().iloc[0, 1]\n",
    "\n",
    "correlation_waste_biogas = df[['Total_Waste_Ibs/day', 'Biogas Generation Estimate (cu-ft/day)']].corr().iloc[0, 1]\n",
    "\n",
    "correlations = {\n",
    "    'Biogas & Total Animals': correlation_biogas_animals,\n",
    "    'Electricity & Biogas': correlation_electricity_biogas,\n",
    "    'Waste & Biogas': correlation_waste_biogas\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.bar(correlations.keys(), correlations.values(), color=['blue', 'green', 'red'])\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.title('Correlation Analysis Results')\n",
    "plt.ylim(-1, 1)\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Correlation Analysis Results.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "841d8540",
   "metadata": {
    "papermill": {
     "duration": 0.095605,
     "end_time": "2023-09-11T21:34:55.624920",
     "exception": false,
     "start_time": "2023-09-11T21:34:55.529315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>35 |</span></b> <b>Total Waste Production by Animal Type</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "0436a890",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:55.815551Z",
     "iopub.status.busy": "2023-09-11T21:34:55.814354Z",
     "iopub.status.idle": "2023-09-11T21:34:56.518374Z",
     "shell.execute_reply": "2023-09-11T21:34:56.517237Z"
    },
    "papermill": {
     "duration": 0.801439,
     "end_time": "2023-09-11T21:34:56.521121",
     "exception": false,
     "start_time": "2023-09-11T21:34:55.719682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "animal_types_corrected = ['Cattle', 'Dairy', 'Poultry', 'Swine']\n",
    "\n",
    "total_waste_per_animal_type_corrected = df[animal_types_corrected].multiply(df['Total_Waste_Ibs/day'], axis=0).sum()\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "total_waste_per_animal_type_corrected.sort_values().plot(kind='barh', color='lightseagreen')\n",
    "plt.title('Total Waste Production by Animal Type', weight='bold', fontsize=16)\n",
    "plt.xlabel('Total Waste (kg)', weight='bold', fontsize=14)\n",
    "plt.ylabel('Animal Types', weight='bold', fontsize=14)\n",
    "plt.xticks(weight='bold', fontsize=12)\n",
    "plt.yticks(weight='bold', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Total Waste Production by Animal Type.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a30aa5dc",
   "metadata": {
    "papermill": {
     "duration": 0.095107,
     "end_time": "2023-09-11T21:34:56.713846",
     "exception": false,
     "start_time": "2023-09-11T21:34:56.618739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>36 |</span></b> <b>Biogas Project Status Map in the USA</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "9b154228",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:56.904714Z",
     "iopub.status.busy": "2023-09-11T21:34:56.903916Z",
     "iopub.status.idle": "2023-09-11T21:34:58.228917Z",
     "shell.execute_reply": "2023-09-11T21:34:58.227827Z"
    },
    "papermill": {
     "duration": 1.423097,
     "end_time": "2023-09-11T21:34:58.231935",
     "exception": false,
     "start_time": "2023-09-11T21:34:56.808838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Download the GeoJSON file for US states\n",
    "url = \"https://raw.githubusercontent.com/PublicaMundi/MappingAPI/master/data/geojson/us-states.json\"\n",
    "states_geo = gpd.read_file(url)\n",
    "\n",
    "state_status_counts = df.groupby(['State', 'Status']).size().unstack().fillna(0)\n",
    "\n",
    "# Calculate the center coordinates for the states\n",
    "states_geo['center'] = states_geo['geometry'].centroid\n",
    "states_points = states_geo.copy()\n",
    "states_points.set_geometry(\"center\", inplace=True)\n",
    "\n",
    "# Create the map\n",
    "m = folium.Map(location=[37.0902, -95.7129], zoom_start=4)\n",
    "\n",
    "colors = {\n",
    "    \"Shut down\": \"red\",\n",
    "    \"Operational\": \"green\",\n",
    "    \"Construction\": \"orange\"\n",
    "}\n",
    "\n",
    "for idx, row in states_points.iterrows():\n",
    "    state_name = row['name']\n",
    "    for status, color in colors.items():\n",
    "        count = state_status_counts.loc[state_name, status] if state_name in state_status_counts.index else 0\n",
    "        if count > 0:\n",
    "            folium.Marker(\n",
    "                location=(row['center'].y, row['center'].x),\n",
    "                icon=folium.Icon(color=color, icon=\"info-sign\"),\n",
    "                popup=f\"{state_name}<br>{status}: {count}\"\n",
    "            ).add_to(m)\n",
    "\n",
    "# Add a legend displaying color information (moved to the top right corner)\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; \n",
    "            top: 10px; right: 10px; \n",
    "            width: 150px; height: 90px; \n",
    "            border:2px solid grey; z-index:9999;\n",
    "            font-size:14px;\">\n",
    "    &nbsp;<b>Status</b><br>\n",
    "    &nbsp;<i class=\"fa fa-map-marker fa-2x\" style=\"color:red\"></i>&nbsp;Shut down<br>\n",
    "    &nbsp;<i class=\"fa fa-map-marker fa-2x\" style=\"color:green\"></i>&nbsp;Operational<br>\n",
    "    &nbsp;<i class=\"fa fa-map-marker fa-2x\" style=\"color:orange\"></i>&nbsp;Construction<br>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "m\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7d19fb25",
   "metadata": {
    "papermill": {
     "duration": 0.106428,
     "end_time": "2023-09-11T21:34:58.442174",
     "exception": false,
     "start_time": "2023-09-11T21:34:58.335746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>37 |</span></b> <b>Analysis Table</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "01c2e0d0",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-09-11T21:34:58.647910Z",
     "iopub.status.busy": "2023-09-11T21:34:58.647234Z",
     "iopub.status.idle": "2023-09-11T21:34:58.747452Z",
     "shell.execute_reply": "2023-09-11T21:34:58.746224Z"
    },
    "papermill": {
     "duration": 0.206761,
     "end_time": "2023-09-11T21:34:58.750081",
     "exception": false,
     "start_time": "2023-09-11T21:34:58.543320",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "state_grouped = df.groupby('State').agg(\n",
    "    Total_Projects=pd.NamedAgg(column='Project Name', aggfunc='count'),\n",
    "    Operational_Projects=pd.NamedAgg(column='Status', aggfunc=lambda x: (x == 'Operational').sum()),\n",
    "    Avg_Operational_Years=pd.NamedAgg(column='Operational Years', aggfunc='mean'),\n",
    "    Total_Animals=pd.NamedAgg(column='Total_Animals', aggfunc='sum'),\n",
    "    Cattle=pd.NamedAgg(column='Cattle', aggfunc='sum'),\n",
    "    Dairy=pd.NamedAgg(column='Dairy', aggfunc='sum'),\n",
    "    Poultry=pd.NamedAgg(column='Poultry', aggfunc='sum'),\n",
    "    Swine=pd.NamedAgg(column='Swine', aggfunc='sum'),\n",
    "    Avg_Biogas_Production=pd.NamedAgg(column='Biogas Generation Estimate (cu-ft/day)', aggfunc='mean'),\n",
    "    Total_Biogas_Production=pd.NamedAgg(column='Biogas Generation Estimate (cu-ft/day)', aggfunc='sum'),\n",
    "    Avg_Waste=pd.NamedAgg(column='Total_Waste_Ibs/day', aggfunc='mean'),\n",
    "    Total_Waste=pd.NamedAgg(column='Total_Waste_Ibs/day', aggfunc='sum'),\n",
    "    Total_Electricity_Generated=pd.NamedAgg(column='Electricity Generated (kWh/yr)', aggfunc='sum'),\n",
    "    Avg_Emission_Reduction=pd.NamedAgg(column='Total Emission Reductions (MTCO2e/yr)', aggfunc='mean'),\n",
    "    Number_of_Digesters=pd.NamedAgg(column='Digester Type', aggfunc='nunique'),\n",
    "    USDA_Funded_Projects=pd.NamedAgg(column='Awarded USDA Funding?', aggfunc=lambda x: (x == 'Yes').sum()),\n",
    "    Most_Common_Use=pd.NamedAgg(column='Biogas End Use(s)', aggfunc=lambda x: x.value_counts().index[0] if not x.empty else None),\n",
    "    Use_Types=pd.NamedAgg(column='Biogas End Use(s)', aggfunc=lambda x: \", \".join(x.unique()))\n",
    ").reset_index()\n",
    "\n",
    "state_grouped.index = range(1, len(state_grouped) + 1)\n",
    "\n",
    "state_grouped = state_grouped.sort_values(by='Total_Projects', ascending=False)\n",
    "\n",
    "state_grouped"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d0f49f4a",
   "metadata": {
    "papermill": {
     "duration": 0.102866,
     "end_time": "2023-09-11T21:34:58.955386",
     "exception": false,
     "start_time": "2023-09-11T21:34:58.852520",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>38 |</span></b> <b>Conclusions and Recommendations</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5b212",
   "metadata": {
    "papermill": {
     "duration": 0.103154,
     "end_time": "2023-09-11T21:34:59.160929",
     "exception": false,
     "start_time": "2023-09-11T21:34:59.057775",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color: #e8f0ff; padding: 40px; border-radius: 20px; border: 8px double #1976D2; box-shadow: 0 4px 16px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size: 36px; font-weight: bold; color: #0D47A1; text-align: center; font-family: 'Arial'; padding: 25px 0; margin-top: 20px; border-top: 6px solid #FFEB3B; border-bottom: 6px solid #FFEB3B; text-shadow: 1px 1px 3px #90caf9;\">🌍 Comprehensive and In-depth Analysis of Livestock Farms with Biogas Facilities in the US 🌍</h2>\n",
    "    <ul style=\"font-size: 22px; font-family: 'Arial'; padding: 25px 40px; color: #212121; line-height: 2.5; background-color: #e3f2fd; border-radius: 15px; box-shadow: 0 3px 8px 0 rgba(0, 0, 0, 0.1);\">\n",
    "        <li>🌿 <strong style=\"color: #1565C0; font-size: 24px; text-decoration: underline;\">General Structure and Organization:</strong> The organization, regulatory compliance, and adaptability to local requirements of biogas farms in different states are pivotal for the effectiveness and economic viability of sustainable energy production.</li>\n",
    "        <li>⚙️ <strong style=\"color: #1565C0; font-size: 24px; text-decoration: underline;\">Operation, Management, and Economic Factors:</strong> Business strategies, environmental sustainability, societal contributions, and economic efficiency are paramount. Competent management is the cornerstone for future achievements and financial sustainability in the biogas sector.</li>\n",
    "        <li>📈 <strong style=\"color: #1565C0; font-size: 24px; text-decoration: underline;\">Impacts, Opportunities, and Profitability:</strong> The social, economic, and environmental ramifications of biogas farms pave the way for potential opportunities, risks, and economic profitability. Grasping these implications is crucial for sculpting future strategies, policies, and investment choices.</li>\n",
    "        <li>🔍 <strong style=\"color: #1565C0; font-size: 24px; text-decoration: underline;\">Suggestions and Future Perspectives:</strong> The analysis underscores the necessity for enhanced control, regulation, innovation, and economic scrutiny in managing and operating biogas farms. Collaborations with business sectors, investors, governmental bodies, and other organizations can aid in making strategic investment choices and forging innovative solutions. Moreover, endorsing practices that mitigate environmental impacts and promote sustainable energy can bolster economic growth, generate employment opportunities, and uplift societal welfare.</li>\n",
    "    </ul>\n",
    "    <p style=\"font-size: 26px; color: #333; text-align: center; font-family: 'Arial'; padding: 25px; border-top: 4px dashed #1976D2; margin-top: 30px; background-color: #e1f5fe; border-radius: 15px; box-shadow: 0 3px 8px 0 rgba(0, 0, 0, 0.1);\">Enhancing biogas facilities and practices can lead to a more sustainable and economically prosperous future.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94268a5",
   "metadata": {
    "papermill": {
     "duration": 0.101271,
     "end_time": "2023-09-11T21:34:59.363616",
     "exception": false,
     "start_time": "2023-09-11T21:34:59.262345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"text-align: center; padding: 60px; background: url('https://source.unsplash.com/800x600?energy') no-repeat center/cover; border: 5px solid #FFEB3B; border-radius: 35px; box-shadow: 0 15px 25px rgba(0, 0, 0, 0.2);\">\n",
    "    <p style=\"font-size: 26px; margin-bottom: 35px; color: #000000; font-family: 'Arial', sans-serif; font-weight: bold; text-transform: uppercase; letter-spacing: 3px; text-shadow: 4px 4px 8px rgba(0, 0, 0, 0.5); animation: slide 3s infinite alternate;\">\n",
    "        Click for the Next Analysis => U.S. Farms Biogas ML Prediction(Livestock)\n",
    "    </p>\n",
    "    <a href=\"https://www.kaggle.com/code/mehmetisik/u-s-farms-biogas-ml-prediction-livestock/notebook\" target=\"_blank\" style=\"text-decoration: none; display: inline-block; padding: 15px 30px; font-size: 24px; color: #673AB7; background-color: #FFFFFF; border-radius: 50px; transition: transform 0.3s ease; box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2);\">👉</a>\n",
    "    <style>\n",
    "        @keyframes pulsate {\n",
    "            0% { transform: scale(1); }\n",
    "            50% { transform: scale(1.08); }\n",
    "            100% { transform: scale(1); }\n",
    "        }\n",
    "        @keyframes slide {\n",
    "            0% { transform: translateX(-10px); }\n",
    "            100% { transform: translateX(10px); }\n",
    "        }\n",
    "        a:hover {\n",
    "            transform: translateY(-5px);\n",
    "            box-shadow: 0 8px 16px rgba(0, 0, 0, 0.3);\n",
    "        }\n",
    "    </style>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59299275",
   "metadata": {
    "papermill": {
     "duration": 0.104904,
     "end_time": "2023-09-11T21:34:59.575760",
     "exception": false,
     "start_time": "2023-09-11T21:34:59.470856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<style>\n",
    "  .link-card {\n",
    "    background-color: #f9f9f9;\n",
    "    border: 1px solid #e0e0e0;\n",
    "    padding: 15px;\n",
    "    border-radius: 8px;\n",
    "    margin: 10px 0;\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    text-decoration: none;\n",
    "    color: #333;\n",
    "    transition: background-color 0.3s, transform 0.3s, box-shadow 0.3s;\n",
    "  }\n",
    "  \n",
    "  .link-card:hover {\n",
    "    background-color: #f0f0f0;\n",
    "    transform: translateY(-5px);\n",
    "    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); /* Hover efekti için gölge eklendi */\n",
    "  }\n",
    "  \n",
    "  .link-icon {\n",
    "    font-size: 50px;\n",
    "    margin-right: 15px;\n",
    "  }\n",
    "  \n",
    "  .link-text {\n",
    "    font-size: 20px; /* İsimlerin boyutunu 20px olarak güncelledim */\n",
    "    font-weight: bold; /* Kalın yazı tipi */\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<a href=\"https://www.kaggle.com/mehmetisik/code\" class=\"link-card\">\n",
    "  <span class=\"link-icon\">📊</span>\n",
    "  <span class=\"link-text\">Mehmet ISIK's Notebook</span>\n",
    "</a>"
   ]
  }
 ],
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": 3
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.6.4"
 },
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 71.740753,
   "end_time": "2023-09-11T21:35:00.602373",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-09-11T21:33:48.861620",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
