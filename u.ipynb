{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae93cc3",
   "metadata": {
    "papermill": {
     "duration": 0.028415,
     "end_time": "2023-09-10T22:55:04.627837",
     "exception": false,
     "start_time": "2023-09-10T22:55:04.599422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>1 |</span></b> <b>Importing Libraries</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "ecf58fcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:04.687412Z",
     "iopub.status.busy": "2023-09-10T22:55:04.686866Z",
     "iopub.status.idle": "2023-09-10T22:55:08.660634Z",
     "shell.execute_reply": "2023-09-10T22:55:08.659397Z"
    },
    "papermill": {
     "duration": 4.007158,
     "end_time": "2023-09-10T22:55:08.663593",
     "exception": false,
     "start_time": "2023-09-10T22:55:04.656435",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "!pip install catboost\n",
    "!pip install lightgbm\n",
    "!pip install xgboost\n",
    "\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML as html_print\n",
    "from termcolor import colored\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"scipy\")\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e007830e",
   "metadata": {
    "papermill": {
     "duration": 0.02804,
     "end_time": "2023-09-10T22:55:08.720483",
     "exception": false,
     "start_time": "2023-09-10T22:55:08.692443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>2 |</span></b> <b>Adjusting Row & Column Settings</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "291f974e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:08.781130Z",
     "iopub.status.busy": "2023-09-10T22:55:08.780663Z",
     "iopub.status.idle": "2023-09-10T22:55:08.785886Z",
     "shell.execute_reply": "2023-09-10T22:55:08.785086Z"
    },
    "papermill": {
     "duration": 0.037447,
     "end_time": "2023-09-10T22:55:08.787954",
     "exception": false,
     "start_time": "2023-09-10T22:55:08.750507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "259f9dd9",
   "metadata": {
    "papermill": {
     "duration": 0.02812,
     "end_time": "2023-09-10T22:55:08.844840",
     "exception": false,
     "start_time": "2023-09-10T22:55:08.816720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>3 |</span></b> <b>Loading The Data Set</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "36cf32fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:08.904156Z",
     "iopub.status.busy": "2023-09-10T22:55:08.903354Z",
     "iopub.status.idle": "2023-09-10T22:55:08.935451Z",
     "shell.execute_reply": "2023-09-10T22:55:08.934061Z"
    },
    "papermill": {
     "duration": 0.064988,
     "end_time": "2023-09-10T22:55:08.938305",
     "exception": false,
     "start_time": "2023-09-10T22:55:08.873317",
     "status": "completed"
    },
    "tags": []
   },
   "source": "df = pd.read_csv('D:/code/junma/600000/0424/2.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63407290",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:08.999336Z",
     "iopub.status.busy": "2023-09-10T22:55:08.998912Z",
     "iopub.status.idle": "2023-09-10T22:55:09.010513Z",
     "shell.execute_reply": "2023-09-10T22:55:09.009287Z"
    },
    "papermill": {
     "duration": 0.04579,
     "end_time": "2023-09-10T22:55:09.013141",
     "exception": false,
     "start_time": "2023-09-10T22:55:08.967351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def print_section_title(title):\n",
    "    print(colored(title, 'blue', attrs=['bold', 'underline']))\n",
    "    \n",
    "def display_head_and_tail(dataframe, head=5):\n",
    "    display(dataframe.head(head).style.set_caption(\"Head\"))\n",
    "    display(dataframe.tail(head).style.set_caption(\"Tail\"))\n",
    "\n",
    "def display_na(dataframe):\n",
    "    na_df = dataframe.isnull().sum().reset_index()\n",
    "    na_df.columns = ['Column', 'Number of NA']\n",
    "    display(na_df.style.set_caption(\"Number of NA Values\"))\n",
    "\n",
    "def display_quantiles(dataframe):\n",
    "    quantiles_df = dataframe.describe([0, 0.05, 0.50, 0.95, 0.99, 1]).T\n",
    "    display(quantiles_df.style.format(\"{:.2f}\").set_caption(\"Quantiles\"))\n",
    "\n",
    "def check_df(dataframe, head=5):\n",
    "    print_section_title('Shape')\n",
    "    print(dataframe.shape)\n",
    "    print_section_title('Types')\n",
    "    print(dataframe.dtypes.to_frame('Data Type').style.set_caption(\"Data Types\"))\n",
    "    print_section_title('Info')\n",
    "    print(dataframe.info())\n",
    "    print_section_title('Head & Tail')\n",
    "    display_head_and_tail(dataframe, head)\n",
    "    print_section_title('NA Values')\n",
    "    display_na(dataframe)\n",
    "    print_section_title('Quantiles')\n",
    "    display_quantiles(dataframe)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b27ac7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:09.072383Z",
     "iopub.status.busy": "2023-09-10T22:55:09.071594Z",
     "iopub.status.idle": "2023-09-10T22:55:09.258915Z",
     "shell.execute_reply": "2023-09-10T22:55:09.257727Z"
    },
    "papermill": {
     "duration": 0.219952,
     "end_time": "2023-09-10T22:55:09.261406",
     "exception": false,
     "start_time": "2023-09-10T22:55:09.041454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "check_df(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53d2f7aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:09.474311Z",
     "iopub.status.busy": "2023-09-10T22:55:09.473886Z",
     "iopub.status.idle": "2023-09-10T22:55:09.501440Z",
     "shell.execute_reply": "2023-09-10T22:55:09.500091Z"
    },
    "papermill": {
     "duration": 0.063099,
     "end_time": "2023-09-10T22:55:09.504262",
     "exception": false,
     "start_time": "2023-09-10T22:55:09.441163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "68892701",
   "metadata": {
    "papermill": {
     "duration": 0.031018,
     "end_time": "2023-09-10T22:55:09.566983",
     "exception": false,
     "start_time": "2023-09-10T22:55:09.535965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>4 |</span></b> <b>Capturing / Detecting Numeric and Categorical Variables</b></div>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def grab_col_names(dataframe, cat_th=10, car_th=20, force_cat_cols=None, force_num_cols=None):\n",
    "    \"\"\"\n",
    "    Returns the names of categorical, numeric and categorical but cardinal variables in the data set.\n",
    "    Now with options to force specific columns to be treated as categorical or numeric.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        dataframe: dataframe\n",
    "                Variable names of the dataframe to be taken\n",
    "        cat_th: int, optional\n",
    "                class threshold for numeric but categorical variables\n",
    "        car_th: int, optional\n",
    "                class threshold for categorical but cardinal variables\n",
    "        force_cat_cols: list, optional\n",
    "                List of column names to force treat as categorical\n",
    "        force_num_cols: list, optional\n",
    "                List of column names to force treat as numeric\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "        cat_cols: list\n",
    "                Categorical variable list\n",
    "        num_cols: list\n",
    "                Numeric variable list\n",
    "        cat_but_car: list\n",
    "                List of cardinal variables with categorical appearance\n",
    "        num_but_cat: list\n",
    "                List of numeric but categorical variables\n",
    "    \"\"\"\n",
    "    # Initialize force_cat_cols and force_num_cols if not provided\n",
    "    if force_cat_cols is None:\n",
    "        force_cat_cols = []\n",
    "    if force_num_cols is None:\n",
    "        force_num_cols = []\n",
    "\n",
    "    # 1. Get standard categorical columns (object type)\n",
    "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n",
    "\n",
    "    # 2. Get numeric but categorical columns (excluding force_num_cols)\n",
    "    num_but_cat = [col for col in dataframe.columns\n",
    "                   if dataframe[col].nunique() < cat_th\n",
    "                   and dataframe[col].dtypes != \"O\"\n",
    "                   and col not in force_num_cols]\n",
    "\n",
    "    # 3. Get categorical but cardinal columns\n",
    "    cat_but_car = [col for col in dataframe.columns\n",
    "                   if dataframe[col].nunique() > car_th\n",
    "                   and dataframe[col].dtypes == \"O\"]\n",
    "\n",
    "    # 4. Combine standard categorical and numeric-but-categorical\n",
    "    cat_cols = cat_cols + num_but_cat\n",
    "\n",
    "    # 5. Remove cardinal columns from categorical\n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "\n",
    "    # 6. Get numeric columns (including force_num_cols)\n",
    "    num_cols = [col for col in dataframe.columns\n",
    "                if dataframe[col].dtypes != \"O\"\n",
    "                and col not in num_but_cat\n",
    "                or col in force_num_cols]\n",
    "\n",
    "    # 7. Handle forced categorical columns\n",
    "    for col in force_cat_cols:\n",
    "        if col in num_cols:\n",
    "            num_cols.remove(col)\n",
    "        if col in num_but_cat:\n",
    "            num_but_cat.remove(col)\n",
    "        if col in cat_but_car:\n",
    "            cat_but_car.remove(col)\n",
    "        if col not in cat_cols:\n",
    "            cat_cols.append(col)\n",
    "\n",
    "    # 8. Handle forced numeric columns\n",
    "    for col in force_num_cols:\n",
    "        if col in cat_cols:\n",
    "            cat_cols.remove(col)\n",
    "        if col in num_but_cat:\n",
    "            num_but_cat.remove(col)\n",
    "        if col in cat_but_car:\n",
    "            cat_but_car.remove(col)\n",
    "        if col not in num_cols:\n",
    "            num_cols.append(col)\n",
    "\n",
    "    print(f\"Observations: {dataframe.shape[0]}\")\n",
    "    print(f\"Variables: {dataframe.shape[1]}\")\n",
    "    print(f'cat_cols: {len(cat_cols)}')\n",
    "    print(f'num_cols: {len(num_cols)}')\n",
    "    print(f'cat_but_car: {len(cat_but_car)}')\n",
    "    print(f'num_but_cat: {len(num_but_cat)}')\n",
    "    forced_caterical = (\n",
    "  #  [f'M_dw{i}' for i in range(2, 7)] +  # M_dw2到M_dw6\n",
    "    [f'R_dw{i}' for i in range(22, 27)] +  # R_dw22到R_dw26\n",
    "    [f'R_dw{i}' for i in range(29, 34)]  # R_dw29到R_dw33\n",
    ")\n",
    "    forced_numerical = (\n",
    "    [f'M_dw{i}' for i in range(2, 7)]   # M_dw2到M_dw6\n",
    "   # [f'R_dw{i}' for i in range(22, 27)] +  # R_dw22到R_dw26\n",
    "   # [f'R_dw{i}' for i in range(29, 34)]  # R_dw29到R_dw33\n",
    ")\n",
    "    return cat_cols, num_cols, cat_but_car, num_but_cat\n",
    "\n",
    "\n",
    "\n",
    "# 指定要强制作为数值变量的列\n",
    "#forced_numerical = ['m_dw1', 'm_dw4', 'm_dw5']\n",
    "\n",
    "# 指定要强制作为分类变量的列\n",
    "    # 生成强制数值列\n"
   ],
   "id": "5f2aadd50c2d1e14",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11ef50bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:09.715571Z",
     "iopub.status.busy": "2023-09-10T22:55:09.715171Z",
     "iopub.status.idle": "2023-09-10T22:55:09.728051Z",
     "shell.execute_reply": "2023-09-10T22:55:09.726773Z"
    },
    "papermill": {
     "duration": 0.049275,
     "end_time": "2023-09-10T22:55:09.730642",
     "exception": false,
     "start_time": "2023-09-10T22:55:09.681367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "cat_cols, num_cols, cat_but_car,  num_but_cat = grab_col_names(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a492f18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:09.804400Z",
     "iopub.status.busy": "2023-09-10T22:55:09.803082Z",
     "iopub.status.idle": "2023-09-10T22:55:09.813336Z",
     "shell.execute_reply": "2023-09-10T22:55:09.812196Z"
    },
    "papermill": {
     "duration": 0.04918,
     "end_time": "2023-09-10T22:55:09.817362",
     "exception": false,
     "start_time": "2023-09-10T22:55:09.768182",
     "status": "completed"
    },
    "tags": []
   },
   "source": "cat_cols",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28200b61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:09.905530Z",
     "iopub.status.busy": "2023-09-10T22:55:09.904803Z",
     "iopub.status.idle": "2023-09-10T22:55:09.911581Z",
     "shell.execute_reply": "2023-09-10T22:55:09.910768Z"
    },
    "papermill": {
     "duration": 0.053279,
     "end_time": "2023-09-10T22:55:09.913756",
     "exception": false,
     "start_time": "2023-09-10T22:55:09.860477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "num_cols"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "70af3edb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:09.980729Z",
     "iopub.status.busy": "2023-09-10T22:55:09.979947Z",
     "iopub.status.idle": "2023-09-10T22:55:09.986597Z",
     "shell.execute_reply": "2023-09-10T22:55:09.985243Z"
    },
    "papermill": {
     "duration": 0.042348,
     "end_time": "2023-09-10T22:55:09.988856",
     "exception": false,
     "start_time": "2023-09-10T22:55:09.946508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "cat_but_car"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3b4740b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:10.060870Z",
     "iopub.status.busy": "2023-09-10T22:55:10.060444Z",
     "iopub.status.idle": "2023-09-10T22:55:10.067695Z",
     "shell.execute_reply": "2023-09-10T22:55:10.066347Z"
    },
    "papermill": {
     "duration": 0.044279,
     "end_time": "2023-09-10T22:55:10.070122",
     "exception": false,
     "start_time": "2023-09-10T22:55:10.025843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "num_but_cat"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a19d29b0",
   "metadata": {
    "papermill": {
     "duration": 0.032142,
     "end_time": "2023-09-10T22:55:10.134865",
     "exception": false,
     "start_time": "2023-09-10T22:55:10.102723",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>5 |</span></b> <b>Analysis of Categorical Variables</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "71a679c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:10.203271Z",
     "iopub.status.busy": "2023-09-10T22:55:10.202883Z",
     "iopub.status.idle": "2023-09-10T22:55:10.210043Z",
     "shell.execute_reply": "2023-09-10T22:55:10.208780Z"
    },
    "papermill": {
     "duration": 0.043492,
     "end_time": "2023-09-10T22:55:10.212374",
     "exception": false,
     "start_time": "2023-09-10T22:55:10.168882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def cat_summary(dataframe, col_name, plot=False):\n",
    "    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n",
    "                        'Ratio': 100 * dataframe[col_name].value_counts() / len(dataframe)}))\n",
    "    print('##########################################')\n",
    "    if plot:\n",
    "        plt.figure(figsize=(20,8))\n",
    "        sns.countplot(x=dataframe[col_name], data=dataframe)\n",
    "        plt.xticks(rotation=90)  # Rotate x-axis labels by 90 degrees\n",
    "        plt.show(block=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "#\n",
    "#\n",
    "# df = pd.read_csv('D:/code/junma/600000/0424/1.csv')\n",
    "# def plot_device_parameters(dataframe, device_name, subsystem_col='subsystem', time_col='created_at'):\n",
    "#     \"\"\"\n",
    "#     绘制指定设备的所有参数随时间变化的图表\n",
    "#\n",
    "#     参数:\n",
    "#     dataframe -- 包含设备数据的DataFrame\n",
    "#     device_name -- 要绘制的设备名称\n",
    "#     subsystem_col -- 包含子系统/设备名称的列名\n",
    "#     time_col -- 包含时间戳的列名\n",
    "#     \"\"\"\n",
    "#     # 筛选指定设备的数据\n",
    "#     device_data = dataframe[dataframe[subsystem_col] == device_name]\n",
    "#\n",
    "#     if device_data.empty:\n",
    "#         print(f\"未找到设备: {device_name}\")\n",
    "#         return\n",
    "#\n",
    "#     # 获取所有数值型参数列（排除非数值列）\n",
    "#     numeric_cols = device_data.select_dtypes(include=['number']).columns.tolist()\n",
    "#\n",
    "#     # 创建图表\n",
    "#     plt.figure(figsize=(20, 10))\n",
    "#\n",
    "#     # 为每个数值参数创建子图\n",
    "#     for i, col in enumerate(numeric_cols, 1):\n",
    "#         plt.subplot(len(numeric_cols), 1, i)\n",
    "#         sns.lineplot(x=time_col, y=col, data=device_data)\n",
    "#         plt.title(f\"{device_name} - {col} 随时间变化\")\n",
    "#         plt.xlabel('时间')\n",
    "#         plt.ylabel(col)\n",
    "#         plt.xticks(rotation=45)\n",
    "#\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "#\n",
    "# def analyze_devices(dataframe, subsystem_col='subsystem', time_col='created_at', sample_devices=None):\n",
    "#     \"\"\"\n",
    "#     分析所有设备的时间序列数据\n",
    "#\n",
    "#     参数:\n",
    "#     dataframe -- 包含设备数据的DataFrame\n",
    "#     subsystem_col -- 包含子系统/设备名称的列名\n",
    "#     time_col -- 包含时间戳的列名\n",
    "#     sample_devices -- 可选，指定要分析的设备列表（None表示分析所有设备）\n",
    "#     \"\"\"\n",
    "#      # 自动检测列名\n",
    "#     if subsystem_col is None:\n",
    "#         possible_names = ['subsystem', 'Subsystem', 'device', 'Device', 'device_name']\n",
    "#         for name in possible_names:\n",
    "#             if name in dataframe.columns:\n",
    "#                 subsystem_col = name\n",
    "#                 break\n",
    "#         if subsystem_col is None:\n",
    "#             raise ValueError(\"无法识别设备名称列，请手动指定 subsystem_col 参数\")\n",
    "#         print(f\"使用的设备名称列: {subsystem_col}\")\n",
    "#     dataframe[time_col] = pd.to_datetime(dataframe[time_col])\n",
    "#\n",
    "#     # 确保时间列是datetime类型\n",
    "#     dataframe[time_col] = pd.to_datetime(dataframe[time_col])\n",
    "#\n",
    "#     # 获取所有设备名称\n",
    "#     all_devices = dataframe[subsystem_col].unique()\n",
    "#\n",
    "#     # 如果指定了样本设备，只分析这些设备\n",
    "#     if sample_devices is not None:\n",
    "#         devices_to_analyze = [d for d in sample_devices if d in all_devices]\n",
    "#     else:\n",
    "#         devices_to_analyze = all_devices\n",
    "#\n",
    "#     print(f\"将分析 {len(devices_to_analyze)} 台设备的数据...\")\n",
    "#\n",
    "#     # 为每台设备绘制图表\n",
    "#     for device in devices_to_analyze:\n",
    "#         print(f\"\\n正在分析设备: {device}\")\n",
    "#         plot_device_parameters(dataframe, device, subsystem_col, time_col)\n",
    "#\n",
    "#         # 添加设备参数的统计摘要\n",
    "#         device_data = dataframe[dataframe[subsystem_col] == device]\n",
    "#         print(f\"\\n设备 {device} 的参数统计摘要:\")\n",
    "#         print(device_data.describe())\n",
    "#\n",
    "# # 使用示例:\n",
    "# # 1. 分析所有设备\n",
    "# analyze_devices(df)\n",
    "# # 选择第一个subsystem列\n",
    "# # df = df.rename(columns={df.columns[0]: 'subsystem'})\n",
    "# # df = df.loc[:, ~df.columns.duplicated()]  # 删除重复列\n",
    "#\n",
    "# # 2. 分析特定设备（例如NX16-L102）\n",
    "# analyze_devices(df, sample_devices=['NX16-L102'])\n",
    "#\n",
    "# # 3. 分析前5台设备\n",
    "# analyze_devices(df, sample_devices=df['subsystem'].unique()[:1])"
   ],
   "id": "4113893d1e822816",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1b6a439d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:10.281149Z",
     "iopub.status.busy": "2023-09-10T22:55:10.280766Z",
     "iopub.status.idle": "2023-09-10T22:55:12.939966Z",
     "shell.execute_reply": "2023-09-10T22:55:12.938822Z"
    },
    "papermill": {
     "duration": 2.697527,
     "end_time": "2023-09-10T22:55:12.943389",
     "exception": false,
     "start_time": "2023-09-10T22:55:10.245862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "for col in cat_cols:\n",
    "    cat_summary(df, col, plot=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c864351c",
   "metadata": {
    "papermill": {
     "duration": 0.042047,
     "end_time": "2023-09-10T22:55:13.026489",
     "exception": false,
     "start_time": "2023-09-10T22:55:12.984442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>6 |</span></b> <b>Analysis of Numerical Variables</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "2c39df43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:13.108801Z",
     "iopub.status.busy": "2023-09-10T22:55:13.108367Z",
     "iopub.status.idle": "2023-09-10T22:55:13.115426Z",
     "shell.execute_reply": "2023-09-10T22:55:13.114277Z"
    },
    "papermill": {
     "duration": 0.051128,
     "end_time": "2023-09-10T22:55:13.117780",
     "exception": false,
     "start_time": "2023-09-10T22:55:13.066652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def num_summary(dataframe, numerical_col, plot=False):\n",
    "    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n",
    "    print(dataframe[numerical_col].describe(quantiles).T)\n",
    "\n",
    "    if plot:\n",
    "        dataframe[numerical_col].hist(bins=20)\n",
    "        \n",
    "        plt.xlabel(numerical_col)\n",
    "        plt.title(numerical_col)\n",
    "        plt.show(block=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0b114fa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:13.200623Z",
     "iopub.status.busy": "2023-09-10T22:55:13.199845Z",
     "iopub.status.idle": "2023-09-10T22:55:17.811118Z",
     "shell.execute_reply": "2023-09-10T22:55:17.810246Z"
    },
    "papermill": {
     "duration": 4.655633,
     "end_time": "2023-09-10T22:55:17.813252",
     "exception": false,
     "start_time": "2023-09-10T22:55:13.157619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "for col in num_cols:\n",
    "    num_summary(df, col, plot=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2568f1b7",
   "metadata": {
    "papermill": {
     "duration": 0.051275,
     "end_time": "2023-09-10T22:55:17.915810",
     "exception": false,
     "start_time": "2023-09-10T22:55:17.864535",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>7 |</span></b> <b>Analysis of Categorical Variables by Target</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "d69de311",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:18.021282Z",
     "iopub.status.busy": "2023-09-10T22:55:18.020242Z",
     "iopub.status.idle": "2023-09-10T22:55:18.027638Z",
     "shell.execute_reply": "2023-09-10T22:55:18.026759Z"
    },
    "papermill": {
     "duration": 0.063262,
     "end_time": "2023-09-10T22:55:18.030053",
     "exception": false,
     "start_time": "2023-09-10T22:55:17.966791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def target_summary_with_cat(dataframe, target, categorical_col, plot=False):\n",
    "    print(pd.DataFrame({'TARGET_MEAN': dataframe.groupby(categorical_col)[target].mean()}), end='\\n\\n\\n')\n",
    "    if plot:\n",
    "        plt.figure(figsize=(20,8))\n",
    "        sns.barplot(x=categorical_col, y=target, data=dataframe)\n",
    "        plt.xticks(rotation=90)  \n",
    "        plt.show(block=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('D:/code/junma/600000/0424/2.csv')\n",
    "# df = pd.DataFrame(\"D:/code/junma/600000/0424/1.csv\")\n",
    "# for index, row in df.iterrows():\n",
    "#     for spindle in range(1, 101):\n",
    "#             # 构建目标变量 (y)\n",
    "#         is_break = 1 if row[f'D_dw{spindle}'] == 3 else 0\n",
    "#         return DataFrame\n",
    "# y = df['broken_status']\n",
    "# 读取数据\n",
    "# df = pd.read_csv('D:/code/junma/600000/0424/second_final.csv')\n",
    "#\n",
    "# # 向量化处理所有 D_dw 列\n",
    "# for spindle in range(1, 101):\n",
    "#     col_name = f'D_dw{spindle}'\n",
    "#     if col_name in df.columns:\n",
    "#         df[f'break_{spindle}'] = (df[col_name] == 3).astype(int)\n",
    "#\n",
    "# # 保存结果（可选）\n",
    "# df.to_csv('processed_data.csv', index=False)\n",
    "\n",
    "\n",
    "# # num_cols = [col for col in num_cols if col not in [\"M_dw5\"]]\n",
    "# import pandas as pd\n",
    "#\n",
    "# # 读取CSV文件\n",
    "# df = pd.read_csv('D:/code/junma/600000/0424/second_final.csv')\n",
    "#\n",
    "# # 为每个锭子(1-100)创建is_break列\n",
    "# for spindle in range(1, 101):\n",
    "#     df[f'is_break_dw{spindle}'] = df[f'D_dw{spindle}'].apply(lambda x: 1 if x == 3 else 0)\n",
    "#\n",
    "# # 现在df中新增了is_break_dw1到is_break_dw100列\n",
    "# print(df.head())\n",
    "#\n",
    "# # 如果你想要一个汇总的is_break列(所有锭子中是否有断纱)\n",
    "# df['any_break'] = df[[f'D_dw{i}' for i in range(1, 101)]].apply(lambda row: any(x == 3 for x in row), axis=1)\n",
    "df[f'is_break_dw{1}'] = df[f'D_dw{1}'].apply(lambda x: 1 if x == 3 else 0)\n",
    "# 检查 is_break_dw1 列中是否存在 1\n",
    "if 1 in df[f'is_break_dw{1}'].values:\n",
    "    print(\"is_break_dw1 列中存在值为 1 的元素。\")\n",
    "    # 展示值为 1 的元素的行索引\n",
    "    rows_with_one = df[df[f'is_break_dw{1}'] == 1].index.tolist()\n",
    "    print(f\"值为 1 的元素所在的行索引为: {rows_with_one}\")\n",
    "    # 展示值为 1 的元素所在的完整行\n",
    "    print(\"值为 1 的元素所在的完整行信息：\")\n",
    "    print(df[df[f'is_break_dw{1}'] == 1])\n",
    "else:\n",
    "    print(\"is_break_dw1 列中不存在值为 1 的元素。\")\n",
    "\n",
    "# 对 is_break_dw1 列中 0 和 1 的个数进行计数\n",
    "count_zeros = (df[f'is_break_dw{1}'] == 0).sum()\n",
    "count_ones = (df[f'is_break_dw{1}'] == 1).sum()\n",
    "\n",
    "print(f\"is_break_dw1 列中值为 0 的元素个数为: {count_zeros}\")\n",
    "print(f\"is_break_dw1 列中值为 1 的元素个数为: {count_ones}\")\n",
    "\n",
    "# 输出 is_break_dw1 列的值\n",
    "print(\"is_break_dw1 列的值为：\")\n",
    "print(df[f'is_break_dw{1}'].values)\n",
    "print(df[f'is_break_dw{1}'].values)"
   ],
   "id": "8edb5b3d2d28e0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c321888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:18.135323Z",
     "iopub.status.busy": "2023-09-10T22:55:18.134684Z",
     "iopub.status.idle": "2023-09-10T22:55:23.223827Z",
     "shell.execute_reply": "2023-09-10T22:55:23.222603Z"
    },
    "papermill": {
     "duration": 5.144604,
     "end_time": "2023-09-10T22:55:23.226301",
     "exception": false,
     "start_time": "2023-09-10T22:55:18.081697",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "for col in cat_cols:\n",
    "    target_summary_with_cat(df, 'is_break_dw1', col, plot=True)\n",
    "# # 数据预处理\n",
    "# # 1. 检查并预处理数据\n",
    "# print(\"broken_status唯一值:\", df['broken_status'].unique())\n",
    "# print(\"类别列缺失值统计:\")\n",
    "# print(df[cat_cols].isnull().sum())\n",
    "#\n",
    "# # 更新映射字典以包含所有可能的值\n",
    "# value_mapping = {'正常':0, '断纱':1, '完好':0, '断纱':1}  # 根据实际情况添加\n",
    "# df['broken_status'] = df['broken_status'].map(value_mapping).fillna(-1)  # 未映射的值设为-1\n",
    "#\n",
    "# # 2. 处理缺失值\n",
    "# df = df.dropna(subset=cat_cols + ['broken_status'])  # 删除关键列缺失的行\n",
    "#\n",
    "# # 3. 修改后的函数\n",
    "# def target_summary_with_cat(df, target_col, cat_col, plot=True):\n",
    "#     # 检查列是否存在\n",
    "#     if cat_col not in df.columns or target_col not in df.columns:\n",
    "#         print(f\"错误: 列 {cat_col} 或 {target_col} 不存在\")\n",
    "#         return None\n",
    "#\n",
    "#     # 执行分组\n",
    "#     result = df.groupby(cat_col)[target_col].mean()\n",
    "#\n",
    "#     # 检查结果是否为空\n",
    "#     if result.empty:\n",
    "#         print(f\"警告: {cat_col} 列分组结果为空\")\n",
    "#         return None\n",
    "#\n",
    "#     # 可视化\n",
    "#     if plot:\n",
    "#         result.plot(kind='bar')\n",
    "#         plt.title(f'{cat_col} vs {target_col}')\n",
    "#         plt.ylabel('断纱比例')\n",
    "#         plt.show()\n",
    "#\n",
    "#     return result\n",
    "#\n",
    "# # 4. 执行分析\n",
    "# for col in cat_cols:\n",
    "#     target_summary_with_cat(df, 'broken_status', col, plot=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9804e3af",
   "metadata": {
    "papermill": {
     "duration": 0.061815,
     "end_time": "2023-09-10T22:55:23.351025",
     "exception": false,
     "start_time": "2023-09-10T22:55:23.289210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>8 |</span></b> <b>Analysis of Numeric Variables by Target</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "488e72c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:23.477230Z",
     "iopub.status.busy": "2023-09-10T22:55:23.476202Z",
     "iopub.status.idle": "2023-09-10T22:55:23.482705Z",
     "shell.execute_reply": "2023-09-10T22:55:23.481753Z"
    },
    "papermill": {
     "duration": 0.071753,
     "end_time": "2023-09-10T22:55:23.484951",
     "exception": false,
     "start_time": "2023-09-10T22:55:23.413198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def target_summary_with_num(dataframe, target, numerical_col, plot=False):\n",
    "    print(pd.DataFrame({numerical_col+'_mean': dataframe.groupby(target)[numerical_col].mean()}), end='\\n\\n\\n')\n",
    "    if plot:\n",
    "        sns.barplot(x=target, y=numerical_col, data=dataframe)\n",
    "        plt.show(block=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "60b9fabe",
   "metadata": {
    "papermill": {
     "duration": 0.06187,
     "end_time": "2023-09-10T22:55:23.786859",
     "exception": false,
     "start_time": "2023-09-10T22:55:23.724989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>9 |</span></b> <b>Analysis of Correlation</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "2e0115be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:23.918853Z",
     "iopub.status.busy": "2023-09-10T22:55:23.918133Z",
     "iopub.status.idle": "2023-09-10T22:55:23.927091Z",
     "shell.execute_reply": "2023-09-10T22:55:23.925891Z"
    },
    "papermill": {
     "duration": 0.077336,
     "end_time": "2023-09-10T22:55:23.929594",
     "exception": false,
     "start_time": "2023-09-10T22:55:23.852258",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def high_correlated_cols(dataframe, plot=False, corr_th=0.70):\n",
    "    corr = dataframe.corr(numeric_only=True) \n",
    "    cor_matrix = corr.abs()\n",
    "    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))  # np.bool changed to bool\n",
    "    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n",
    "    if plot:\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        sns.set(rc={'figure.figsize': (16, 14)})\n",
    "        sns.heatmap(corr, cmap=\"RdBu\", annot=True, fmt=\".2f\")\n",
    "        plt.show()\n",
    "    return drop_list"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6c574299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:24.060092Z",
     "iopub.status.busy": "2023-09-10T22:55:24.059007Z",
     "iopub.status.idle": "2023-09-10T22:55:25.484353Z",
     "shell.execute_reply": "2023-09-10T22:55:25.483216Z"
    },
    "papermill": {
     "duration": 1.496641,
     "end_time": "2023-09-10T22:55:25.489095",
     "exception": false,
     "start_time": "2023-09-10T22:55:23.992454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "high_correlated_cols(df, plot=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "542f8823",
   "metadata": {
    "papermill": {
     "duration": 0.070591,
     "end_time": "2023-09-10T22:55:25.629245",
     "exception": false,
     "start_time": "2023-09-10T22:55:25.558654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>10 |</span></b> <b>Distribution of the Dependent Variable</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "4473c588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:25.771349Z",
     "iopub.status.busy": "2023-09-10T22:55:25.770603Z",
     "iopub.status.idle": "2023-09-10T22:55:26.375113Z",
     "shell.execute_reply": "2023-09-10T22:55:26.373605Z"
    },
    "papermill": {
     "duration": 0.678679,
     "end_time": "2023-09-10T22:55:26.377815",
     "exception": false,
     "start_time": "2023-09-10T22:55:25.699136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "df[\"D_dw1\"].hist(bins=100)\n",
    "plt.show(block=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ab272ec9",
   "metadata": {
    "papermill": {
     "duration": 0.066162,
     "end_time": "2023-09-10T22:55:26.514126",
     "exception": false,
     "start_time": "2023-09-10T22:55:26.447964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>11 |</span></b> <b>Examining the Logarithm of the Dependent Variable</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "dbe3bbaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:26.648478Z",
     "iopub.status.busy": "2023-09-10T22:55:26.648055Z",
     "iopub.status.idle": "2023-09-10T22:55:27.159045Z",
     "shell.execute_reply": "2023-09-10T22:55:27.157830Z"
    },
    "papermill": {
     "duration": 0.581813,
     "end_time": "2023-09-10T22:55:27.161585",
     "exception": false,
     "start_time": "2023-09-10T22:55:26.579772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# np.log1p(df['broken_status']).hist(bins=50)\n",
    "# plt.show(block=True)\n",
    "df['D_dw1'].value_counts().plot(kind='bar')  # 类别频数直方图\n",
    "plt.title('D_dw1 类别分布')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b92df953",
   "metadata": {
    "papermill": {
     "duration": 0.065131,
     "end_time": "2023-09-10T22:55:27.299605",
     "exception": false,
     "start_time": "2023-09-10T22:55:27.234474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>12 |</span></b> <b>Outliers Analysis</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "63e5e5dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:27.442177Z",
     "iopub.status.busy": "2023-09-10T22:55:27.441365Z",
     "iopub.status.idle": "2023-09-10T22:55:27.447721Z",
     "shell.execute_reply": "2023-09-10T22:55:27.446959Z"
    },
    "papermill": {
     "duration": 0.080539,
     "end_time": "2023-09-10T22:55:27.449987",
     "exception": false,
     "start_time": "2023-09-10T22:55:27.369448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n",
    "    quartile1 = dataframe[col_name].quantile(q1)\n",
    "    quartile3 = dataframe[col_name].quantile(q3)\n",
    "    interquantile_range = quartile3 - quartile1\n",
    "    up_limit = quartile3 + 1.5 * interquantile_range\n",
    "    low_limit = quartile1 - 1.5 * interquantile_range\n",
    "    return low_limit, up_limit"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "01ffce11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:27.594845Z",
     "iopub.status.busy": "2023-09-10T22:55:27.594417Z",
     "iopub.status.idle": "2023-09-10T22:55:27.600460Z",
     "shell.execute_reply": "2023-09-10T22:55:27.599688Z"
    },
    "papermill": {
     "duration": 0.080787,
     "end_time": "2023-09-10T22:55:27.602822",
     "exception": false,
     "start_time": "2023-09-10T22:55:27.522035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def check_outlier(dataframe, col_name):\n",
    "    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n",
    "    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5e1f7c13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:27.745918Z",
     "iopub.status.busy": "2023-09-10T22:55:27.745050Z",
     "iopub.status.idle": "2023-09-10T22:55:27.751504Z",
     "shell.execute_reply": "2023-09-10T22:55:27.750627Z"
    },
    "papermill": {
     "duration": 0.081415,
     "end_time": "2023-09-10T22:55:27.753955",
     "exception": false,
     "start_time": "2023-09-10T22:55:27.672540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def replace_with_thresholds(dataframe, variable):\n",
    "    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n",
    "    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n",
    "    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49b87384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:27.895628Z",
     "iopub.status.busy": "2023-09-10T22:55:27.895197Z",
     "iopub.status.idle": "2023-09-10T22:55:27.963425Z",
     "shell.execute_reply": "2023-09-10T22:55:27.962251Z"
    },
    "papermill": {
     "duration": 0.142406,
     "end_time": "2023-09-10T22:55:27.965956",
     "exception": false,
     "start_time": "2023-09-10T22:55:27.823550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "for col in num_cols:\n",
    "    print(col, check_outlier(df, col))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "48c51a6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:28.110619Z",
     "iopub.status.busy": "2023-09-10T22:55:28.109510Z",
     "iopub.status.idle": "2023-09-10T22:55:28.216506Z",
     "shell.execute_reply": "2023-09-10T22:55:28.215097Z"
    },
    "papermill": {
     "duration": 0.182057,
     "end_time": "2023-09-10T22:55:28.219355",
     "exception": false,
     "start_time": "2023-09-10T22:55:28.037298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "for col in num_cols:\n",
    "    if check_outlier(df, col):\n",
    "        replace_with_thresholds(df, col)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5df5a775",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:28.363582Z",
     "iopub.status.busy": "2023-09-10T22:55:28.363127Z",
     "iopub.status.idle": "2023-09-10T22:55:28.433391Z",
     "shell.execute_reply": "2023-09-10T22:55:28.431733Z"
    },
    "papermill": {
     "duration": 0.145858,
     "end_time": "2023-09-10T22:55:28.435883",
     "exception": false,
     "start_time": "2023-09-10T22:55:28.290025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "for col in num_cols:\n",
    "    print(col, check_outlier(df, col))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f4ca8d85",
   "metadata": {
    "papermill": {
     "duration": 0.070535,
     "end_time": "2023-09-10T22:55:28.578441",
     "exception": false,
     "start_time": "2023-09-10T22:55:28.507906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>13 |</span></b> <b>Missing Value Analysis</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d815a",
   "metadata": {
    "papermill": {
     "duration": 0.069469,
     "end_time": "2023-09-10T22:55:28.718229",
     "exception": false,
     "start_time": "2023-09-10T22:55:28.648760",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border: 2px solid #007BFF; padding: 20px; border-radius: 10px; background-color: #f9f9f9;\">\n",
    "    <h2 style=\"color: #007BFF;\">Missing Value Analysis</h2>\n",
    "    <p>I have handled the Feature Engineering processes in the <strong>GENERAL REVIEW OF BIOGAS IN U.S. FARMS</strong> section.</p>\n",
    "    <p>You can access it from the link below:</p>\n",
    "    <a href=\"https://www.kaggle.com/code/mehmetisik/general-review-of-biogas-in-u-s-farms\" style=\"background-color: #007BFF; color: #FFFFFF; padding: 10px 20px; text-decoration: none; border-radius: 5px;\">Click here to access</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "c4e7fd64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:28.861729Z",
     "iopub.status.busy": "2023-09-10T22:55:28.861272Z",
     "iopub.status.idle": "2023-09-10T22:55:28.869304Z",
     "shell.execute_reply": "2023-09-10T22:55:28.868145Z"
    },
    "papermill": {
     "duration": 0.083009,
     "end_time": "2023-09-10T22:55:28.871543",
     "exception": false,
     "start_time": "2023-09-10T22:55:28.788534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def missing_values_table(dataframe, na_name=False):\n",
    "    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n",
    "\n",
    "    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n",
    "\n",
    "    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n",
    "\n",
    "    print(missing_df, end=\"\\n\")\n",
    "\n",
    "    if na_name:\n",
    "        return na_columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a3dc9484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:29.014648Z",
     "iopub.status.busy": "2023-09-10T22:55:29.014198Z",
     "iopub.status.idle": "2023-09-10T22:55:29.029995Z",
     "shell.execute_reply": "2023-09-10T22:55:29.028653Z"
    },
    "papermill": {
     "duration": 0.090995,
     "end_time": "2023-09-10T22:55:29.032735",
     "exception": false,
     "start_time": "2023-09-10T22:55:28.941740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "missing_values_table(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7632c62",
   "metadata": {
    "papermill": {
     "duration": 0.070453,
     "end_time": "2023-09-10T22:55:29.175051",
     "exception": false,
     "start_time": "2023-09-10T22:55:29.104598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>14 |</span></b> <b>Rare Analysis</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "c696bc45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:29.318876Z",
     "iopub.status.busy": "2023-09-10T22:55:29.318419Z",
     "iopub.status.idle": "2023-09-10T22:55:29.326470Z",
     "shell.execute_reply": "2023-09-10T22:55:29.325054Z"
    },
    "papermill": {
     "duration": 0.082837,
     "end_time": "2023-09-10T22:55:29.328795",
     "exception": false,
     "start_time": "2023-09-10T22:55:29.245958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def rare_analyser(dataframe, target, cat_cols):\n",
    "    for col in cat_cols:\n",
    "        print(col, ':', len(dataframe[col].value_counts()))\n",
    "        print(pd.DataFrame({'COUNT': dataframe[col].value_counts(),\n",
    "                            'RATIO': dataframe[col].value_counts() / len(dataframe),\n",
    "                            'TARGET_MEAN': dataframe.groupby(col)[target].mean()}), end='\\n\\n\\n')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8863c027",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:29.476811Z",
     "iopub.status.busy": "2023-09-10T22:55:29.476396Z",
     "iopub.status.idle": "2023-09-10T22:55:29.525997Z",
     "shell.execute_reply": "2023-09-10T22:55:29.524511Z"
    },
    "papermill": {
     "duration": 0.127106,
     "end_time": "2023-09-10T22:55:29.529189",
     "exception": false,
     "start_time": "2023-09-10T22:55:29.402083",
     "status": "completed"
    },
    "tags": []
   },
   "source": "rare_analyser(df, \"D_dw1\", cat_cols)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb1c3031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:29.683332Z",
     "iopub.status.busy": "2023-09-10T22:55:29.682903Z",
     "iopub.status.idle": "2023-09-10T22:55:29.690777Z",
     "shell.execute_reply": "2023-09-10T22:55:29.689593Z"
    },
    "papermill": {
     "duration": 0.089507,
     "end_time": "2023-09-10T22:55:29.693086",
     "exception": false,
     "start_time": "2023-09-10T22:55:29.603579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def rare_encoder(dataframe, rare_perc):\n",
    "    temp_df = dataframe.copy()\n",
    "\n",
    "    rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'\n",
    "                    and (temp_df[col].value_counts() / len(temp_df) < rare_perc).any(axis=None)]\n",
    "\n",
    "    for var in rare_columns:\n",
    "        tmp = temp_df[var].value_counts() / len(temp_df)\n",
    "        rare_labels = tmp[tmp < rare_perc].index\n",
    "        temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n",
    "    return temp_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3077bd65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:29.843507Z",
     "iopub.status.busy": "2023-09-10T22:55:29.843086Z",
     "iopub.status.idle": "2023-09-10T22:55:29.899445Z",
     "shell.execute_reply": "2023-09-10T22:55:29.896929Z"
    },
    "papermill": {
     "duration": 0.137892,
     "end_time": "2023-09-10T22:55:29.902530",
     "exception": false,
     "start_time": "2023-09-10T22:55:29.764638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "rare_encoder(df, 0.01)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1cd7114e",
   "metadata": {
    "papermill": {
     "duration": 0.073315,
     "end_time": "2023-09-10T22:55:30.052974",
     "exception": false,
     "start_time": "2023-09-10T22:55:29.979659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>15 |</span></b> <b>Feature Extraction</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed0665f",
   "metadata": {
    "papermill": {
     "duration": 0.072006,
     "end_time": "2023-09-10T22:55:30.197859",
     "exception": false,
     "start_time": "2023-09-10T22:55:30.125853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border: 2px solid #007BFF; padding: 20px; border-radius: 10px; background-color: #f9f9f9;\">\n",
    "    <h2 style=\"color: #007BFF;\">Feature Engineering</h2>\n",
    "    <p>I have handled the Feature Engineering processes in the <strong>GENERAL REVIEW OF BIOGAS IN U.S. FARMS</strong> section.</p>\n",
    "    <p>You can access it from the link below:</p>\n",
    "    <a href=\"https://www.kaggle.com/code/mehmetisik/general-review-of-biogas-in-u-s-farms\" style=\"background-color: #007BFF; color: #FFFFFF; padding: 10px 20px; text-decoration: none; border-radius: 5px;\">Click here to access</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c2d18",
   "metadata": {
    "papermill": {
     "duration": 0.072852,
     "end_time": "2023-09-10T22:55:30.343800",
     "exception": false,
     "start_time": "2023-09-10T22:55:30.270948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>16 |</span></b> <b>Encoding</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "fbf3ddfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:30.491075Z",
     "iopub.status.busy": "2023-09-10T22:55:30.490668Z",
     "iopub.status.idle": "2023-09-10T22:55:30.505062Z",
     "shell.execute_reply": "2023-09-10T22:55:30.503822Z"
    },
    "papermill": {
     "duration": 0.091174,
     "end_time": "2023-09-10T22:55:30.507586",
     "exception": false,
     "start_time": "2023-09-10T22:55:30.416412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 定义要强制作为数值变量的列\n",
    "forced_caterical = (\n",
    "   # [f'M_dw{i}' for i in range(2, 7)] +  # M_dw2到M_dw6\n",
    "    [f'R_dw{i}' for i in range(22, 27)] +  # R_dw22到R_dw26\n",
    "    [f'R_dw{i}' for i in range(29, 34)]  # R_dw29到R_dw33\n",
    ")\n",
    "forced_numerical = (\n",
    "    [f'M_dw{i}' for i in range(2, 7)]   # M_dw2到M_dw6\n",
    "   # [f'R_dw{i}' for i in range(22, 27)] +  # R_dw22到R_dw26\n",
    "   # [f'R_dw{i}' for i in range(29, 34)]  # R_dw29到R_dw33\n",
    ")\n",
    "# 调用函数时传入强制参数\n",
    "cat_cols, num_cols, cat_but_car, num_but_cat = grab_col_names(\n",
    "    df,\n",
    "    force_cat_cols=forced_caterical,\n",
    "    force_num_cols=forced_numerical\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9de3043e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:30.659232Z",
     "iopub.status.busy": "2023-09-10T22:55:30.658467Z",
     "iopub.status.idle": "2023-09-10T22:55:30.665515Z",
     "shell.execute_reply": "2023-09-10T22:55:30.664295Z"
    },
    "papermill": {
     "duration": 0.084591,
     "end_time": "2023-09-10T22:55:30.667755",
     "exception": false,
     "start_time": "2023-09-10T22:55:30.583164",
     "status": "completed"
    },
    "tags": []
   },
   "source": "cat_cols\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23366fdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:30.814820Z",
     "iopub.status.busy": "2023-09-10T22:55:30.814060Z",
     "iopub.status.idle": "2023-09-10T22:55:30.818709Z",
     "shell.execute_reply": "2023-09-10T22:55:30.817845Z"
    },
    "papermill": {
     "duration": 0.080792,
     "end_time": "2023-09-10T22:55:30.820891",
     "exception": false,
     "start_time": "2023-09-10T22:55:30.740099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# cat_cols = ['Project Type',\n",
    "#  'Digester Type',\n",
    "#  'Status',\n",
    "#  'Animal/Farm Type(s)',\n",
    "#  'Co-Digestion',\n",
    "#  'Biogas End Use(s)',\n",
    "#  'LCFS Pathway?',\n",
    "#  'Receiving Utility',\n",
    "#  'Awarded USDA Funding?']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b50087c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:30.969080Z",
     "iopub.status.busy": "2023-09-10T22:55:30.968319Z",
     "iopub.status.idle": "2023-09-10T22:55:30.974278Z",
     "shell.execute_reply": "2023-09-10T22:55:30.973476Z"
    },
    "papermill": {
     "duration": 0.082452,
     "end_time": "2023-09-10T22:55:30.976612",
     "exception": false,
     "start_time": "2023-09-10T22:55:30.894160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "cat_cols"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "61cf0f6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:31.127648Z",
     "iopub.status.busy": "2023-09-10T22:55:31.126943Z",
     "iopub.status.idle": "2023-09-10T22:55:31.135674Z",
     "shell.execute_reply": "2023-09-10T22:55:31.134842Z"
    },
    "papermill": {
     "duration": 0.088585,
     "end_time": "2023-09-10T22:55:31.138857",
     "exception": false,
     "start_time": "2023-09-10T22:55:31.050272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "num_cols"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2fc96419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:31.295857Z",
     "iopub.status.busy": "2023-09-10T22:55:31.294839Z",
     "iopub.status.idle": "2023-09-10T22:55:31.301981Z",
     "shell.execute_reply": "2023-09-10T22:55:31.300645Z"
    },
    "papermill": {
     "duration": 0.085598,
     "end_time": "2023-09-10T22:55:31.304542",
     "exception": false,
     "start_time": "2023-09-10T22:55:31.218944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# num_cols = ['Year Operational',\n",
    "#  'Dairy',\n",
    "#  'Biogas Generation Estimate (cu-ft/day)',\n",
    "#  'Electricity Generated (kWh/yr)',\n",
    "#  'Total Emission Reductions (MTCO2e/yr)',\n",
    "#  'Operational Years',\n",
    "#  'Total_Animals',\n",
    "#  'Biogas_per_Animal (cu-ft/day)',\n",
    "#  'Emission_Reduction_per_Year',\n",
    "#  'Electricity_to_Biogas_Ratio',\n",
    "#  'Total_Waste_kg/day',\n",
    "#  'Waste_Efficiency',\n",
    "#  'Electricity_Efficiency', 'Cattle',\n",
    "#  'Poultry',\n",
    "#  'Swine']\n",
    "num_cols = [\n",
    "  'M_dw4',\n",
    " 'M_dw5',\n",
    " 'M_dw6'\n",
    "            ]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "05435289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:31.460862Z",
     "iopub.status.busy": "2023-09-10T22:55:31.459895Z",
     "iopub.status.idle": "2023-09-10T22:55:31.467703Z",
     "shell.execute_reply": "2023-09-10T22:55:31.466636Z"
    },
    "papermill": {
     "duration": 0.086563,
     "end_time": "2023-09-10T22:55:31.469976",
     "exception": false,
     "start_time": "2023-09-10T22:55:31.383413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "num_cols"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dde61b88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:31.634066Z",
     "iopub.status.busy": "2023-09-10T22:55:31.633090Z",
     "iopub.status.idle": "2023-09-10T22:55:31.640234Z",
     "shell.execute_reply": "2023-09-10T22:55:31.639213Z"
    },
    "papermill": {
     "duration": 0.088775,
     "end_time": "2023-09-10T22:55:31.642523",
     "exception": false,
     "start_time": "2023-09-10T22:55:31.553748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "cat_but_car"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4404816e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:31.793540Z",
     "iopub.status.busy": "2023-09-10T22:55:31.792837Z",
     "iopub.status.idle": "2023-09-10T22:55:31.800114Z",
     "shell.execute_reply": "2023-09-10T22:55:31.799186Z"
    },
    "papermill": {
     "duration": 0.086059,
     "end_time": "2023-09-10T22:55:31.802379",
     "exception": false,
     "start_time": "2023-09-10T22:55:31.716320",
     "status": "completed"
    },
    "tags": []
   },
   "source": "num_but_cat\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69f890e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:31.952916Z",
     "iopub.status.busy": "2023-09-10T22:55:31.952498Z",
     "iopub.status.idle": "2023-09-10T22:55:31.957457Z",
     "shell.execute_reply": "2023-09-10T22:55:31.956376Z"
    },
    "papermill": {
     "duration": 0.082196,
     "end_time": "2023-09-10T22:55:31.959694",
     "exception": false,
     "start_time": "2023-09-10T22:55:31.877498",
     "status": "completed"
    },
    "tags": []
   },
   "source": "# num_but_cat =[]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "848b5510",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:32.110976Z",
     "iopub.status.busy": "2023-09-10T22:55:32.110425Z",
     "iopub.status.idle": "2023-09-10T22:55:32.117794Z",
     "shell.execute_reply": "2023-09-10T22:55:32.116644Z"
    },
    "papermill": {
     "duration": 0.087369,
     "end_time": "2023-09-10T22:55:32.120205",
     "exception": false,
     "start_time": "2023-09-10T22:55:32.032836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "num_but_cat"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "449f4949",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:32.274378Z",
     "iopub.status.busy": "2023-09-10T22:55:32.273543Z",
     "iopub.status.idle": "2023-09-10T22:55:32.279062Z",
     "shell.execute_reply": "2023-09-10T22:55:32.278311Z"
    },
    "papermill": {
     "duration": 0.085591,
     "end_time": "2023-09-10T22:55:32.281322",
     "exception": false,
     "start_time": "2023-09-10T22:55:32.195731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n",
    "    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n",
    "    return dataframe\n",
    "\n",
    "# def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n",
    "#     # 创建原始列的副本\n",
    "#     original_cols = dataframe[categorical_cols].copy()\n",
    "#\n",
    "#     # 进行独热编码\n",
    "#     dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n",
    "#\n",
    "#     # 将原始列添加回数据框\n",
    "#     for col in categorical_cols:\n",
    "#         dataframe[col + 'broken_status'] = original_cols[col]\n",
    "#\n",
    "#     return dataframe\n",
    "# df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69e9f129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:32.432347Z",
     "iopub.status.busy": "2023-09-10T22:55:32.431203Z",
     "iopub.status.idle": "2023-09-10T22:55:32.448141Z",
     "shell.execute_reply": "2023-09-10T22:55:32.446965Z"
    },
    "papermill": {
     "duration": 0.095352,
     "end_time": "2023-09-10T22:55:32.450647",
     "exception": false,
     "start_time": "2023-09-10T22:55:32.355295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# df = one_hot_encoder(df, cat_cols, drop_first=True)\n",
    "df = one_hot_encoder(df, cat_cols,drop_first=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dd657aa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:32.599986Z",
     "iopub.status.busy": "2023-09-10T22:55:32.599146Z",
     "iopub.status.idle": "2023-09-10T22:55:32.636730Z",
     "shell.execute_reply": "2023-09-10T22:55:32.635609Z"
    },
    "papermill": {
     "duration": 0.115058,
     "end_time": "2023-09-10T22:55:32.639480",
     "exception": false,
     "start_time": "2023-09-10T22:55:32.524422",
     "status": "completed"
    },
    "tags": []
   },
   "source": "df.head(1000)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9ad16ae8",
   "metadata": {
    "papermill": {
     "duration": 0.07392,
     "end_time": "2023-09-10T22:55:32.788177",
     "exception": false,
     "start_time": "2023-09-10T22:55:32.714257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>17 |</span></b> <b>Standardization Process</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "d32c0992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:32.938729Z",
     "iopub.status.busy": "2023-09-10T22:55:32.937939Z",
     "iopub.status.idle": "2023-09-10T22:55:32.943437Z",
     "shell.execute_reply": "2023-09-10T22:55:32.942433Z"
    },
    "papermill": {
     "duration": 0.083762,
     "end_time": "2023-09-10T22:55:32.945890",
     "exception": false,
     "start_time": "2023-09-10T22:55:32.862128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# for spindle in range(1, 101):\n",
    "#             # 构建目标变量 (y)\n",
    "#             is_break = 1 if row[f'D_dw{spindle}'] == 3 else 0\n",
    "# y = is_break\n",
    "#\n",
    "# num_cols = [col for col in num_cols if col not in [\"is_break\"]]\n",
    "y =df['is_break_dw1_1']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56b3ddec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:33.096841Z",
     "iopub.status.busy": "2023-09-10T22:55:33.096401Z",
     "iopub.status.idle": "2023-09-10T22:55:33.100842Z",
     "shell.execute_reply": "2023-09-10T22:55:33.099842Z"
    },
    "papermill": {
     "duration": 0.082617,
     "end_time": "2023-09-10T22:55:33.103159",
     "exception": false,
     "start_time": "2023-09-10T22:55:33.020542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "scaler = RobustScaler()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "086ef4ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:33.253932Z",
     "iopub.status.busy": "2023-09-10T22:55:33.253110Z",
     "iopub.status.idle": "2023-09-10T22:55:33.270190Z",
     "shell.execute_reply": "2023-09-10T22:55:33.269014Z"
    },
    "papermill": {
     "duration": 0.095791,
     "end_time": "2023-09-10T22:55:33.273039",
     "exception": false,
     "start_time": "2023-09-10T22:55:33.177248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "df[num_cols] = scaler.fit_transform(df[num_cols])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d0ecc29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:33.424531Z",
     "iopub.status.busy": "2023-09-10T22:55:33.424138Z",
     "iopub.status.idle": "2023-09-10T22:55:33.474120Z",
     "shell.execute_reply": "2023-09-10T22:55:33.471650Z"
    },
    "papermill": {
     "duration": 0.129089,
     "end_time": "2023-09-10T22:55:33.476744",
     "exception": false,
     "start_time": "2023-09-10T22:55:33.347655",
     "status": "completed"
    },
    "tags": []
   },
   "source": "df.head(1000)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53c3c033",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:33.633083Z",
     "iopub.status.busy": "2023-09-10T22:55:33.632659Z",
     "iopub.status.idle": "2023-09-10T22:55:33.640457Z",
     "shell.execute_reply": "2023-09-10T22:55:33.639429Z"
    },
    "papermill": {
     "duration": 0.08841,
     "end_time": "2023-09-10T22:55:33.642643",
     "exception": false,
     "start_time": "2023-09-10T22:55:33.554233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Editing of variable names.\n",
    "# X = df.drop([\"M_dw5\",\"created_at\",\"name\",\"name1\",\"C_dw3\",\"C_dw4\",\"M_dw3\",\"M_dw2\",\"C_dw2\",\"time\",\"unnamed_0\",\"R_dw33\"], axis=1)\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "df.columns = df.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "df.columns = df.columns.str.lower()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a279bf49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:33.799726Z",
     "iopub.status.busy": "2023-09-10T22:55:33.799273Z",
     "iopub.status.idle": "2023-09-10T22:55:33.837999Z",
     "shell.execute_reply": "2023-09-10T22:55:33.836820Z"
    },
    "papermill": {
     "duration": 0.120995,
     "end_time": "2023-09-10T22:55:33.841308",
     "exception": false,
     "start_time": "2023-09-10T22:55:33.720313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "df.head(1000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "35a4ef9ab3a16b3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2c6603029c309ecf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "56f2a9269bc9094b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b00910cb",
   "metadata": {
    "papermill": {
     "duration": 0.07817,
     "end_time": "2023-09-10T22:55:33.998741",
     "exception": false,
     "start_time": "2023-09-10T22:55:33.920571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>18 |</span></b> <b>Creating Model</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "1d553a7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:34.160692Z",
     "iopub.status.busy": "2023-09-10T22:55:34.159823Z",
     "iopub.status.idle": "2023-09-10T22:55:34.164818Z",
     "shell.execute_reply": "2023-09-10T22:55:34.164066Z"
    },
    "papermill": {
     "duration": 0.087751,
     "end_time": "2023-09-10T22:55:34.167033",
     "exception": false,
     "start_time": "2023-09-10T22:55:34.079282",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# for spindle in range(1, 101):\n",
    "#             # 构建目标变量 (y)\n",
    "#             is_break = 1 if row[f'D_dw{spindle}'] == 3 else 0\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# y = df.m_dw5\n",
    "# y = is_break\n",
    "y =df['is_break_dw1_1']\n",
    "# df.shape"
   ],
   "id": "2dcc879cac703e46",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "22499520",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:34.324847Z",
     "iopub.status.busy": "2023-09-10T22:55:34.324040Z",
     "iopub.status.idle": "2023-09-10T22:55:34.330895Z",
     "shell.execute_reply": "2023-09-10T22:55:34.329730Z"
    },
    "papermill": {
     "duration": 0.088853,
     "end_time": "2023-09-10T22:55:34.333314",
     "exception": false,
     "start_time": "2023-09-10T22:55:34.244461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "X = df.drop([\"m_dw1_1\",\"m_dw1_2\",\"m_dw1_3\",\"m_dw6\",\"is_break_dw1_1\",\"d_dw1_1\",\"d_dw1_2\",\"d_dw1_3\",\"m_dw5\",\"created_at\",\"name\",\"subsystem1\",\"subsystem\",\"c_dw3\",\"c_dw4\",\"m_dw3\",\"m_dw2\",\"c_dw2\",\"unnamed_0\",\"r_dw33_28200\",\"r_dw33_27700\",\"r_dw33_28400\",\"r_dw33_32200\",\"r_dw33_32600\",\"r_dw33_33400\",\"r_dw33_38200\"], axis=1)\n",
    "# X = df.drop([\"is_break_dw1_1\",\"m_dw5\",\"created_at\",\"name\",\"subsystem1\",\"subsystem\",\"c_dw3\",\"c_dw4\",\"c_dw2\",\"unnamed_0\",\"r_dw33_28200\",\"r_dw33_27700\",\"r_dw33_28400\",\"r_dw33_32200\",\"r_dw33_32600\",\"r_dw33_33400\",\"r_dw33_38200\"], axis=1)\n",
    "# X = df.drop([\"r_dw33\",'device_name','spindle','broken_start_time','prev_time','next_time','prev_status','next_status'], axis=1)\n",
    "# # 1. 定义需要保留的列（R_dw和C_dw相关）\n",
    "# selected_features = [\n",
    "#     'r_dw22','r_dw22', 'r_dw23', 'r_dw24', 'r_dw25', 'r_dw26',\n",
    "#     'r_dw29', 'r_dw30', 'r_dw31', 'r_dw32', 'r_dw33', 'c_dw2', 'c_dw3', 'c_dw4'\n",
    "# ]\n",
    "#\n",
    "# # 2. 直接从原始数据框df中提取这些列（删除其他所有列）\n",
    "# X = df[selected_features].copy()  # 使用.copy()避免SettingWithCopyWarning\n",
    "#\n",
    "# # 3. 验证结果（可选）\n",
    "# print(\"保留的列：\", X.columns.tolist())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e6ecd885",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:55:34.491736Z",
     "iopub.status.busy": "2023-09-10T22:55:34.490856Z",
     "iopub.status.idle": "2023-09-10T22:55:34.499141Z",
     "shell.execute_reply": "2023-09-10T22:55:34.498019Z"
    },
    "papermill": {
     "duration": 0.090432,
     "end_time": "2023-09-10T22:55:34.501690",
     "exception": false,
     "start_time": "2023-09-10T22:55:34.411258",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=17)\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "#\n",
    "# # 确保时间列是datetime类型\n",
    "# df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "#\n",
    "# # 按时间排序\n",
    "# df = df.sort_values('created_at').reset_index(drop=True)\n",
    "#\n",
    "# # 计算总时间跨度\n",
    "# start_time = df['created_at'].min()\n",
    "# end_time = df['created_at'].max()\n",
    "# total_duration = end_time - start_time\n",
    "#\n",
    "# # 计算9.6小时的训练集截止时间点\n",
    "# train_duration = pd.Timedelta(hours=7.62)  # 9小时36分钟\n",
    "# cutoff_time = start_time + train_duration\n",
    "#\n",
    "# print(f\"数据总时间跨度: {total_duration}\")\n",
    "# print(f\"训练集截止时间: {cutoff_time}\")\n",
    "#\n",
    "# # 基于时间划分训练集和测试集\n",
    "# train_mask = df['created_at'] <= cutoff_time\n",
    "# test_mask = df['created_at'] > cutoff_time\n",
    "#\n",
    "# X_train = X[train_mask]\n",
    "# X_test = X[test_mask]\n",
    "# y_train = y[train_mask]\n",
    "# y_test = y[test_mask]\n",
    "\n",
    "print(f\"训练集大小: {len(X_train)}\")\n",
    "print(f\"测试集大小: {len(X_test)}\")\n",
    "print(f\"训练集比例: {len(X_train)/len(X):.2%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=17)\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # 确保时间列是datetime类型\n",
    "# df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "#\n",
    "# # 按时间排序\n",
    "# df = df.sort_values('created_at').reset_index(drop=True)\n",
    "#\n",
    "# # 计算总时间跨度\n",
    "# start_time = df['created_at'].min()\n",
    "# end_time = df['created_at'].max()\n",
    "# total_duration = end_time - start_time\n",
    "#\n",
    "# # 计算7.62小时的训练集截止时间点\n",
    "# train_duration = pd.Timedelta(hours=7.62)  # 7小时37分钟\n",
    "# cutoff_time = start_time + train_duration\n",
    "#\n",
    "# print(f\"数据总时间跨度: {total_duration}\")\n",
    "# print(f\"训练集截止时间: {cutoff_time}\")\n",
    "#\n",
    "# # 基于时间划分训练集和测试集\n",
    "# train_mask = df['created_at'] <= cutoff_time\n",
    "# test_mask = df['created_at'] > cutoff_time\n",
    "#\n",
    "# X_train = X[train_mask]\n",
    "# X_test = X[test_mask]\n",
    "# y_train = y[train_mask]\n",
    "# y_test = y[test_mask]\n",
    "#\n",
    "# print(f\"训练集大小: {len(X_train)}\")\n",
    "# print(f\"测试集大小: {len(X_test)}\")\n",
    "# print(f\"训练集比例: {len(X_train)/len(X):.2%}\")\n",
    "#\n",
    "# # 统计断纱事件（y==1）的数量和占比\n",
    "# print(\"\\n=== 断纱事件统计 ===\")\n",
    "\n",
    "# 训练集断纱事件统计\n",
    "# train_break_count = np.sum(y_train == 1)\n",
    "# train_break_ratio = train_break_count / len(y_train)\n",
    "#\n",
    "# # 测试集断纱事件统计\n",
    "# test_break_count = np.sum(y_test == 1)\n",
    "# test_break_ratio = test_break_count / len(y_test)\n",
    "#\n",
    "# # 总体断纱事件统计\n",
    "# total_break_count = np.sum(y == 1)\n",
    "# total_break_ratio = total_break_count / len(y)\n",
    "#\n",
    "# print(f\"训练集断纱事件数量: {train_break_count} (占比: {train_break_ratio:.2%})\")\n",
    "# print(f\"测试集断纱事件数量: {test_break_count} (占比: {test_break_ratio:.2%})\")\n",
    "# print(f\"总体断纱事件数量: {total_break_count} (占比: {total_break_ratio:.2%})\")\n",
    "#\n",
    "# # 检查类别分布是否均衡\n",
    "# print(\"\\n=== 类别分布分析 ===\")\n",
    "# print(f\"训练集类别分布 - 正常事件: {len(y_train) - train_break_count}, 断纱事件: {train_break_count}\")\n",
    "# print(f\"测试集类别分布 - 正常事件: {len(y_test) - test_break_count}, 断纱事件: {test_break_count}\")\n",
    "#\n",
    "# # 如果类别不平衡严重，给出警告\n",
    "# if train_break_ratio < 0.05 or train_break_ratio > 0.95:\n",
    "#     print(\"警告: 训练集类别不平衡严重，可能影响模型性能!\")\n",
    "# if test_break_ratio < 0.05 or test_break_ratio > 0.95:\n",
    "#     print(\"警告: 测试集类别不平衡严重，评估结果可能不可靠!\")\n",
    "#\n",
    "# # 计算断纱事件的时间分布\n",
    "# print(\"\\n=== 断纱事件时间分布 ===\")\n",
    "# break_events = df[y == 1] if 'y' in df.columns else df[df.index.isin(np.where(y == 1)[0])]\n",
    "# train_break_events = break_events[break_events['created_at'] <= cutoff_time]\n",
    "# test_break_events = break_events[break_events['created_at'] > cutoff_time]\n",
    "#\n",
    "# print(f\"训练集断纱事件时间范围: {train_break_events['created_at'].min()} 到 {train_break_events['created_at'].max()}\")\n",
    "# print(f\"测试集断纱事件时间范围: {test_break_events['created_at'].min()} 到 {test_break_events['created_at'].max()}\")\n",
    "\n",
    "# # 断纱事件在时间上的分布密度\n",
    "# if len(train_break_events) > 0:\n",
    "#     train_hours = (train_break_events['created_at'] - start_time).dt.total_seconds() / 3600\n",
    "#     print(f\"训练集断纱事件平均发生时间: {train_hours.mean():.2f} 小时后\")\n",
    "#\n",
    "# if len(test_break_events) > 0:\n",
    "#     test_hours = (test_break_events['created_at'] - cutoff_time).dt.total_seconds() / 3600\n",
    "#     print(f\"测试集断纱事件平均发生时间: {test_hours.mean():.2f} 小时后\")"
   ],
   "id": "9ac92c500c386a7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 导入必要的库\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# 模型列表\n",
    "models = [\n",
    "    # ('LR', LogisticRegression()),\n",
    "    # ('KNN', KNeighborsClassifier()),\n",
    "    # ('CART', DecisionTreeClassifier()),\n",
    "    ('RF', RandomForestClassifier()),\n",
    "    # ('GBM', GradientBoostingClassifier()),\n",
    "    ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n",
    "    ('LightGBM', LGBMClassifier()),\n",
    "    # ('CatBoost', CatBoostClassifier(verbose=False))\n",
    "]\n",
    "\n",
    "# 初始化空列表以存储性能指标和执行时间\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "execution_times = []\n",
    "model_names = []\n",
    "\n",
    "# 主循环：训练、预测和评估每个模型\n",
    "for name, classifier in models:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 训练模型\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # 预测\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    y_pred_proba = classifier.predict_proba(X_test)[:, 1]  # 用于 ROC-AUC\n",
    "\n",
    "    # 计算 Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "    # 计算 Precision\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    precision_scores.append(precision)\n",
    "\n",
    "    # 计算 Recall\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "    # 计算 F1-Score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # 计算 ROC-AUC\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "\n",
    "    # 计算模型执行时间\n",
    "    execution_time = time.time() - start_time\n",
    "    execution_times.append(execution_time)\n",
    "\n",
    "    # 存储模型名称\n",
    "    model_names.append(name)\n",
    "\n",
    "# 创建 DataFrame 以存储所有性能指标和执行时间\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1-Score': f1_scores,\n",
    "    'ROC-AUC': roc_auc_scores,\n",
    "    'Execution Time (s)': execution_times\n",
    "})\n",
    "\n",
    "# 显示结果\n",
    "results_df"
   ],
   "id": "e9dc96c7a4e0f951",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# 确保时间列是datetime类型\n",
    "# df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "#\n",
    "# # 按时间排序\n",
    "# df = df.sort_values('created_at').reset_index(drop=True)\n",
    "#\n",
    "# # 计算总时间跨度\n",
    "# start_time = df['created_at'].min()\n",
    "# end_time = df['created_at'].max()\n",
    "# total_duration = end_time - start_time\n",
    "#\n",
    "# # 计算7.62小时的训练集截止时间点\n",
    "# train_duration = pd.Timedelta(hours=7.62)  # 7小时37分钟\n",
    "# cutoff_time = start_time + train_duration\n",
    "#\n",
    "# print(f\"数据总时间跨度: {total_duration}\")\n",
    "# print(f\"训练集截止时间: {cutoff_time}\")\n",
    "#\n",
    "# # 基于时间划分训练集和测试集\n",
    "# train_mask = df['created_at'] <= cutoff_time\n",
    "# test_mask = df['created_at'] > cutoff_time\n",
    "#\n",
    "# X_train = X[train_mask]\n",
    "# X_test = X[test_mask]\n",
    "# y_train = y[train_mask]\n",
    "# y_test = y[test_mask]\n",
    "\n",
    "print(f\"训练集大小: {len(X_train)}\")\n",
    "print(f\"测试集大小: {len(X_test)}\")\n",
    "print(f\"训练集比例: {len(X_train)/len(X):.2%}\")\n",
    "\n",
    "# 统计断纱事件（y==1）的数量和占比\n",
    "print(\"\\n=== 断纱事件统计 ===\")\n",
    "\n",
    "# 训练集断纱事件统计\n",
    "train_break_count = np.sum(y_train == 1)\n",
    "train_break_ratio = train_break_count / len(y_train)\n",
    "\n",
    "# 测试集断纱事件统计\n",
    "test_break_count = np.sum(y_test == 1)\n",
    "test_break_ratio = test_break_count / len(y_test)\n",
    "\n",
    "# 总体断纱事件统计\n",
    "total_break_count = np.sum(y == 1)\n",
    "total_break_ratio = total_break_count / len(y)\n",
    "\n",
    "print(f\"训练集断纱事件数量: {train_break_count} (占比: {train_break_ratio:.2%})\")\n",
    "print(f\"测试集断纱事件数量: {test_break_count} (占比: {test_break_ratio:.2%})\")\n",
    "print(f\"总体断纱事件数量: {total_break_count} (占比: {total_break_ratio:.2%})\")\n",
    "\n",
    "# 检查类别分布是否均衡\n",
    "print(\"\\n=== 类别分布分析 ===\")\n",
    "print(f\"训练集类别分布 - 正常事件: {len(y_train) - train_break_count}, 断纱事件: {train_break_count}\")\n",
    "print(f\"测试集类别分布 - 正常事件: {len(y_test) - test_break_count}, 断纱事件: {test_break_count}\")\n",
    "\n",
    "# 如果类别不平衡严重，给出警告和处理建议\n",
    "if train_break_ratio < 0.05 or train_break_ratio > 0.95:\n",
    "    print(\"警告: 训练集类别不平衡严重，可能影响模型性能!\")\n",
    "    print(\"处理建议:\")\n",
    "    print(\"1. 使用类别权重 (class_weight='balanced')\")\n",
    "    print(\"2. 使用过采样技术 (如SMOTE)\")\n",
    "    print(\"3. 使用欠采样技术\")\n",
    "    print(\"4. 使用合适的评估指标 (如F1-score, ROC-AUC)\")\n",
    "\n",
    "if test_break_ratio < 0.05 or test_break_ratio > 0.95:\n",
    "    print(\"警告: 测试集类别不平衡严重，评估结果可能不可靠!\")\n",
    "    print(\"处理建议:\")\n",
    "    print(\"1. 使用分层抽样确保测试集代表性\")\n",
    "    print(\"2. 使用合适的评估指标 (如F1-score, ROC-AUC)\")\n",
    "\n",
    "# # 计算断纱事件的时间分布\n",
    "# print(\"\\n=== 断纱事件时间分布 ===\")\n",
    "# break_events = df[y == 1] if 'y' in df.columns else df[df.index.isin(np.where(y == 1)[0])]\n",
    "# train_break_events = break_events[break_events['created_at'] <= cutoff_time]\n",
    "# test_break_events = break_events[break_events['created_at'] > cutoff_time]\n",
    "#\n",
    "# if len(train_break_events) > 0:\n",
    "#     print(f\"训练集断纱事件时间范围: {train_break_events['created_at'].min()} 到 {train_break_events['created_at'].max()}\")\n",
    "#     train_hours = (train_break_events['created_at'] - start_time).dt.total_seconds() / 3600\n",
    "#     print(f\"训练集断纱事件平均发生时间: {train_hours.mean():.2f} 小时后\")\n",
    "#\n",
    "# if len(test_break_events) > 0:\n",
    "#     print(f\"测试集断纱事件时间范围: {test_break_events['created_at'].min()} 到 {test_break_events['created_at'].max()}\")\n",
    "#     test_hours = (test_break_events['created_at'] - cutoff_time).dt.total_seconds() / 3600\n",
    "#     print(f\"测试集断纱事件平均发生时间: {test_hours.mean():.2f} 小时后\")\n",
    "\n",
    "# 处理类别不平衡的解决方案\n",
    "print(\"\\n=== 类别不平衡处理方案 ===\")\n",
    "\n",
    "# 1. 计算类别权重\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "print(f\"类别权重: 正常事件={class_weights[0]:.2f}, 断纱事件={class_weights[1]:.2f}\")\n",
    "\n",
    "# 2. 显示过采样前后的数据分布\n",
    "print(f\"\\n过采样前训练集分布: 正常事件={len(y_train) - train_break_count}, 断纱事件={train_break_count}\")\n",
    "print(\"应用SMOTE过采样后，断纱事件将与正常事件数量相等\")\n",
    "\n",
    "# 3. 修改模型定义，加入类别权重和采样策略\n",
    "print(\"\\n=== 修改模型定义以处理类别不平衡 ===\")\n",
    "\n",
    "# 支持类别权重的模型\n",
    "models_balanced = [\n",
    "    ('RF_balanced', RandomForestClassifier(class_weight='balanced', random_state=17)),\n",
    "    ('XGBoost_balanced', XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight=(len(y_train) - train_break_count) / train_break_count  # 设置正例权重\n",
    "    )),\n",
    "    ('LightGBM_balanced', LGBMClassifier(\n",
    "        class_weight='balanced',\n",
    "        random_state=17\n",
    "    )),\n",
    "]\n",
    "\n",
    "# 创建采样器\n",
    "smote = SMOTE(random_state=17)\n",
    "under_sampler = RandomUnderSampler(random_state=17)\n",
    "\n",
    "# 使用采样器的模型管道\n",
    "models_sampled = [\n",
    "    ('RF_sampled', Pipeline([\n",
    "        ('smote', smote),\n",
    "        ('classifier', RandomForestClassifier(random_state=17))\n",
    "    ])),\n",
    "    ('XGBoost_sampled', Pipeline([\n",
    "        ('smote', smote),\n",
    "        ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "    ])),\n",
    "    ('LightGBM_sampled', Pipeline([\n",
    "        ('smote', smote),\n",
    "        ('classifier', LGBMClassifier(random_state=17))\n",
    "    ])),\n",
    "]\n",
    "\n",
    "print(\"已创建处理类别不平衡的模型:\")\n",
    "print(\"1. 使用类别权重的模型: RF_balanced, XGBoost_balanced, LightGBM_balanced\")\n",
    "print(\"2. 使用SMOTE过采样的模型: RF_sampled, XGBoost_sampled, LightGBM_sampled\")\n",
    "\n",
    "# 评估指标建议\n",
    "print(\"\\n=== 评估指标建议 ===\")\n",
    "print(\"对于不平衡数据，建议重点关注以下指标:\")\n",
    "print(\"1. F1-Score: 精确率和召回率的调和平均\")\n",
    "print(\"2. ROC-AUC: 不受类别分布影响\")\n",
    "print(\"3. Precision-Recall曲线下面积: 更适合不平衡数据\")\n",
    "print(\"4. 召回率(Recall): 确保尽可能多地识别断纱事件\")\n",
    "\n",
    "# 最终建议\n",
    "print(\"\\n=== 最终建议 ===\")\n",
    "print(\"1. 优先使用带类别权重的模型或SMOTE过采样模型\")\n",
    "print(\"2. 重点关注F1-Score和ROC-AUC指标\")\n",
    "print(\"3. 考虑使用Precision-Recall曲线评估模型性能\")\n",
    "print(\"4. 在实际应用中，可能需要根据业务需求调整分类阈值\")"
   ],
   "id": "50296b5ba4b532ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # 导入必要的库\n",
    "# import time\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn import metrics\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "#\n",
    "# # 模型列表\n",
    "# models = [\n",
    "#     # ('LR', LogisticRegression()),\n",
    "#     # ('KNN', KNeighborsClassifier()),\n",
    "#     # ('CART', DecisionTreeClassifier()),\n",
    "#     ('RF', RandomForestClassifier()),\n",
    "#     # ('GBM', GradientBoostingClassifier()),\n",
    "#     ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n",
    "#     ('LightGBM', LGBMClassifier()),\n",
    "#     # ('CatBoost', CatBoostClassifier(verbose=False))\n",
    "# ]\n",
    "#\n",
    "# # 初始化空列表以存储性能指标和执行时间\n",
    "# accuracy_scores = []\n",
    "# precision_scores = []\n",
    "# recall_scores = []\n",
    "# f1_scores = []\n",
    "# roc_auc_scores = []\n",
    "# execution_times = []\n",
    "# model_names = []\n",
    "#\n",
    "# # 主循环：训练、预测和评估每个模型（在训练集上评估）\n",
    "# for name, classifier in models:\n",
    "#     start_time = time.time()\n",
    "#\n",
    "#     # 训练模型\n",
    "#     classifier.fit(X_train, y_train)\n",
    "#\n",
    "#     # 在训练集上进行预测（关键修改）\n",
    "#     y_train_pred = classifier.predict(X_train)\n",
    "#     y_train_pred_proba = classifier.predict_proba(X_train)[:, 1]  # 用于 ROC-AUC\n",
    "#\n",
    "#     # 计算 Accuracy（在训练集上）\n",
    "#     accuracy = accuracy_score(y_train, y_train_pred)\n",
    "#     accuracy_scores.append(accuracy)\n",
    "#\n",
    "#     # 计算 Precision（在训练集上）\n",
    "#     precision = precision_score(y_train, y_train_pred)\n",
    "#     precision_scores.append(precision)\n",
    "#\n",
    "#     # 计算 Recall（在训练集上）\n",
    "#     recall = recall_score(y_train, y_train_pred)\n",
    "#     recall_scores.append(recall)\n",
    "#\n",
    "#     # 计算 F1-Score（在训练集上）\n",
    "#     f1 = f1_score(y_train, y_train_pred)\n",
    "#     f1_scores.append(f1)\n",
    "#\n",
    "#     # 计算 ROC-AUC（在训练集上）\n",
    "#     roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "#     roc_auc_scores.append(roc_auc)\n",
    "#\n",
    "#     # 计算模型执行时间\n",
    "#     execution_time = time.time() - start_time\n",
    "#     execution_times.append(execution_time)\n",
    "#\n",
    "#     # 存储模型名称\n",
    "#     model_names.append(name)\n",
    "#\n",
    "# # 创建 DataFrame 以存储所有性能指标和执行时间\n",
    "# results_df = pd.DataFrame({\n",
    "#     'Model': model_names,\n",
    "#     'Accuracy': accuracy_scores,\n",
    "#     'Precision': precision_scores,\n",
    "#     'Recall': recall_scores,\n",
    "#     'F1-Score': f1_scores,\n",
    "#     'ROC-AUC': roc_auc_scores,\n",
    "#     'Execution Time (s)': execution_times\n",
    "# })\n",
    "#\n",
    "# # 显示结果\n",
    "# print(\"训练集上的性能评估结果：\")\n",
    "# results_df"
   ],
   "id": "1b388b79ef55ed66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# 统计断纱事件（y==1）的数量和占比\n",
    "print(\"\\n=== 断纱事件统计 ===\")\n",
    "\n",
    "# 训练集断纱事件统计\n",
    "train_break_count = np.sum(y_train == 1)\n",
    "train_break_ratio = train_break_count / len(y_train)\n",
    "\n",
    "# 测试集断纱事件统计\n",
    "test_break_count = np.sum(y_test == 1)\n",
    "test_break_ratio = test_break_count / len(y_test)\n",
    "\n",
    "# 总体断纱事件统计\n",
    "total_break_count = np.sum(y == 1)\n",
    "total_break_ratio = total_break_count / len(y)\n",
    "\n",
    "print(f\"训练集断纱事件数量: {train_break_count} (占比: {train_break_ratio:.2%})\")\n",
    "print(f\"测试集断纱事件数量: {test_break_count} (占比: {test_break_ratio:.2%})\")\n",
    "print(f\"总体断纱事件数量: {total_break_count} (占比: {total_break_ratio:.2%})\")\n",
    "\n",
    "# 检查类别分布是否均衡\n",
    "print(\"\\n=== 类别分布分析 ===\")\n",
    "print(f\"训练集类别分布 - 正常事件: {len(y_train) - train_break_count}, 断纱事件: {train_break_count}\")\n",
    "print(f\"测试集类别分布 - 正常事件: {len(y_test) - test_break_count}, 断纱事件: {test_break_count}\")\n",
    "\n",
    "# 如果类别不平衡严重，给出警告和处理建议\n",
    "if train_break_ratio < 0.05 or train_break_ratio > 0.95:\n",
    "    print(\"警告: 训练集类别不平衡严重，可能影响模型性能!\")\n",
    "    print(\"处理建议:\")\n",
    "    print(\"1. 使用类别权重 (class_weight='balanced')\")\n",
    "    print(\"2. 使用过采样技术 (如SMOTE)\")\n",
    "    print(\"3. 使用欠采样技术\")\n",
    "    print(\"4. 使用合适的评估指标 (如F1-score, ROC-AUC)\")\n",
    "\n",
    "if test_break_ratio < 0.05 or test_break_ratio > 0.95:\n",
    "    print(\"警告: 测试集类别不平衡严重，评估结果可能不可靠!\")\n",
    "    print(\"处理建议:\")\n",
    "    print(\"1. 使用分层抽样确保测试集代表性\")\n",
    "    print(\"2. 使用合适的评估指标 (如F1-score, ROC-AUC)\")\n",
    "\n",
    "# 处理类别不平衡的解决方案\n",
    "print(\"\\n=== 类别不平衡处理方案 ===\")\n",
    "\n",
    "# 1. 计算类别权重\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "print(f\"类别权重: 正常事件={class_weights[0]:.2f}, 断纱事件={class_weights[1]:.2f}\")\n",
    "\n",
    "# 2. 显示过采样前后的数据分布\n",
    "print(f\"\\n过采样前训练集分布: 正常事件={len(y_train) - train_break_count}, 断纱事件={train_break_count}\")\n",
    "print(\"应用SMOTE过采样后，断纱事件将与正常事件数量相等\")\n",
    "\n",
    "# 3. 修改模型定义，加入类别权重和采样策略\n",
    "print(\"\\n=== 修改模型定义以处理类别不平衡 ===\")\n",
    "\n",
    "# 计算精确的类别权重字典\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "print(f\"使用的类别权重字典: {class_weight_dict}\")\n",
    "\n",
    "# 支持类别权重的模型 - 使用计算出的精确权重\n",
    "models_balanced = [\n",
    "    ('RF_balanced', RandomForestClassifier(\n",
    "        class_weight=class_weight_dict,  # 使用计算出的精确权重\n",
    "        random_state=17\n",
    "    )),\n",
    "    ('XGBoost_balanced', XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight=(len(y_train) - train_break_count) / train_break_count  # 设置正例权重\n",
    "    )),\n",
    "    ('LightGBM_balanced', LGBMClassifier(\n",
    "        class_weight=class_weight_dict,  # 使用计算出的精确权重\n",
    "        random_state=17\n",
    "    )),\n",
    "]\n",
    "\n",
    "# 创建采样器\n",
    "smote = SMOTE(random_state=17)\n",
    "under_sampler = RandomUnderSampler(random_state=17)\n",
    "\n",
    "# 使用采样器的模型管道\n",
    "models_sampled = [\n",
    "    ('RF_sampled', Pipeline([\n",
    "        ('smote', smote),\n",
    "        ('classifier', RandomForestClassifier(random_state=17))\n",
    "    ])),\n",
    "    ('XGBoost_sampled', Pipeline([\n",
    "        ('smote', smote),\n",
    "        ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "    ])),\n",
    "    ('LightGBM_sampled', Pipeline([\n",
    "        ('smote', smote),\n",
    "        ('classifier', LGBMClassifier(random_state=17))\n",
    "    ])),\n",
    "]\n",
    "\n",
    "print(\"已创建处理类别不平衡的模型:\")\n",
    "print(\"1. 使用类别权重的模型: RF_balanced, XGBoost_balanced, LightGBM_balanced\")\n",
    "print(\"2. 使用SMOTE过采样的模型: RF_sampled, XGBoost_sampled, LightGBM_sampled\")\n",
    "\n",
    "# 评估指标建议\n",
    "print(\"\\n=== 评估指标建议 ===\")\n",
    "print(\"对于不平衡数据，建议重点关注以下指标:\")\n",
    "print(\"1. F1-Score: 精确率和召回率的调和平均\")\n",
    "print(\"2. ROC-AUC: 不受类别分布影响\")\n",
    "print(\"3. Precision-Recall曲线下面积: 更适合不平衡数据\")\n",
    "print(\"4. 召回率(Recall): 确保尽可能多地识别断纱事件\")\n",
    "\n",
    "# 最终建议\n",
    "print(\"\\n=== 最终建议 ===\")\n",
    "print(\"1. 优先使用带类别权重的模型或SMOTE过采样模型\")\n",
    "print(\"2. 重点关注F1-Score和ROC-AUC指标\")\n",
    "print(\"3. 考虑使用Precision-Recall曲线评估模型性能\")\n",
    "print(\"4. 在实际应用中，可能需要根据业务需求调整分类阈值\")\n",
    "\n",
    "# 保存更新后的RF_balanced模型和相关信息\n",
    "print(\"\\n=== 保存更新后的模型和数据 ===\")\n",
    "\n",
    "# 提取RF_balanced模型\n",
    "rf_balanced_model = None\n",
    "for name, model in models_balanced:\n",
    "    if name == 'RF_balanced':\n",
    "        rf_balanced_model = model\n",
    "        break\n",
    "\n",
    "if rf_balanced_model is not None:\n",
    "    # 训练模型\n",
    "    print(\"训练RF_balanced模型...\")\n",
    "    rf_balanced_model.fit(X_train, y_train)\n",
    "    print(\"RF_balanced模型训练完成\")\n",
    "\n",
    "    # 创建模型信息字典\n",
    "    model_info = {\n",
    "        'model': rf_balanced_model,\n",
    "        'class_weights': class_weight_dict,\n",
    "        'train_break_count': train_break_count,\n",
    "        'train_break_ratio': train_break_ratio,\n",
    "        'test_break_count': test_break_count,\n",
    "        'test_break_ratio': test_break_ratio,\n",
    "        'total_break_count': total_break_count,\n",
    "        'total_break_ratio': total_break_ratio,\n",
    "        'train_size': len(X_train),\n",
    "        'test_size': len(X_test),\n",
    "        'feature_names': X_train.columns.tolist() if hasattr(X_train, 'columns') else [f'Feature_{i}' for i in range(X_train.shape[1])]\n",
    "    }\n",
    "\n",
    "    # 保存模型和信息\n",
    "    joblib.dump(model_info, 'rf_balanced_updated.pkl')\n",
    "    print(\"✅ RF_balanced模型和相关信息已保存至: rf_balanced_updated.pkl\")\n",
    "\n",
    "    # 保存类别分布信息到CSV\n",
    "    distribution_info = pd.DataFrame({\n",
    "        'Dataset': ['训练集', '测试集', '总体'],\n",
    "        '正常事件数量': [\n",
    "            len(y_train) - train_break_count,\n",
    "            len(y_test) - test_break_count,\n",
    "            len(y) - total_break_count\n",
    "        ],\n",
    "        '断纱事件数量': [train_break_count, test_break_count, total_break_count],\n",
    "        '断纱事件占比': [train_break_ratio, test_break_ratio, total_break_ratio]\n",
    "    })\n",
    "\n",
    "    distribution_info.to_csv('class_distribution_info.csv', index=False)\n",
    "    print(\"✅ 类别分布信息已保存至: class_distribution_info.csv\")\n",
    "\n",
    "    # 保存模型配置信息\n",
    "    config_info = {\n",
    "        'class_weight_dict': class_weight_dict,\n",
    "        'models_balanced_names': [name for name, _ in models_balanced],\n",
    "        'models_sampled_names': [name for name, _ in models_sampled],\n",
    "        'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "\n",
    "    config_df = pd.DataFrame([config_info])\n",
    "    config_df.to_csv('model_configuration.csv', index=False)\n",
    "    print(\"✅ 模型配置信息已保存至: model_configuration.csv\")\n",
    "\n",
    "    print(f\"\\n📊 模型详细信息:\")\n",
    "    print(f\"   类别权重: {class_weight_dict}\")\n",
    "    print(f\"   训练集大小: {len(X_train)}\")\n",
    "    print(f\"   测试集大小: {len(X_test)}\")\n",
    "    print(f\"   特征数量: {X_train.shape[1]}\")\n",
    "    print(f\"   RF_balanced模型参数: {rf_balanced_model.get_params()}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ 错误: 未找到RF_balanced模型\")\n",
    "\n",
    "print(\"\\n🎯 所有更新已完成！您现在可以调用保存的RF_balanced模型进行后续分析。\")\n",
    "\n",
    "# 提供后续调用示例\n",
    "print(\"\\n=== 后续调用示例 ===\")\n",
    "print(\"\"\"\n",
    "# 加载保存的模型和信息\n",
    "import joblib\n",
    "model_info = joblib.load('rf_balanced_updated.pkl')\n",
    "\n",
    "# 获取模型\n",
    "rf_balanced = model_info['model']\n",
    "\n",
    "# 获取类别权重\n",
    "class_weights = model_info['class_weights']\n",
    "\n",
    "# 进行预测\n",
    "y_pred = rf_balanced.predict(X_new)\n",
    "y_pred_proba = rf_balanced.predict_proba(X_new)\n",
    "\n",
    "print(\"模型加载成功，可以进行预测！\")\n",
    "\"\"\")"
   ],
   "id": "2bac904772bc77d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "\n",
    "print(\"=== 更新后RF_balanced模型性能评估 ===\")\n",
    "print(\"使用优化类别权重处理不平衡数据\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 初始化存储结果的列表\n",
    "metrics_results = []\n",
    "execution_times = []\n",
    "\n",
    "try:\n",
    "    # 加载之前保存的更新后RF_balanced模型\n",
    "    print(\"加载已保存的RF_balanced模型...\")\n",
    "    load_start_time = time.time()\n",
    "    model_info = joblib.load('rf_balanced_updated.pkl')\n",
    "    load_time = time.time() - load_start_time\n",
    "\n",
    "    # 获取模型和相关信息\n",
    "    rf_balanced_model = model_info['model']\n",
    "    class_weights = model_info['class_weights']\n",
    "    train_break_count = model_info['train_break_count']\n",
    "    train_break_ratio = model_info['train_break_ratio']\n",
    "\n",
    "    print(f\"模型加载完成，耗时: {load_time:.2f}秒\")\n",
    "    print(f\"使用的类别权重: {class_weights}\")\n",
    "    print(f\"训练集断纱事件: {train_break_count} (占比: {train_break_ratio:.2%})\")\n",
    "\n",
    "    # 由于模型已经训练过，直接进行预测\n",
    "    print(\"\\n开始模型预测...\")\n",
    "    predict_start_time = time.time()\n",
    "\n",
    "    # 在训练集和测试集上进行预测\n",
    "    y_train_pred = rf_balanced_model.predict(X_train)\n",
    "    y_test_pred = rf_balanced_model.predict(X_test)\n",
    "\n",
    "    # 获取概率预测\n",
    "    y_train_pred_proba = rf_balanced_model.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred_proba = rf_balanced_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    predict_time = time.time() - predict_start_time\n",
    "    print(f\"预测完成，耗时: {predict_time:.2f}秒\")\n",
    "\n",
    "    # 计算训练集指标\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred, zero_division=0)\n",
    "    train_recall = recall_score(y_train, y_train_pred, zero_division=0)\n",
    "    train_f1 = f1_score(y_train, y_train_pred, zero_division=0)\n",
    "    train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "\n",
    "    # 计算测试集指标\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "    # 计算混淆矩阵\n",
    "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    total_time = load_time + predict_time\n",
    "\n",
    "    # 存储结果\n",
    "    metrics_results.append({\n",
    "        'Dataset': '训练集',\n",
    "        'Accuracy': train_accuracy,\n",
    "        'Precision': train_precision,\n",
    "        'Recall': train_recall,\n",
    "        'F1-Score': train_f1,\n",
    "        'ROC-AUC': train_roc_auc\n",
    "    })\n",
    "\n",
    "    metrics_results.append({\n",
    "        'Dataset': '测试集',\n",
    "        'Accuracy': test_accuracy,\n",
    "        'Precision': test_precision,\n",
    "        'Recall': test_recall,\n",
    "        'F1-Score': test_f1,\n",
    "        'ROC-AUC': test_roc_auc\n",
    "    })\n",
    "\n",
    "    execution_times.append({\n",
    "        '模型加载时间': load_time,\n",
    "        '预测时间': predict_time,\n",
    "        '总时间': total_time\n",
    "    })\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"\\n更新后RF_balanced模型评估结果:\")\n",
    "    print(\"训练集性能:\")\n",
    "    print(f\"  准确率: {train_accuracy:.4f}\")\n",
    "    print(f\"  精确率: {train_precision:.4f}\")\n",
    "    print(f\"  召回率: {train_recall:.4f}\")\n",
    "    print(f\"  F1分数: {train_f1:.4f}\")\n",
    "    print(f\"  ROC-AUC: {train_roc_auc:.4f}\")\n",
    "\n",
    "    print(\"\\n测试集性能:\")\n",
    "    print(f\"  准确率: {test_accuracy:.4f}\")\n",
    "    print(f\"  精确率: {test_precision:.4f}\")\n",
    "    print(f\"  召回率: {test_recall:.4f}\")\n",
    "    print(f\"  F1分数: {test_f1:.4f}\")\n",
    "    print(f\"  ROC-AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "    print(\"\\n混淆矩阵:\")\n",
    "    print(\"训练集混淆矩阵:\")\n",
    "    print(cm_train)\n",
    "    print(\"测试集混淆矩阵:\")\n",
    "    print(cm_test)\n",
    "\n",
    "    print(\"\\n详细分类报告 - 测试集:\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=['正常', '断纱']))\n",
    "\n",
    "    print(f\"\\n时间统计:\")\n",
    "    print(f\"  模型加载时间: {load_time:.2f}秒\")\n",
    "    print(f\"  预测时间: {predict_time:.2f}秒\")\n",
    "    print(f\"  总执行时间: {total_time:.2f}秒\")\n",
    "\n",
    "    # 创建结果DataFrame\n",
    "    results_df = pd.DataFrame(metrics_results)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"更新后RF_balanced模型性能总结\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results_df.round(4))\n",
    "\n",
    "    # 创建时间统计DataFrame\n",
    "    time_df = pd.DataFrame(execution_times)\n",
    "    print(\"\\n时间统计:\")\n",
    "    print(time_df.round(2))\n",
    "\n",
    "    # 保存结果到CSV文件\n",
    "    results_df.to_csv('RF_balanced_updated_performance.csv', index=False)\n",
    "    time_df.to_csv('RF_balanced_updated_timing.csv', index=False)\n",
    "    print(f\"\\n结果已保存至: RF_balanced_updated_performance.csv 和 RF_balanced_updated_timing.csv\")\n",
    "\n",
    "    # 特征重要性分析\n",
    "    print(\"\\n=== 特征重要性分析 ===\")\n",
    "    feature_importances = rf_balanced_model.feature_importances_\n",
    "    feature_names = model_info['feature_names']\n",
    "\n",
    "    # 创建特征重要性DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    print(\"前10个最重要的特征:\")\n",
    "    print(importance_df.head(10).round(4))\n",
    "\n",
    "    # 保存特征重要性\n",
    "    importance_df.to_csv('RF_balanced_feature_importance.csv', index=False)\n",
    "    print(f\"特征重要性已保存至: RF_balanced_feature_importance.csv\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"错误: 未找到保存的RF_balanced模型文件 'rf_balanced_updated.pkl'\")\n",
    "    print(\"请先运行模型创建和保存代码\")\n",
    "except Exception as e:\n",
    "    print(f\"模型加载或评估过程中出错: {e}\")\n",
    "\n",
    "print(\"\\n评估完成！\")"
   ],
   "id": "67e9a7fa219023d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "\n",
    "# 设置绘图样式 - 英文专业格式\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.linewidth': 2.5,\n",
    "    'font.size': 16,\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 17,\n",
    "    'xtick.labelsize': 15,\n",
    "    'ytick.labelsize': 15,\n",
    "    'legend.fontsize': 15,\n",
    "    'grid.linewidth': 1.2,\n",
    "    'lines.linewidth': 3.5,\n",
    "    'figure.dpi': 1200,\n",
    "    'savefig.dpi': 1200,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.unicode_minus': False\n",
    "})\n",
    "\n",
    "print(\"=== Updated RF_balanced Model Performance Evaluation ===\")\n",
    "print(\"Using Optimized Class Weights for Imbalanced Data\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize lists to store results\n",
    "metrics_results = []\n",
    "execution_times = []\n",
    "\n",
    "try:\n",
    "    # Load the previously saved updated RF_balanced model\n",
    "    print(\"Loading saved RF_balanced model...\")\n",
    "    load_start_time = time.time()\n",
    "    model_info = joblib.load('rf_balanced_updated.pkl')\n",
    "    load_time = time.time() - load_start_time\n",
    "\n",
    "    # Get model and related information\n",
    "    rf_balanced_model = model_info['model']\n",
    "    class_weights = model_info['class_weights']\n",
    "    train_break_count = model_info['train_break_count']\n",
    "    train_break_ratio = model_info['train_break_ratio']\n",
    "\n",
    "    print(f\"Model loaded successfully, time: {load_time:.2f} seconds\")\n",
    "    print(f\"Class weights used: {class_weights}\")\n",
    "    print(f\"Training set yarn break events: {train_break_count} (ratio: {train_break_ratio:.2%})\")\n",
    "\n",
    "    # Since the model is already trained, proceed directly to prediction\n",
    "    print(\"\\nStarting model prediction...\")\n",
    "    predict_start_time = time.time()\n",
    "\n",
    "    # Make predictions on training and test sets\n",
    "    y_train_pred = rf_balanced_model.predict(X_train)\n",
    "    y_test_pred = rf_balanced_model.predict(X_test)\n",
    "\n",
    "    # Get probability predictions\n",
    "    y_train_pred_proba = rf_balanced_model.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred_proba = rf_balanced_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    predict_time = time.time() - predict_start_time\n",
    "    print(f\"Prediction completed, time: {predict_time:.2f} seconds\")\n",
    "\n",
    "    # Calculate training set metrics\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred, zero_division=0)\n",
    "    train_recall = recall_score(y_train, y_train_pred, zero_division=0)\n",
    "    train_f1 = f1_score(y_train, y_train_pred, zero_division=0)\n",
    "    train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "\n",
    "    # Calculate test set metrics\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "    # Calculate confusion matrices\n",
    "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    total_time = load_time + predict_time\n",
    "\n",
    "    # 6. Visualize confusion matrices with enhanced styling\n",
    "    print(\"\\n=== Generating Confusion Matrix Visualizations ===\")\n",
    "\n",
    "    # Create training set confusion matrix heatmap with enhanced styling\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Yarn Break'],\n",
    "                yticklabels=['Normal', 'Yarn Break'],\n",
    "                annot_kws={'size': 16, 'weight': 'bold'},  # Enlarge and bold annotation font\n",
    "                linewidths=2, linecolor='black')  # Bold cell borders\n",
    "\n",
    "    plt.title('Training Set Confusion Matrix - RF_balanced Model',\n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Bold the entire plot border\n",
    "    ax = plt.gca()\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('RF_balanced_training_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Training set confusion matrix saved to: RF_balanced_training_confusion_matrix.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Create test set confusion matrix heatmap with enhanced styling\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Yarn Break'],\n",
    "                yticklabels=['Normal', 'Yarn Break'],\n",
    "                annot_kws={'size': 16, 'weight': 'bold'},  # Enlarge and bold annotation font\n",
    "                linewidths=2, linecolor='black')  # Bold cell borders\n",
    "\n",
    "    plt.title('Test Set Confusion Matrix - RF_balanced Model',\n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Bold the entire plot border\n",
    "    ax = plt.gca()\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('RF_balanced_test_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Test set confusion matrix saved to: RF_balanced_test_confusion_matrix.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Store results\n",
    "    metrics_results.append({\n",
    "        'Dataset': 'Training Set',\n",
    "        'Accuracy': train_accuracy,\n",
    "        'Precision': train_precision,\n",
    "        'Recall': train_recall,\n",
    "        'F1-Score': train_f1,\n",
    "        'ROC-AUC': train_roc_auc\n",
    "    })\n",
    "\n",
    "    metrics_results.append({\n",
    "        'Dataset': 'Test Set',\n",
    "        'Accuracy': test_accuracy,\n",
    "        'Precision': test_precision,\n",
    "        'Recall': test_recall,\n",
    "        'F1-Score': test_f1,\n",
    "        'ROC-AUC': test_roc_auc\n",
    "    })\n",
    "\n",
    "    execution_times.append({\n",
    "        'Model Loading Time': load_time,\n",
    "        'Prediction Time': predict_time,\n",
    "        'Total Time': total_time\n",
    "    })\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nUpdated RF_balanced Model Evaluation Results:\")\n",
    "    print(\"Training Set Performance:\")\n",
    "    print(f\"  Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {train_precision:.4f}\")\n",
    "    print(f\"  Recall: {train_recall:.4f}\")\n",
    "    print(f\"  F1-Score: {train_f1:.4f}\")\n",
    "    print(f\"  ROC-AUC: {train_roc_auc:.4f}\")\n",
    "\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(f\"  Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {test_precision:.4f}\")\n",
    "    print(f\"  Recall: {test_recall:.4f}\")\n",
    "    print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "    print(f\"  ROC-AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrices:\")\n",
    "    print(\"Training Set Confusion Matrix:\")\n",
    "    print(cm_train)\n",
    "    print(\"Test Set Confusion Matrix:\")\n",
    "    print(cm_test)\n",
    "\n",
    "    print(\"\\nDetailed Classification Report - Test Set:\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=['Normal', 'Yarn Break']))\n",
    "\n",
    "    print(f\"\\nTime Statistics:\")\n",
    "    print(f\"  Model Loading Time: {load_time:.2f} seconds\")\n",
    "    print(f\"  Prediction Time: {predict_time:.2f} seconds\")\n",
    "    print(f\"  Total Execution Time: {total_time:.2f} seconds\")\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(metrics_results)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Updated RF_balanced Model Performance Summary\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results_df.round(4))\n",
    "\n",
    "    # Create time statistics DataFrame\n",
    "    time_df = pd.DataFrame(execution_times)\n",
    "    print(\"\\nTime Statistics:\")\n",
    "    print(time_df.round(2))\n",
    "\n",
    "    # Save results to CSV files\n",
    "    results_df.to_csv('RF_balanced_updated_performance.csv', index=False)\n",
    "    time_df.to_csv('RF_balanced_updated_timing.csv', index=False)\n",
    "    print(f\"\\nResults saved to: RF_balanced_updated_performance.csv and RF_balanced_updated_timing.csv\")\n",
    "\n",
    "    # Feature importance analysis\n",
    "    print(\"\\n=== Feature Importance Analysis ===\")\n",
    "    feature_importances = rf_balanced_model.feature_importances_\n",
    "    feature_names = model_info['feature_names']\n",
    "\n",
    "    # Create feature importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(importance_df.head(10).round(4))\n",
    "\n",
    "    # Save feature importance\n",
    "    importance_df.to_csv('RF_balanced_feature_importance.csv', index=False)\n",
    "    print(f\"Feature importance saved to: RF_balanced_feature_importance.csv\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Saved RF_balanced model file 'rf_balanced_updated.pkl' not found\")\n",
    "    print(\"Please run the model creation and saving code first\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model loading or evaluation: {e}\")\n",
    "\n",
    "print(\"\\nEvaluation completed!\")"
   ],
   "id": "d97a285ff92b5b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "\n",
    "# 设置绘图样式 - 英文专业格式\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.linewidth': 2.5,\n",
    "    'font.size': 16,\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 17,\n",
    "    'xtick.labelsize': 15,\n",
    "    'ytick.labelsize': 15,\n",
    "    'legend.fontsize': 15,\n",
    "    'grid.linewidth': 1.2,\n",
    "    'lines.linewidth': 3.5,\n",
    "    'figure.dpi': 1200,\n",
    "    'savefig.dpi': 1200,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.unicode_minus': False\n",
    "})\n",
    "\n",
    "print(\"=== Updated RF_balanced Model Performance Evaluation ===\")\n",
    "print(\"Using Optimized Class Weights for Imbalanced Data\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize lists to store results\n",
    "metrics_results = []\n",
    "execution_times = []\n",
    "\n",
    "try:\n",
    "    # Load the previously saved updated RF_balanced model\n",
    "    print(\"Loading saved RF_balanced model...\")\n",
    "    load_start_time = time.time()\n",
    "    model_info = joblib.load('rf_balanced_updated.pkl')\n",
    "    load_time = time.time() - load_start_time\n",
    "\n",
    "    # Get model and related information\n",
    "    rf_balanced_model = model_info['model']\n",
    "    class_weights = model_info['class_weights']\n",
    "    train_break_count = model_info['train_break_count']\n",
    "    train_break_ratio = model_info['train_break_ratio']\n",
    "\n",
    "    print(f\"Model loaded successfully, time: {load_time:.2f} seconds\")\n",
    "    print(f\"Class weights used: {class_weights}\")\n",
    "    print(f\"Training set yarn break events: {train_break_count} (ratio: {train_break_ratio:.2%})\")\n",
    "\n",
    "    # Since the model is already trained, proceed directly to prediction\n",
    "    print(\"\\nStarting model prediction...\")\n",
    "    predict_start_time = time.time()\n",
    "\n",
    "    # Make predictions on training and test sets\n",
    "    y_train_pred = rf_balanced_model.predict(X_train)\n",
    "    y_test_pred = rf_balanced_model.predict(X_test)\n",
    "\n",
    "    # Get probability predictions\n",
    "    y_train_pred_proba = rf_balanced_model.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred_proba = rf_balanced_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    predict_time = time.time() - predict_start_time\n",
    "    print(f\"Prediction completed, time: {predict_time:.2f} seconds\")\n",
    "\n",
    "    # Calculate training set metrics\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred, zero_division=0)\n",
    "    train_recall = recall_score(y_train, y_train_pred, zero_division=0)\n",
    "    train_f1 = f1_score(y_train, y_train_pred, zero_division=0)\n",
    "    train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "\n",
    "    # Calculate test set metrics\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "    # Calculate confusion matrices\n",
    "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    total_time = load_time + predict_time\n",
    "\n",
    "    # Store results\n",
    "    metrics_results.append({\n",
    "        'Dataset': 'Training Set',\n",
    "        'Accuracy': train_accuracy,\n",
    "        'Precision': train_precision,\n",
    "        'Recall': train_recall,\n",
    "        'F1-Score': train_f1,\n",
    "        'ROC-AUC': train_roc_auc\n",
    "    })\n",
    "\n",
    "    metrics_results.append({\n",
    "        'Dataset': 'Test Set',\n",
    "        'Accuracy': test_accuracy,\n",
    "        'Precision': test_precision,\n",
    "        'Recall': test_recall,\n",
    "        'F1-Score': test_f1,\n",
    "        'ROC-AUC': test_roc_auc\n",
    "    })\n",
    "\n",
    "    execution_times.append({\n",
    "        'Model Loading Time': load_time,\n",
    "        'Prediction Time': predict_time,\n",
    "        'Total Time': total_time\n",
    "    })\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nUpdated RF_balanced Model Evaluation Results:\")\n",
    "    print(\"Training Set Performance:\")\n",
    "    print(f\"  Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {train_precision:.4f}\")\n",
    "    print(f\"  Recall: {train_recall:.4f}\")\n",
    "    print(f\"  F1-Score: {train_f1:.4f}\")\n",
    "    print(f\"  ROC-AUC: {train_roc_auc:.4f}\")\n",
    "\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(f\"  Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {test_precision:.4f}\")\n",
    "    print(f\"  Recall: {test_recall:.4f}\")\n",
    "    print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "    print(f\"  ROC-AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrices:\")\n",
    "    print(\"Training Set Confusion Matrix:\")\n",
    "    print(cm_train)\n",
    "    print(\"Test Set Confusion Matrix:\")\n",
    "    print(cm_test)\n",
    "\n",
    "    print(\"\\nDetailed Classification Report - Test Set:\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=['Normal', 'Yarn Break']))\n",
    "\n",
    "    print(f\"\\nTime Statistics:\")\n",
    "    print(f\"  Model Loading Time: {load_time:.2f} seconds\")\n",
    "    print(f\"  Prediction Time: {predict_time:.2f} seconds\")\n",
    "    print(f\"  Total Execution Time: {total_time:.2f} seconds\")\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(metrics_results)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Updated RF_balanced Model Performance Summary\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results_df.round(4))\n",
    "\n",
    "    # Create time statistics DataFrame\n",
    "    time_df = pd.DataFrame(execution_times)\n",
    "    print(\"\\nTime Statistics:\")\n",
    "    print(time_df.round(2))\n",
    "\n",
    "    # Save results to CSV files\n",
    "    results_df.to_csv('RF_balanced_updated_performance.csv', index=False)\n",
    "    time_df.to_csv('RF_balanced_updated_timing.csv', index=False)\n",
    "    print(f\"\\nResults saved to: RF_balanced_updated_performance.csv and RF_balanced_updated_timing.csv\")\n",
    "\n",
    "    # 6. Visualize confusion matrices with enhanced styling - 按照参考代码格式修改\n",
    "    print(\"\\n=== Generating Confusion Matrix Visualizations ===\")\n",
    "\n",
    "    # Visualization - Confusion Matrix in one figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('RF_balanced Model Performance - Confusion Matrices', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Training set confusion matrix with enhanced styling\n",
    "    sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "                annot_kws={'size': 16, 'weight': 'bold'},  # Enlarge and bold annotation font\n",
    "                linewidths=2, linecolor='black',  # Bold cell borders\n",
    "                cbar_kws={'shrink': 0.8})\n",
    "    axes[0].set_title('Training Set - RF_balanced Model', fontsize=16, fontweight='bold', pad=20)\n",
    "    axes[0].set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "    axes[0].tick_params(axis='both', which='major', labelsize=13)\n",
    "\n",
    "    # Bold the entire plot border\n",
    "    for spine in axes[0].spines.values():\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "    # Test set confusion matrix with enhanced styling\n",
    "    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "                annot_kws={'size': 16, 'weight': 'bold'},  # Enlarge and bold annotation font\n",
    "                linewidths=2, linecolor='black',  # Bold cell borders\n",
    "                cbar_kws={'shrink': 0.8})\n",
    "    axes[1].set_title('Test Set - RF_balanced Model', fontsize=16, fontweight='bold', pad=20)\n",
    "    axes[1].set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "    axes[1].tick_params(axis='both', which='major', labelsize=13)\n",
    "\n",
    "    # Bold the entire plot border\n",
    "    for spine in axes[1].spines.values():\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.savefig('RF_balanced_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Confusion matrices saved to: RF_balanced_confusion_matrices.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Probability distribution visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    break_proba = y_test_pred_proba[y_test == 1]\n",
    "    normal_proba = y_test_pred_proba[y_test == 0]\n",
    "\n",
    "    plt.hist(normal_proba, bins=50, alpha=0.7, label='Normal Events', color='green', edgecolor='black')\n",
    "    plt.hist(break_proba, bins=20, alpha=0.7, label='Yarn Break Events', color='red', edgecolor='black')\n",
    "    plt.axvline(x=0.5, color='orange', linestyle='--', linewidth=2, label='Default Threshold(0.5)')\n",
    "    plt.xlabel('Predicted Probability', fontweight='bold')\n",
    "    plt.ylabel('Frequency', fontweight='bold')\n",
    "    plt.title('Test Set Predicted Probability Distribution', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # ROC curve\n",
    "    from sklearn.metrics import roc_curve\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_test_pred_proba)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {test_roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "    plt.xlabel('False Positive Rate', fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate', fontweight='bold')\n",
    "    plt.title('ROC Curve', fontweight='bold')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('RF_balanced_probability_roc.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Probability distribution and ROC curve saved to: RF_balanced_probability_roc.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Feature importance analysis\n",
    "    print(\"\\n=== Feature Importance Analysis ===\")\n",
    "    feature_importances = rf_balanced_model.feature_importances_\n",
    "    feature_names = model_info['feature_names']\n",
    "\n",
    "    # Create feature importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(importance_df.head(10).round(4))\n",
    "\n",
    "    # Save feature importance\n",
    "    importance_df.to_csv('RF_balanced_feature_importance.csv', index=False)\n",
    "    print(f\"Feature importance saved to: RF_balanced_feature_importance.csv\")\n",
    "\n",
    "    # Final conclusions\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🎯 Final Conclusions and Recommendations\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"✅ Model Performance Summary:\")\n",
    "    print(f\"   - ROC-AUC: {test_roc_auc:.3f}\")\n",
    "    print(f\"   - F1-Score: {test_f1:.4f}\")\n",
    "    print(f\"   - Precision: {test_precision:.4f}\")\n",
    "    print(f\"   - Recall: {test_recall:.4f}\")\n",
    "    print(f\"   - Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\n📋 Production Environment Recommendations:\")\n",
    "    print(\"1. Model is ready for deployment with optimized class weights\")\n",
    "    print(\"2. Monitor model performance and data drift in production\")\n",
    "    print(\"3. Consider periodic retraining with new data\")\n",
    "    print(\"4. Use default threshold (0.5) for classification\")\n",
    "\n",
    "    print(f\"\\n💡 Technical Notes:\")\n",
    "    print(\"- Model shows good discrimination ability with ROC-AUC performance\")\n",
    "    print(\"- Performance metrics indicate balanced performance across classes\")\n",
    "    print(\"- Confusion matrix shows model's prediction pattern\")\n",
    "    print(\"- Optimized class weights help handle imbalanced data\")\n",
    "\n",
    "    # Save model information\n",
    "    updated_model_info = {\n",
    "        'model': rf_balanced_model,\n",
    "        'class_weights': class_weights,\n",
    "        'performance': {\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_precision': test_precision,\n",
    "            'test_recall': test_recall,\n",
    "            'test_f1': test_f1,\n",
    "            'test_roc_auc': test_roc_auc\n",
    "        }\n",
    "    }\n",
    "\n",
    "    joblib.dump(updated_model_info, 'rf_balanced_final.pkl')\n",
    "    print(f\"\\n💾 Model information saved to: rf_balanced_final.pkl\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Saved RF_balanced model file 'rf_balanced_updated.pkl' not found\")\n",
    "    print(\"Please run the model creation and saving code first\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model loading or evaluation: {e}\")\n",
    "\n",
    "print(\"\\nEvaluation completed!\")"
   ],
   "id": "ef50871bc65add30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "\n",
    "# 设置绘图样式 - 英文专业格式\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.linewidth': 2.5,\n",
    "    'font.size': 16,\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 17,\n",
    "    'xtick.labelsize': 15,\n",
    "    'ytick.labelsize': 15,\n",
    "    'legend.fontsize': 15,\n",
    "    'grid.linewidth': 1.2,\n",
    "    'lines.linewidth': 3.5,\n",
    "    'figure.dpi': 1200,\n",
    "    'savefig.dpi': 1200,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.unicode_minus': False\n",
    "})\n",
    "\n",
    "print(\"=== Updated RF_balanced Model Performance Evaluation ===\")\n",
    "print(\"Using Optimized Class Weights for Imbalanced Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 初始化存储结果的列表\n",
    "metrics_results = []\n",
    "execution_times = []\n",
    "\n",
    "def plot_confusion_matrix(cm, dataset_name, filename):\n",
    "    \"\"\"绘制混淆矩阵热力图\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # 创建热力图\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                cbar=True, annot_kws={\"size\": 16, \"weight\": \"bold\"},\n",
    "                xticklabels=['Normal', 'Yarn Break'],\n",
    "                yticklabels=['Normal', 'Yarn Break'])\n",
    "\n",
    "    # 设置标题和标签\n",
    "    plt.title(f'Confusion Matrix - {dataset_name}\\nRF_balanced Model',\n",
    "              fontsize=18, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Predicted Label', fontsize=17, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=17, fontweight='bold')\n",
    "\n",
    "    # 设置坐标轴标签样式\n",
    "    plt.xticks(rotation=0, fontsize=15, fontweight='bold')\n",
    "    plt.yticks(rotation=0, fontsize=15, fontweight='bold')\n",
    "\n",
    "    # 调整布局并保存\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=1200, bbox_inches='tight',\n",
    "                facecolor='white', edgecolor='none')\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved to: {filename}\")\n",
    "\n",
    "try:\n",
    "    # 加载之前保存的更新后RF_balanced模型\n",
    "    print(\"Loading saved RF_balanced model...\")\n",
    "    load_start_time = time.time()\n",
    "    model_info = joblib.load('rf_balanced_updated.pkl')\n",
    "    load_time = time.time() - load_start_time\n",
    "\n",
    "    # 获取模型和相关信息\n",
    "    rf_balanced_model = model_info['model']\n",
    "    class_weights = model_info['class_weights']\n",
    "    train_break_count = model_info['train_break_count']\n",
    "    train_break_ratio = model_info['train_break_ratio']\n",
    "\n",
    "    print(f\"Model loading completed, time: {load_time:.2f} seconds\")\n",
    "    print(f\"Class weights used: {class_weights}\")\n",
    "    print(f\"Training set yarn break events: {train_break_count} (ratio: {train_break_ratio:.2%})\")\n",
    "\n",
    "    # 由于模型已经训练过，直接进行预测\n",
    "    print(\"\\nStarting model prediction...\")\n",
    "    predict_start_time = time.time()\n",
    "\n",
    "    # 在训练集和测试集上进行预测\n",
    "    y_train_pred = rf_balanced_model.predict(X_train)\n",
    "    y_test_pred = rf_balanced_model.predict(X_test)\n",
    "\n",
    "    # 获取概率预测\n",
    "    y_train_pred_proba = rf_balanced_model.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred_proba = rf_balanced_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    predict_time = time.time() - predict_start_time\n",
    "    print(f\"Prediction completed, time: {predict_time:.2f} seconds\")\n",
    "\n",
    "    # 计算训练集指标\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred, zero_division=0)\n",
    "    train_recall = recall_score(y_train, y_train_pred, zero_division=0)\n",
    "    train_f1 = f1_score(y_train, y_train_pred, zero_division=0)\n",
    "    train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "\n",
    "    # 计算测试集指标\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "    # 计算混淆矩阵\n",
    "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    total_time = load_time + predict_time\n",
    "\n",
    "    # 存储结果\n",
    "    metrics_results.append({\n",
    "        'Dataset': 'Training Set',\n",
    "        'Accuracy': train_accuracy,\n",
    "        'Precision': train_precision,\n",
    "        'Recall': train_recall,\n",
    "        'F1-Score': train_f1,\n",
    "        'ROC-AUC': train_roc_auc\n",
    "    })\n",
    "\n",
    "    metrics_results.append({\n",
    "        'Dataset': 'Test Set',\n",
    "        'Accuracy': test_accuracy,\n",
    "        'Precision': test_precision,\n",
    "        'Recall': test_recall,\n",
    "        'F1-Score': test_f1,\n",
    "        'ROC-AUC': test_roc_auc\n",
    "    })\n",
    "\n",
    "    execution_times.append({\n",
    "        'Model Loading Time': load_time,\n",
    "        'Prediction Time': predict_time,\n",
    "        'Total Time': total_time\n",
    "    })\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"\\nUpdated RF_balanced Model Evaluation Results:\")\n",
    "    print(\"Training Set Performance:\")\n",
    "    print(f\"  Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {train_precision:.4f}\")\n",
    "    print(f\"  Recall: {train_recall:.4f}\")\n",
    "    print(f\"  F1-Score: {train_f1:.4f}\")\n",
    "    print(f\"  ROC-AUC: {train_roc_auc:.4f}\")\n",
    "\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(f\"  Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {test_precision:.4f}\")\n",
    "    print(f\"  Recall: {test_recall:.4f}\")\n",
    "    print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "    print(f\"  ROC-AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrices:\")\n",
    "    print(\"Training Set Confusion Matrix:\")\n",
    "    print(cm_train)\n",
    "    print(\"Test Set Confusion Matrix:\")\n",
    "    print(cm_test)\n",
    "\n",
    "    print(\"\\nDetailed Classification Report - Test Set:\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=['Normal', 'Yarn Break']))\n",
    "\n",
    "    # 生成混淆矩阵可视化\n",
    "    print(\"\\n=== Generating Confusion Matrix Visualizations ===\")\n",
    "    plot_confusion_matrix(cm_train, \"Training Set\", \"training_confusion_matrix.png\")\n",
    "    plot_confusion_matrix(cm_test, \"Test Set\", \"test_confusion_matrix.png\")\n",
    "\n",
    "    print(f\"\\nTime Statistics:\")\n",
    "    print(f\"  Model Loading Time: {load_time:.2f} seconds\")\n",
    "    print(f\"  Prediction Time: {predict_time:.2f} seconds\")\n",
    "    print(f\"  Total Execution Time: {total_time:.2f} seconds\")\n",
    "\n",
    "    # 创建结果DataFrame\n",
    "    results_df = pd.DataFrame(metrics_results)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Updated RF_balanced Model Performance Summary\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results_df.round(4))\n",
    "\n",
    "    # 创建时间统计DataFrame\n",
    "    time_df = pd.DataFrame(execution_times)\n",
    "    print(\"\\nTime Statistics:\")\n",
    "    print(time_df.round(2))\n",
    "\n",
    "    # 保存结果到CSV文件\n",
    "    results_df.to_csv('RF_balanced_updated_performance.csv', index=False)\n",
    "    time_df.to_csv('RF_balanced_updated_timing.csv', index=False)\n",
    "    print(f\"\\nResults saved to: RF_balanced_updated_performance.csv and RF_balanced_updated_timing.csv\")\n",
    "\n",
    "    # 特征重要性分析\n",
    "    print(\"\\n=== Feature Importance Analysis ===\")\n",
    "    feature_importances = rf_balanced_model.feature_importances_\n",
    "    feature_names = model_info['feature_names']\n",
    "\n",
    "    # 创建特征重要性DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(importance_df.head(10).round(4))\n",
    "\n",
    "    # 保存特征重要性\n",
    "    importance_df.to_csv('RF_balanced_feature_importance.csv', index=False)\n",
    "    print(f\"Feature importance saved to: RF_balanced_feature_importance.csv\")\n",
    "\n",
    "    # 生成特征重要性可视化\n",
    "    print(\"\\n=== Generating Feature Importance Visualization ===\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # 获取前15个最重要的特征\n",
    "    top_features = importance_df.head(15)\n",
    "\n",
    "    # 创建水平条形图\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'],\n",
    "             color='steelblue', alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "    # 设置标签和标题\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'], fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Feature Importance', fontsize=17, fontweight='bold')\n",
    "    plt.title('Top 15 Most Important Features\\nRF_balanced Model',\n",
    "              fontsize=18, fontweight='bold', pad=20)\n",
    "\n",
    "    # 添加数值标签\n",
    "    for i, v in enumerate(top_features['Importance']):\n",
    "        plt.text(v + 0.001, i, f'{v:.3f}', fontsize=12, fontweight='bold',\n",
    "                va='center', ha='left')\n",
    "\n",
    "    # 调整布局\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance_visualization.png', dpi=1200,\n",
    "                bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    plt.close()\n",
    "    print(\"Feature importance visualization saved to: feature_importance_visualization.png\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Saved RF_balanced model file 'rf_balanced_updated.pkl' not found\")\n",
    "    print(\"Please run the model creation and saving code first\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model loading or evaluation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nEvaluation completed!\")"
   ],
   "id": "16fccddba2ea93a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# 设置绘图样式 - 英文专业格式\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.linewidth': 2.5,\n",
    "    'font.size': 16,\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 17,\n",
    "    'xtick.labelsize': 15,\n",
    "    'ytick.labelsize': 15,\n",
    "    'legend.fontsize': 15,\n",
    "    'grid.linewidth': 1.2,\n",
    "    'lines.linewidth': 3.5,\n",
    "    'figure.dpi': 1200,\n",
    "    'savefig.dpi': 1200,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.unicode_minus': False\n",
    "})\n",
    "\n",
    "print(\"=== RF_balanced Model Evaluation (Default Threshold) ===\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Load the previously saved updated RF_balanced model\n",
    "    print(\"Loading saved RF_balanced model...\")\n",
    "    model_info = joblib.load('rf_balanced_updated.pkl')\n",
    "\n",
    "    # Get model and related information\n",
    "    rf_balanced_model = model_info['model']\n",
    "    class_weights = model_info['class_weights']\n",
    "\n",
    "    print(f\"Model loaded successfully\")\n",
    "    print(f\"Class weights used: {class_weights}\")\n",
    "\n",
    "    # Get probability predictions\n",
    "    y_train_pred_proba = rf_balanced_model.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred_proba = rf_balanced_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Default threshold (0.5) results\n",
    "    y_train_pred_default = (y_train_pred_proba >= 0.5).astype(int)\n",
    "    y_test_pred_default = (y_test_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate metrics with default threshold\n",
    "    train_accuracy_def = accuracy_score(y_train, y_train_pred_default)\n",
    "    train_precision_def = precision_score(y_train, y_train_pred_default, zero_division=0)\n",
    "    train_recall_def = recall_score(y_train, y_train_pred_default, zero_division=0)\n",
    "    train_f1_def = f1_score(y_train, y_train_pred_default, zero_division=0)\n",
    "    train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "\n",
    "    test_accuracy_def = accuracy_score(y_test, y_test_pred_default)\n",
    "    test_precision_def = precision_score(y_test, y_test_pred_default, zero_division=0)\n",
    "    test_recall_def = recall_score(y_test, y_test_pred_default, zero_division=0)\n",
    "    test_f1_def = f1_score(y_test, y_test_pred_default, zero_division=0)\n",
    "    test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "    # Create results table\n",
    "    results_comparison = []\n",
    "\n",
    "    # Add results to table\n",
    "    results_comparison.append({\n",
    "        'Threshold': 'Default (0.5)',\n",
    "        'Dataset': 'Training Set',\n",
    "        'Accuracy': train_accuracy_def,\n",
    "        'Precision': train_precision_def,\n",
    "        'Recall': train_recall_def,\n",
    "        'F1-Score': train_f1_def,\n",
    "        'ROC-AUC': train_roc_auc\n",
    "    })\n",
    "\n",
    "    results_comparison.append({\n",
    "        'Threshold': 'Default (0.5)',\n",
    "        'Dataset': 'Test Set',\n",
    "        'Accuracy': test_accuracy_def,\n",
    "        'Precision': test_precision_def,\n",
    "        'Recall': test_recall_def,\n",
    "        'F1-Score': test_f1_def,\n",
    "        'ROC-AUC': test_roc_auc\n",
    "    })\n",
    "\n",
    "    results_df = pd.DataFrame(results_comparison)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Model Performance with Default Threshold (0.5)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results_df.round(4))\n",
    "\n",
    "    # Confusion matrix - Only Default Threshold\n",
    "    cm_train_default = confusion_matrix(y_train, y_train_pred_default)\n",
    "    cm_test_default = confusion_matrix(y_test, y_test_pred_default)\n",
    "\n",
    "    print(f\"\\nDefault Threshold(0.5) - Training Set Confusion Matrix:\")\n",
    "    print(cm_train_default)\n",
    "    print(f\"Default Threshold(0.5) - Test Set Confusion Matrix:\")\n",
    "    print(cm_test_default)\n",
    "\n",
    "    # Detailed classification report - Only Default Threshold\n",
    "    print(f\"\\nDefault Threshold(0.5) - Test Set Detailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred_default, target_names=['Normal', 'Yarn Break']))\n",
    "\n",
    "    # Visualization - Only Default Threshold Confusion Matrix\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('Model Performance with Default Threshold (0.5)', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Training set confusion matrix with enhanced styling\n",
    "    sns.heatmap(cm_train_default, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "                annot_kws={'size': 16, 'weight': 'bold'},  # 增大注释字体并加粗\n",
    "                linewidths=2, linecolor='black',  # 加粗单元格边框\n",
    "                cbar_kws={'shrink': 0.8})\n",
    "    axes[0].set_title('Training Set - Default Threshold(0.5)', fontsize=16, fontweight='bold', pad=20)\n",
    "    axes[0].set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "    axes[0].tick_params(axis='both', which='major', labelsize=13)\n",
    "\n",
    "    # 加粗整个图的边框\n",
    "    for spine in axes[0].spines.values():\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "    # Test set confusion matrix with enhanced styling  混肴矩阵设置 /////////////////////\n",
    "    sns.heatmap(cm_test_default, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "                annot_kws={'size': 16, 'weight': 'bold'},  # 增大注释字体并加粗\n",
    "                linewidths=2, linecolor='black',  # 加粗单元格边框\n",
    "                cbar_kws={'shrink': 0.8})\n",
    "    axes[1].set_title('Test Set - Default Threshold(0.5)', fontsize=16, fontweight='bold', pad=20)\n",
    "    axes[1].set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "    axes[1].tick_params(axis='both', which='major', labelsize=13)\n",
    "\n",
    "    # 加粗整个图的边框\n",
    "    for spine in axes[1].spines.values():\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "    # Probability distribution visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    break_proba = y_test_pred_proba[y_test == 1]\n",
    "    normal_proba = y_test_pred_proba[y_test == 0]\n",
    "\n",
    "    plt.hist(normal_proba, bins=50, alpha=0.7, label='Normal Events', color='green', edgecolor='black')\n",
    "    plt.hist(break_proba, bins=20, alpha=0.7, label='Yarn Break Events', color='red', edgecolor='black')\n",
    "    plt.axvline(x=0.5, color='orange', linestyle='--', linewidth=2, label='Default Threshold(0.5)')\n",
    "    plt.xlabel('Predicted Probability', fontweight='bold')\n",
    "    plt.ylabel('Frequency', fontweight='bold')\n",
    "    plt.title('Test Set Predicted Probability Distribution', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # ROC curve\n",
    "    from sklearn.metrics import roc_curve\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_test_pred_proba)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {test_roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "    plt.xlabel('False Positive Rate', fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate', fontweight='bold')\n",
    "    plt.title('ROC Curve', fontweight='bold')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Final conclusions\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🎯 Final Conclusions and Recommendations\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"✅ Model Performance Summary:\")\n",
    "    print(f\"   - ROC-AUC: {test_roc_auc:.3f}\")\n",
    "    print(f\"   - F1-Score: {test_f1_def:.4f}\")\n",
    "    print(f\"   - Precision: {test_precision_def:.4f}\")\n",
    "    print(f\"   - Recall: {test_recall_def:.4f}\")\n",
    "    print(f\"   - Accuracy: {test_accuracy_def:.4f}\")\n",
    "\n",
    "    print(\"\\n📋 Production Environment Recommendations:\")\n",
    "    print(\"1. Model is ready for deployment with default threshold (0.5)\")\n",
    "    print(\"2. Monitor model performance and data drift in production\")\n",
    "    print(\"3. Consider periodic retraining with new data\")\n",
    "\n",
    "    print(f\"\\n💡 Technical Notes:\")\n",
    "    print(\"- Model shows good discrimination ability with ROC-AUC performance\")\n",
    "    print(\"- Performance metrics indicate balanced performance across classes\")\n",
    "    print(\"- Confusion matrix shows model's prediction pattern\")\n",
    "\n",
    "    # Save model information\n",
    "    model_info = {\n",
    "        'model': rf_balanced_model,\n",
    "        'class_weights': class_weights,\n",
    "        'performance': {\n",
    "            'test_accuracy': test_accuracy_def,\n",
    "            'test_precision': test_precision_def,\n",
    "            'test_recall': test_recall_def,\n",
    "            'test_f1': test_f1_def,\n",
    "            'test_roc_auc': test_roc_auc\n",
    "        }\n",
    "    }\n",
    "\n",
    "    joblib.dump(model_info, 'rf_balanced_default.pkl')\n",
    "    print(f\"\\n💾 Model information saved to: rf_balanced_default.pkl\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Saved RF_balanced model file 'rf_balanced_updated.pkl' not found\")\n",
    "    print(\"Please run the model creation and saving code first\")\n",
    "\n",
    "    # If saved model is not found, fall back to original code\n",
    "    print(\"Attempting to use model from original models_balanced...\")\n",
    "\n",
    "    # Extract RF_balanced model directly from models_balanced\n",
    "    rf_balanced_model = None\n",
    "    for name, model in models_balanced:\n",
    "        if name == 'RF_balanced':\n",
    "            rf_balanced_model = model\n",
    "            break\n",
    "\n",
    "    if rf_balanced_model is None:\n",
    "        print(\"Error: RF_balanced model not found\")\n",
    "    else:\n",
    "        try:\n",
    "            # Train model (if not already trained)\n",
    "            rf_balanced_model.fit(X_train, y_train)\n",
    "\n",
    "            # Get probability predictions\n",
    "            y_train_pred_proba = rf_balanced_model.predict_proba(X_train)[:, 1]\n",
    "            y_test_pred_proba = rf_balanced_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            # Default threshold (0.5) results\n",
    "            y_train_pred_default = (y_train_pred_proba >= 0.5).astype(int)\n",
    "            y_test_pred_default = (y_test_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "            # Calculate metrics with default threshold\n",
    "            train_accuracy_def = accuracy_score(y_train, y_train_pred_default)\n",
    "            train_precision_def = precision_score(y_train, y_train_pred_default, zero_division=0)\n",
    "            train_recall_def = recall_score(y_train, y_train_pred_default, zero_division=0)\n",
    "            train_f1_def = f1_score(y_train, y_train_pred_default, zero_division=0)\n",
    "            train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "\n",
    "            test_accuracy_def = accuracy_score(y_test, y_test_pred_default)\n",
    "            test_precision_def = precision_score(y_test, y_test_pred_default, zero_division=0)\n",
    "            test_recall_def = recall_score(y_test, y_test_pred_default, zero_division=0)\n",
    "            test_f1_def = f1_score(y_test, y_test_pred_default, zero_division=0)\n",
    "            test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "            # Create results table\n",
    "            results_comparison = []\n",
    "\n",
    "            # Add results to table\n",
    "            results_comparison.append({\n",
    "                'Threshold': 'Default (0.5)',\n",
    "                'Dataset': 'Training Set',\n",
    "                'Accuracy': train_accuracy_def,\n",
    "                'Precision': train_precision_def,\n",
    "                'Recall': train_recall_def,\n",
    "                'F1-Score': train_f1_def,\n",
    "                'ROC-AUC': train_roc_auc\n",
    "            })\n",
    "\n",
    "            results_comparison.append({\n",
    "                'Threshold': 'Default (0.5)',\n",
    "                'Dataset': 'Test Set',\n",
    "                'Accuracy': test_accuracy_def,\n",
    "                'Precision': test_precision_def,\n",
    "                'Recall': test_recall_def,\n",
    "                'F1-Score': test_f1_def,\n",
    "                'ROC-AUC': test_roc_auc\n",
    "            })\n",
    "\n",
    "            results_df = pd.DataFrame(results_comparison)\n",
    "\n",
    "            # Print results\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"Model Performance with Default Threshold (0.5)\")\n",
    "            print(\"=\" * 80)\n",
    "            print(results_df.round(4))\n",
    "\n",
    "            # Confusion matrix - Only Default Threshold\n",
    "            cm_train_default = confusion_matrix(y_train, y_train_pred_default)\n",
    "            cm_test_default = confusion_matrix(y_test, y_test_pred_default)\n",
    "\n",
    "            print(f\"\\nDefault Threshold(0.5) - Training Set Confusion Matrix:\")\n",
    "            print(cm_train_default)\n",
    "            print(f\"Default Threshold(0.5) - Test Set Confusion Matrix:\")\n",
    "            print(cm_test_default)\n",
    "\n",
    "            # Detailed classification report - Only Default Threshold\n",
    "            print(f\"\\nDefault Threshold(0.5) - Test Set Detailed Classification Report:\")\n",
    "            print(classification_report(y_test, y_test_pred_default, target_names=['Normal', 'Yarn Break']))\n",
    "\n",
    "            # Visualization - Only Default Threshold Confusion Matrix with enhanced styling\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "            fig.suptitle('Model Performance with Default Threshold (0.5)', fontsize=18, fontweight='bold')\n",
    "\n",
    "            # Training set confusion matrix with enhanced styling\n",
    "            sns.heatmap(cm_train_default, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "                        annot_kws={'size': 20, 'weight': 'bold'},  # 增大注释字体并加粗\n",
    "                        linewidths=2, linecolor='black',  # 加粗单元格边框\n",
    "                        cbar_kws={'shrink': 0.8})\n",
    "            axes[0].set_title('Training Set - Default Threshold(0.5)', fontsize=16, fontweight='bold', pad=20)\n",
    "            axes[0].set_xlabel('Predicted Label', fontsize=20, fontweight='bold')\n",
    "            axes[0].set_ylabel('True Label', fontsize=20, fontweight='bold')\n",
    "            axes[0].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "            # 加粗整个图的边框\n",
    "            for spine in axes[0].spines.values():\n",
    "                spine.set_linewidth(3)\n",
    "\n",
    "            # Test set confusion matrix with enhanced styling\n",
    "            sns.heatmap(cm_test_default, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "                        annot_kws={'size': 20, 'weight': 'bold'},  # 增大注释字体并加粗\n",
    "                        linewidths=2, linecolor='black',  # 加粗单元格边框\n",
    "                        cbar_kws={'shrink': 0.8})\n",
    "            axes[1].set_title('Test Set - Default Threshold(0.5)', fontsize=18, fontweight='bold', pad=20)\n",
    "            axes[1].set_xlabel('Predicted Label', fontsize=20, fontweight='bold')\n",
    "            axes[1].set_ylabel('True Label', fontsize=20, fontweight='bold')\n",
    "            axes[1].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "            # 加粗整个图的边框\n",
    "            for spine in axes[1].spines.values():\n",
    "                spine.set_linewidth(3)\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "            plt.show()\n",
    "\n",
    "            # Probability distribution visualization\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            break_proba = y_test_pred_proba[y_test == 1]\n",
    "            normal_proba = y_test_pred_proba[y_test == 0]\n",
    "\n",
    "            plt.hist(normal_proba, bins=50, alpha=0.7, label='Normal Events', color='green', edgecolor='black')\n",
    "            plt.hist(break_proba, bins=20, alpha=0.7, label='Yarn Break Events', color='red', edgecolor='black')\n",
    "            plt.axvline(x=0.5, color='orange', linestyle='--', linewidth=2, label='Default Threshold(0.5)')\n",
    "            plt.xlabel('Predicted Probability', fontweight='bold')\n",
    "            plt.ylabel('Frequency', fontweight='bold')\n",
    "            plt.title('Test Set Predicted Probability Distribution', fontweight='bold')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            # ROC curve\n",
    "            from sklearn.metrics import roc_curve\n",
    "\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, y_test_pred_proba)\n",
    "            plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {test_roc_auc:.3f})')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "            plt.xlabel('False Positive Rate', fontweight='bold')\n",
    "            plt.ylabel('True Positive Rate', fontweight='bold')\n",
    "            plt.title('ROC Curve', fontweight='bold')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Final conclusions\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"🎯 Final Conclusions and Recommendations\")\n",
    "            print(\"=\" * 80)\n",
    "            print(\"✅ Model Performance Summary:\")\n",
    "            print(f\"   - ROC-AUC: {test_roc_auc:.3f}\")\n",
    "            print(f\"   - F1-Score: {test_f1_def:.4f}\")\n",
    "            print(f\"   - Precision: {test_precision_def:.4f}\")\n",
    "            print(f\"   - Recall: {test_recall_def:.4f}\")\n",
    "            print(f\"   - Accuracy: {test_accuracy_def:.4f}\")\n",
    "\n",
    "            print(\"\\n📋 Production Environment Recommendations:\")\n",
    "            print(\"1. Model is ready for deployment with default threshold (0.5)\")\n",
    "            print(\"2. Monitor model performance and data drift in production\")\n",
    "            print(\"3. Consider periodic retraining with new data\")\n",
    "\n",
    "            print(f\"\\n💡 Technical Notes:\")\n",
    "            print(\"- Model shows good discrimination ability with ROC-AUC performance\")\n",
    "            print(\"- Performance metrics indicate balanced performance across classes\")\n",
    "            print(\"- Confusion matrix shows model's prediction pattern\")\n",
    "\n",
    "            # Save model information\n",
    "            model_info = {\n",
    "                'model': rf_balanced_model,\n",
    "                'performance': {\n",
    "                    'test_accuracy': test_accuracy_def,\n",
    "                    'test_precision': test_precision_def,\n",
    "                    'test_recall': test_recall_def,\n",
    "                    'test_f1': test_f1_def,\n",
    "                    'test_roc_auc': test_roc_auc\n",
    "                }\n",
    "            }\n",
    "\n",
    "            joblib.dump(model_info, 'rf_balanced_default.pkl')\n",
    "            print(f\"\\n💾 Model information saved to: rf_balanced_default.pkl\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during model loading or evaluation: {e}\")\n",
    "\n",
    "print(\"\\nEvaluation completed!\")"
   ],
   "id": "49401f2c2e21d4e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# 设置绘图样式 - 英文专业格式\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.linewidth': 2.5,\n",
    "    'font.size': 16,\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 17,\n",
    "    'xtick.labelsize': 15,\n",
    "    'ytick.labelsize': 15,\n",
    "    'legend.fontsize': 15,\n",
    "    'grid.linewidth': 1.2,\n",
    "    'lines.linewidth': 3.5,\n",
    "    'figure.dpi': 1200,\n",
    "    'savefig.dpi': 1200,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.unicode_minus': False\n",
    "})\n",
    "\n",
    "print(\"=== RF_balanced Model Evaluation (Default Threshold) ===\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Load the previously saved updated RF_balanced model\n",
    "    print(\"Loading saved RF_balanced model...\")\n",
    "    model_info = joblib.load('rf_balanced_updated.pkl')\n",
    "\n",
    "    # Get model and related information\n",
    "    rf_balanced_model = model_info['model']\n",
    "    class_weights = model_info['class_weights']\n",
    "\n",
    "    print(f\"Model loaded successfully\")\n",
    "    print(f\"Class weights used: {class_weights}\")\n",
    "\n",
    "    # Get probability predictions\n",
    "    y_train_pred_proba = rf_balanced_model.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred_proba = rf_balanced_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Default threshold (0.5) results\n",
    "    y_train_pred_default = (y_train_pred_proba >= 0.5).astype(int)\n",
    "    y_test_pred_default = (y_test_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate metrics with default threshold\n",
    "    train_accuracy_def = accuracy_score(y_train, y_train_pred_default)\n",
    "    train_precision_def = precision_score(y_train, y_train_pred_default, zero_division=0)\n",
    "    train_recall_def = recall_score(y_train, y_train_pred_default, zero_division=0)\n",
    "    train_f1_def = f1_score(y_train, y_train_pred_default, zero_division=0)\n",
    "    train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "\n",
    "    test_accuracy_def = accuracy_score(y_test, y_test_pred_default)\n",
    "    test_precision_def = precision_score(y_test, y_test_pred_default, zero_division=0)\n",
    "    test_recall_def = recall_score(y_test, y_test_pred_default, zero_division=0)\n",
    "    test_f1_def = f1_score(y_test, y_test_pred_default, zero_division=0)\n",
    "    test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "    # Create results table\n",
    "    results_comparison = []\n",
    "\n",
    "    # Add results to table\n",
    "    results_comparison.append({\n",
    "        'Threshold': 'Default (0.5)',\n",
    "        'Dataset': 'Training Set',\n",
    "        'Accuracy': train_accuracy_def,\n",
    "        'Precision': train_precision_def,\n",
    "        'Recall': train_recall_def,\n",
    "        'F1-Score': train_f1_def,\n",
    "        'ROC-AUC': train_roc_auc\n",
    "    })\n",
    "\n",
    "    results_comparison.append({\n",
    "        'Threshold': 'Default (0.5)',\n",
    "        'Dataset': 'Test Set',\n",
    "        'Accuracy': test_accuracy_def,\n",
    "        'Precision': test_precision_def,\n",
    "        'Recall': test_recall_def,\n",
    "        'F1-Score': test_f1_def,\n",
    "        'ROC-AUC': test_roc_auc\n",
    "    })\n",
    "\n",
    "    results_df = pd.DataFrame(results_comparison)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Model Performance with Default Threshold (0.5)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results_df.round(4))\n",
    "\n",
    "    # Confusion matrix - Only Default Threshold\n",
    "    cm_train_default = confusion_matrix(y_train, y_train_pred_default)\n",
    "    cm_test_default = confusion_matrix(y_test, y_test_pred_default)\n",
    "\n",
    "    print(f\"\\nDefault Threshold(0.5) - Training Set Confusion Matrix:\")\n",
    "    print(cm_train_default)\n",
    "    print(f\"Default Threshold(0.5) - Test Set Confusion Matrix:\")\n",
    "    print(cm_test_default)\n",
    "\n",
    "    # Detailed classification report - Only Default Threshold\n",
    "    print(f\"\\nDefault Threshold(0.5) - Test Set Detailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred_default, target_names=['Normal', 'Yarn Break']))\n",
    "\n",
    "    # Visualization - Only Default Threshold Confusion Matrix\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('Model Performance with Default Threshold (0.5)', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # ==================== 修改位置1：训练集混淆矩阵字体设置 ====================\n",
    "    # Training set confusion matrix with enhanced styling\n",
    "    sns.heatmap(cm_train_default, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "                annot_kws={'size': 16, 'weight': 'bold'},  # 增大注释字体并加粗\n",
    "                linewidths=2, linecolor='black',  # 加粗单元格边框\n",
    "                cbar_kws={'shrink': 0.8})\n",
    "    axes[0].set_title('Training Set - Default Threshold(0.5)', fontsize=16, fontweight='bold', pad=20)\n",
    "    axes[0].set_xlabel('Predicted Label', fontsize=20, fontweight='bold')  # 从14改为20\n",
    "    axes[0].set_ylabel('True Label', fontsize=20, fontweight='bold')       # 从14改为20\n",
    "    axes[0].tick_params(axis='both', which='major', labelsize=18)          # 从13改为18\n",
    "\n",
    "    # 加粗整个图的边框\n",
    "    for spine in axes[0].spines.values():\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "    # ==================== 修改位置2：测试集混淆矩阵字体设置 ====================\n",
    "    # Test set confusion matrix with enhanced styling\n",
    "    sns.heatmap(cm_test_default, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "                annot_kws={'size': 16, 'weight': 'bold'},  # 增大注释字体并加粗\n",
    "                linewidths=2, linecolor='black',  # 加粗单元格边框\n",
    "                cbar_kws={'shrink': 0.8})\n",
    "    axes[1].set_title('Test Set - Default Threshold(0.5)', fontsize=16, fontweight='bold', pad=20)\n",
    "    axes[1].set_xlabel('Predicted Label', fontsize=20, fontweight='bold')  # 从14改为20\n",
    "    axes[1].set_ylabel('True Label', fontsize=20, fontweight='bold')       # 从14改为20\n",
    "    axes[1].tick_params(axis='both', which='major', labelsize=18)          # 从13改为18\n",
    "\n",
    "    # 加粗整个图的边框\n",
    "    for spine in axes[1].spines.values():\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "    # Probability distribution visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    break_proba = y_test_pred_proba[y_test == 1]\n",
    "    normal_proba = y_test_pred_proba[y_test == 0]\n",
    "\n",
    "    plt.hist(normal_proba, bins=50, alpha=0.7, label='Normal Events', color='green', edgecolor='black')\n",
    "    plt.hist(break_proba, bins=20, alpha=0.7, label='Yarn Break Events', color='red', edgecolor='black')\n",
    "    plt.axvline(x=0.5, color='orange', linestyle='--', linewidth=2, label='Default Threshold(0.5)')\n",
    "    plt.xlabel('Predicted Probability', fontweight='bold')\n",
    "    plt.ylabel('Frequency', fontweight='bold')\n",
    "    plt.title('Test Set Predicted Probability Distribution', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # ROC curve\n",
    "    from sklearn.metrics import roc_curve\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_test_pred_proba)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {test_roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "    plt.xlabel('False Positive Rate', fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate', fontweight='bold')\n",
    "    plt.title('ROC Curve', fontweight='bold')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Final conclusions\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🎯 Final Conclusions and Recommendations\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"✅ Model Performance Summary:\")\n",
    "    print(f\"   - ROC-AUC: {test_roc_auc:.3f}\")\n",
    "    print(f\"   - F1-Score: {test_f1_def:.4f}\")\n",
    "    print(f\"   - Precision: {test_precision_def:.4f}\")\n",
    "    print(f\"   - Recall: {test_recall_def:.4f}\")\n",
    "    print(f\"   - Accuracy: {test_accuracy_def:.4f}\")\n",
    "\n",
    "    print(\"\\n📋 Production Environment Recommendations:\")\n",
    "    print(\"1. Model is ready for deployment with default threshold (0.5)\")\n",
    "    print(\"2. Monitor model performance and data drift in production\")\n",
    "    print(\"3. Consider periodic retraining with new data\")\n",
    "\n",
    "    print(f\"\\n💡 Technical Notes:\")\n",
    "    print(\"- Model shows good discrimination ability with ROC-AUC performance\")\n",
    "    print(\"- Performance metrics indicate balanced performance across classes\")\n",
    "    print(\"- Confusion matrix shows model's prediction pattern\")\n",
    "\n",
    "    # Save model information\n",
    "    model_info = {\n",
    "        'model': rf_balanced_model,\n",
    "        'class_weights': class_weights,\n",
    "        'performance': {\n",
    "            'test_accuracy': test_accuracy_def,\n",
    "            'test_precision': test_precision_def,\n",
    "            'test_recall': test_recall_def,\n",
    "            'test_f1': test_f1_def,\n",
    "            'test_roc_auc': test_roc_auc\n",
    "        }\n",
    "    }\n",
    "\n",
    "    joblib.dump(model_info, 'rf_balanced_default.pkl')\n",
    "    print(f\"\\n💾 Model information saved to: rf_balanced_default.pkl\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Saved RF_balanced model file 'rf_balanced_updated.pkl' not found\")\n",
    "    print(\"Please run the model creation and saving code first\")\n",
    "\n",
    "    # If saved model is not found, fall back to original code\n",
    "    print(\"Attempting to use model from original models_balanced...\")\n",
    "\n",
    "    # Extract RF_balanced model directly from models_balanced\n",
    "    rf_balanced_model = None\n",
    "    for name, model in models_balanced:\n",
    "        if name == 'RF_balanced':\n",
    "            rf_balanced_model = model\n",
    "            break\n",
    "\n",
    "    if rf_balanced_model is None:\n",
    "        print(\"Error: RF_balanced model not found\")\n",
    "    else:\n",
    "        try:\n",
    "            # Train model (if not already trained)\n",
    "            rf_balanced_model.fit(X_train, y_train)\n",
    "\n",
    "            # Get probability predictions\n",
    "            y_train_pred_proba = rf_balanced_model.predict_proba(X_train)[:, 1]\n",
    "            y_test_pred_proba = rf_balanced_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            # Default threshold (0.5) results\n",
    "            y_train_pred_default = (y_train_pred_proba >= 0.5).astype(int)\n",
    "            y_test_pred_default = (y_test_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "            # Calculate metrics with default threshold\n",
    "            train_accuracy_def = accuracy_score(y_train, y_train_pred_default)\n",
    "            train_precision_def = precision_score(y_train, y_train_pred_default, zero_division=0)\n",
    "            train_recall_def = recall_score(y_train, y_train_pred_default, zero_division=0)\n",
    "            train_f1_def = f1_score(y_train, y_train_pred_default, zero_division=0)\n",
    "            train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "\n",
    "            test_accuracy_def = accuracy_score(y_test, y_test_pred_default)\n",
    "            test_precision_def = precision_score(y_test, y_test_pred_default, zero_division=0)\n",
    "            test_recall_def = recall_score(y_test, y_test_pred_default, zero_division=0)\n",
    "            test_f1_def = f1_score(y_test, y_test_pred_default, zero_division=0)\n",
    "            test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "            # Create results table\n",
    "            results_comparison = []\n",
    "\n",
    "            # Add results to table\n",
    "            results_comparison.append({\n",
    "                'Threshold': 'Default (0.5)',\n",
    "                'Dataset': 'Training Set',\n",
    "                'Accuracy': train_accuracy_def,\n",
    "                'Precision': train_precision_def,\n",
    "                'Recall': train_recall_def,\n",
    "                'F1-Score': train_f1_def,\n",
    "                'ROC-AUC': train_roc_auc\n",
    "            })\n",
    "\n",
    "            results_comparison.append({\n",
    "                'Threshold': 'Default (0.5)',\n",
    "                'Dataset': 'Test Set',\n",
    "                'Accuracy': test_accuracy_def,\n",
    "                'Precision': test_precision_def,\n",
    "                'Recall': test_recall_def,\n",
    "                'F1-Score': test_f1_def,\n",
    "                'ROC-AUC': test_roc_auc\n",
    "            })\n",
    "\n",
    "            results_df = pd.DataFrame(results_comparison)\n",
    "\n",
    "            # Print results\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"Model Performance with Default Threshold (0.5)\")\n",
    "            print(\"=\" * 80)\n",
    "            print(results_df.round(4))\n",
    "\n",
    "            # Confusion matrix - Only Default Threshold\n",
    "            cm_train_default = confusion_matrix(y_train, y_train_pred_default)\n",
    "            cm_test_default = confusion_matrix(y_test, y_test_pred_default)\n",
    "\n",
    "            print(f\"\\nDefault Threshold(0.5) - Training Set Confusion Matrix:\")\n",
    "            print(cm_train_default)\n",
    "            print(f\"Default Threshold(0.5) - Test Set Confusion Matrix:\")\n",
    "            print(cm_test_default)\n",
    "\n",
    "            # Detailed classification report - Only Default Threshold\n",
    "            print(f\"\\nDefault Threshold(0.5) - Test Set Detailed Classification Report:\")\n",
    "            print(classification_report(y_test, y_test_pred_default, target_names=['Normal', 'Yarn Break']))\n",
    "\n",
    "            # Visualization - Only Default Threshold Confusion Matrix with enhanced styling\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "            fig.suptitle('Model Performance with Default Threshold (0.5)', fontsize=18, fontweight='bold')\n",
    "\n",
    "            # ==================== 修改位置3：训练集混淆矩阵字体设置（异常处理部分） ====================\n",
    "            # Training set confusion matrix with enhanced styling\n",
    "            sns.heatmap(cm_train_default, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "                        annot_kws={'size': 20, 'weight': 'bold'},  # 增大注释字体并加粗\n",
    "                        linewidths=2, linecolor='black',  # 加粗单元格边框\n",
    "                        cbar_kws={'shrink': 0.8})\n",
    "            axes[0].set_title('Training Set - Default Threshold(0.5)', fontsize=16, fontweight='bold', pad=20)\n",
    "            axes[0].set_xlabel('Predicted Label', fontsize=20, fontweight='bold')  # 从20改为20（保持不变）\n",
    "            axes[0].set_ylabel('True Label', fontsize=20, fontweight='bold')       # 从20改为20（保持不变）\n",
    "            axes[0].tick_params(axis='both', which='major', labelsize=18)          # 从14改为18\n",
    "\n",
    "            # 加粗整个图的边框\n",
    "            for spine in axes[0].spines.values():\n",
    "                spine.set_linewidth(3)\n",
    "\n",
    "            # ==================== 修改位置4：测试集混淆矩阵字体设置（异常处理部分） ====================\n",
    "            # Test set confusion matrix with enhanced styling\n",
    "            sns.heatmap(cm_test_default, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "                        annot_kws={'size': 20, 'weight': 'bold'},  # 增大注释字体并加粗\n",
    "                        linewidths=2, linecolor='black',  # 加粗单元格边框\n",
    "                        cbar_kws={'shrink': 0.8})\n",
    "            axes[1].set_title('Test Set - Default Threshold(0.5)', fontsize=18, fontweight='bold', pad=20)\n",
    "            axes[1].set_xlabel('Predicted Label', fontsize=20, fontweight='bold')  # 从20改为20（保持不变）\n",
    "            axes[1].set_ylabel('True Label', fontsize=20, fontweight='bold')       # 从20改为20（保持不变）\n",
    "            axes[1].tick_params(axis='both', which='major', labelsize=18)          # 从14改为18\n",
    "\n",
    "            # 加粗整个图的边框\n",
    "            for spine in axes[1].spines.values():\n",
    "                spine.set_linewidth(3)\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "            plt.show()\n",
    "\n",
    "            # Probability distribution visualization\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            break_proba = y_test_pred_proba[y_test == 1]\n",
    "            normal_proba = y_test_pred_proba[y_test == 0]\n",
    "\n",
    "            plt.hist(normal_proba, bins=50, alpha=0.7, label='Normal Events', color='green', edgecolor='black')\n",
    "            plt.hist(break_proba, bins=20, alpha=0.7, label='Yarn Break Events', color='red', edgecolor='black')\n",
    "            plt.axvline(x=0.5, color='orange', linestyle='--', linewidth=2, label='Default Threshold(0.5)')\n",
    "            plt.xlabel('Predicted Probability', fontweight='bold')\n",
    "            plt.ylabel('Frequency', fontweight='bold')\n",
    "            plt.title('Test Set Predicted Probability Distribution', fontweight='bold')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            # ROC curve\n",
    "            from sklearn.metrics import roc_curve\n",
    "\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, y_test_pred_proba)\n",
    "            plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {test_roc_auc:.3f})')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "            plt.xlabel('False Positive Rate', fontweight='bold')\n",
    "            plt.ylabel('True Positive Rate', fontweight='bold')\n",
    "            plt.title('ROC Curve', fontweight='bold')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Final conclusions\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"🎯 Final Conclusions and Recommendations\")\n",
    "            print(\"=\" * 80)\n",
    "            print(\"✅ Model Performance Summary:\")\n",
    "            print(f\"   - ROC-AUC: {test_roc_auc:.3f}\")\n",
    "            print(f\"   - F1-Score: {test_f1_def:.4f}\")\n",
    "            print(f\"   - Precision: {test_precision_def:.4f}\")\n",
    "            print(f\"   - Recall: {test_recall_def:.4f}\")\n",
    "            print(f\"   - Accuracy: {test_accuracy_def:.4f}\")\n",
    "\n",
    "            print(\"\\n📋 Production Environment Recommendations:\")\n",
    "            print(\"1. Model is ready for deployment with default threshold (0.5)\")\n",
    "            print(\"2. Monitor model performance and data drift in production\")\n",
    "            print(\"3. Consider periodic retraining with new data\")\n",
    "\n",
    "            print(f\"\\n💡 Technical Notes:\")\n",
    "            print(\"- Model shows good discrimination ability with ROC-AUC performance\")\n",
    "            print(\"- Performance metrics indicate balanced performance across classes\")\n",
    "            print(\"- Confusion matrix shows model's prediction pattern\")\n",
    "\n",
    "            # Save model information\n",
    "            model_info = {\n",
    "                'model': rf_balanced_model,\n",
    "                'performance': {\n",
    "                    'test_accuracy': test_accuracy_def,\n",
    "                    'test_precision': test_precision_def,\n",
    "                    'test_recall': test_recall_def,\n",
    "                    'test_f1': test_f1_def,\n",
    "                    'test_roc_auc': test_roc_auc\n",
    "                }\n",
    "            }\n",
    "\n",
    "            joblib.dump(model_info, 'rf_balanced_default.pkl')\n",
    "            print(f\"\\n💾 Model information saved to: rf_balanced_default.pkl\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during model loading or evaluation: {e}\")\n",
    "\n",
    "print(\"\\nEvaluation completed!\")"
   ],
   "id": "ec6a3a239d4b0fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "701a1370",
   "metadata": {
    "papermill": {
     "duration": 0.078193,
     "end_time": "2023-09-10T22:55:55.639798",
     "exception": false,
     "start_time": "2023-09-10T22:55:55.561605",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>19 |</span></b> <b>Hyperparameter optimization</b></div>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import pandas as pd\n",
    "#\n",
    "# # 初始化列表存储分类指标\n",
    "# accuracy_scores = []\n",
    "# precision_scores = []\n",
    "# recall_scores = []\n",
    "# f1_scores = []\n",
    "# roc_auc_scores = []\n",
    "# execution_times = []\n",
    "# model_names = []\n",
    "#\n",
    "# # 定义分类模型\n",
    "# models = [\n",
    "#     ('RF', RandomForestClassifier()),\n",
    "#     ('XGBoost', XGBClassifier()),\n",
    "#     ('LightGBM', LGBMClassifier())\n",
    "# ]\n",
    "#\n",
    "# # 定义超参数网格\n",
    "# param_grids = {\n",
    "#     'RF': {'n_estimators': [10, 30, 50, 70, 100], 'max_depth': [None, 5, 10, 20], 'class_weight': [None, 'balanced']},\n",
    "#     'XGBoost': {'n_estimators': [10, 30, 50, 70, 100], 'learning_rate': [0.005, 0.01, 0.05, 0.1], 'scale_pos_weight': [1, 10, 100]},\n",
    "#     'LightGBM': {'n_estimators': [10, 30, 50, 70, 100], 'learning_rate': [0.005, 0.01, 0.05, 0.1], 'is_unbalance': [True, False]}\n",
    "# }\n",
    "#\n",
    "# # 主循环\n",
    "# for name, classifier in models:\n",
    "#     start_time = time.time()\n",
    "#\n",
    "#     # 超参数调优\n",
    "#     if param_grids.get(name):\n",
    "#         grid_search = GridSearchCV(classifier, param_grid=param_grids[name], cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "#         grid_search.fit(X_train, y_train)\n",
    "#         best_model = grid_search.best_estimator_\n",
    "#     else:\n",
    "#         best_model = classifier\n",
    "#         best_model.fit(X_train, y_train)\n",
    "#\n",
    "#     # 预测\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "#\n",
    "#     # 计算分类指标\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     precision = precision_score(y_test, y_pred)\n",
    "#     recall = recall_score(y_test, y_pred)\n",
    "#     f1 = f1_score(y_test, y_pred)\n",
    "#     roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "#\n",
    "#     accuracy_scores.append(accuracy)\n",
    "#     precision_scores.append(precision)\n",
    "#     recall_scores.append(recall)\n",
    "#     f1_scores.append(f1)\n",
    "#     roc_auc_scores.append(roc_auc)\n",
    "#\n",
    "#     # 记录执行时间\n",
    "#     execution_time = time.time() - start_time\n",
    "#     execution_times.append(execution_time)\n",
    "#\n",
    "#     # 记录模型名称\n",
    "#     model_names.append(name)\n",
    "#\n",
    "# # 保存结果\n",
    "# results_df = pd.DataFrame({\n",
    "#     'Model': model_names,\n",
    "#     'Accuracy': accuracy_scores,\n",
    "#     'Precision': precision_scores,\n",
    "#     'Recall': recall_scores,\n",
    "#     'F1 Score': f1_scores,\n",
    "#     'ROC-AUC': roc_auc_scores,\n",
    "#     'Execution Time (s)': execution_times\n",
    "# })\n",
    "#\n",
    "# results_df"
   ],
   "id": "18f4650d998fb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# 设置中文字体和样式\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"=== RF_balanced模型参数调优与性能评估 ===\")\n",
    "print(\"包含交叉验证和超参数优化\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 初始化存储结果的列表\n",
    "metrics_results = []\n",
    "execution_times = []\n",
    "cv_results = []\n",
    "param_analysis = []\n",
    "\n",
    "try:\n",
    "    # 加载之前保存的更新后RF_balanced模型\n",
    "    print(\"加载已保存的RF_balanced模型...\")\n",
    "    load_start_time = time.time()\n",
    "    model_info = joblib.load('rf_balanced_updated.pkl')\n",
    "    load_time = time.time() - load_start_time\n",
    "\n",
    "    # 获取模型和相关信息\n",
    "    rf_balanced_model = model_info['model']\n",
    "    class_weights = model_info['class_weights']\n",
    "    train_break_count = model_info['train_break_count']\n",
    "    train_break_ratio = model_info['train_break_ratio']\n",
    "    feature_names = model_info['feature_names']\n",
    "\n",
    "    print(f\"模型加载完成，耗时: {load_time:.2f}秒\")\n",
    "    print(f\"使用的类别权重: {class_weights}\")\n",
    "    print(f\"训练集断纱事件: {train_break_count} (占比: {train_break_ratio:.2%})\")\n",
    "\n",
    "    # 1. 交叉验证评估\n",
    "    print(\"\\n=== 交叉验证评估 ===\")\n",
    "    cv_start_time = time.time()\n",
    "\n",
    "    # 使用分层K折交叉验证\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # 评估多个指标\n",
    "    scoring_metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "    cv_scores = {}\n",
    "    for metric in scoring_metrics:\n",
    "        scores = cross_val_score(rf_balanced_model, X_train, y_train,\n",
    "                               cv=cv, scoring=metric, n_jobs=-1)\n",
    "        cv_scores[metric] = scores\n",
    "        cv_results.append({\n",
    "            'Metric': metric,\n",
    "            'Mean_Score': np.mean(scores),\n",
    "            'Std_Score': np.std(scores),\n",
    "            'Scores': scores\n",
    "        })\n",
    "\n",
    "    cv_time = time.time() - cv_start_time\n",
    "    print(f\"交叉验证完成，耗时: {cv_time:.2f}秒\")\n",
    "\n",
    "    # 打印交叉验证结果\n",
    "    print(\"\\n交叉验证结果 (5折):\")\n",
    "    cv_df = pd.DataFrame(cv_results)\n",
    "    print(cv_df[['Metric', 'Mean_Score', 'Std_Score']].round(4))\n",
    "\n",
    "    # 2. 超参数优化\n",
    "    print(\"\\n=== 超参数优化 ===\")\n",
    "    param_search_start = time.time()\n",
    "\n",
    "    # 定义参数网格\n",
    "    param_dist = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30, 40],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # 使用随机搜索进行参数优化\n",
    "    random_search = RandomizedSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,  # 随机搜索的迭代次数\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"开始随机搜索参数优化...\")\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    param_search_time = time.time() - param_search_start\n",
    "    print(f\"参数优化完成，耗时: {param_search_time:.2f}秒\")\n",
    "\n",
    "    # 输出最佳参数\n",
    "    print(\"\\n最佳参数组合:\")\n",
    "    best_params = random_search.best_params_\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print(f\"最佳交叉验证分数 (F1): {random_search.best_score_:.4f}\")\n",
    "\n",
    "    # 3. 参数影响分析\n",
    "    print(\"\\n=== 参数影响分析 ===\")\n",
    "\n",
    "    # 分析不同参数对性能的影响\n",
    "    results_df = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "    # 分析主要参数的影响\n",
    "    key_params = ['param_n_estimators', 'param_max_depth',\n",
    "                 'param_min_samples_split', 'param_min_samples_leaf']\n",
    "\n",
    "    for param in key_params:\n",
    "        if param in results_df.columns:\n",
    "            param_data = results_df.groupby(param)['mean_test_score'].agg(['mean', 'std', 'count']).reset_index()\n",
    "            param_data = param_data.sort_values('mean', ascending=False)\n",
    "\n",
    "            print(f\"\\n{param} 对性能的影响:\")\n",
    "            for _, row in param_data.head(10).iterrows():\n",
    "                param_value = row[param]\n",
    "                if hasattr(param_value, '__len__') and not isinstance(param_value, str):\n",
    "                    param_value = str(param_value)\n",
    "                print(f\"  {param_value}: {row['mean']:.4f} ± {row['std']:.4f}\")\n",
    "\n",
    "            # 存储参数分析结果\n",
    "            param_analysis.extend([\n",
    "                {\n",
    "                    'Parameter': param.replace('param_', ''),\n",
    "                    'Value': row[param],\n",
    "                    'Mean_Score': row['mean'],\n",
    "                    'Std_Score': row['std'],\n",
    "                    'Count': row['count']\n",
    "                } for _, row in param_data.iterrows()\n",
    "            ])\n",
    "\n",
    "    # 4. 参数趋势可视化 - 新增详细可视化\n",
    "    print(\"\\n=== 生成参数趋势可视化 ===\")\n",
    "\n",
    "    # 创建参数分析DataFrame\n",
    "    param_analysis_df = pd.DataFrame(param_analysis)\n",
    "\n",
    "    # 为每个参数创建趋势图\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # 颜色设置\n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "\n",
    "    # 参数显示名称映射\n",
    "    param_display_names = {\n",
    "        'n_estimators': '树的数量 (n_estimators)',\n",
    "        'max_depth': '最大深度 (max_depth)',\n",
    "        'min_samples_split': '分裂最小样本数 (min_samples_split)',\n",
    "        'min_samples_leaf': '叶节点最小样本数 (min_samples_leaf)'\n",
    "    }\n",
    "\n",
    "    for i, (param_name, display_name) in enumerate(param_display_names.items()):\n",
    "        param_data = param_analysis_df[param_analysis_df['Parameter'] == param_name]\n",
    "\n",
    "        if not param_data.empty:\n",
    "            # 确保数值类型排序\n",
    "            param_data = param_data.copy()\n",
    "\n",
    "            # 处理max_depth中的None值\n",
    "            if param_name == 'max_depth':\n",
    "                param_data['Sort_Key'] = param_data['Value'].apply(lambda x: 1000 if x is None else x)\n",
    "                param_data = param_data.sort_values('Sort_Key')\n",
    "                x_labels = [str(x) if x is not None else 'None' for x in param_data['Value']]\n",
    "            else:\n",
    "                param_data = param_data.sort_values('Value')\n",
    "                x_labels = [str(x) for x in param_data['Value']]\n",
    "\n",
    "            # 绘制趋势线\n",
    "            x_positions = range(len(param_data))\n",
    "            axes[i].plot(x_positions, param_data['Mean_Score'],\n",
    "                        'o-', linewidth=3, markersize=8, color=colors[i],\n",
    "                        label='平均F1分数', markerfacecolor='white', markeredgewidth=2)\n",
    "\n",
    "            # 添加误差线\n",
    "            axes[i].fill_between(x_positions,\n",
    "                               param_data['Mean_Score'] - param_data['Std_Score'],\n",
    "                               param_data['Mean_Score'] + param_data['Std_Score'],\n",
    "                               alpha=0.2, color=colors[i], label='标准差范围')\n",
    "\n",
    "            # 标记最佳值\n",
    "            best_idx = param_data['Mean_Score'].idxmax()\n",
    "            best_x = list(x_positions)[list(param_data.index).index(best_idx)]\n",
    "            best_score = param_data.loc[best_idx, 'Mean_Score']\n",
    "            best_value = param_data.loc[best_idx, 'Value']\n",
    "\n",
    "            axes[i].axvline(x=best_x, color='red', linestyle='--', alpha=0.8, linewidth=2)\n",
    "            axes[i].plot(best_x, best_score, 'o', markersize=10, color='red',\n",
    "                       label=f'最佳值: {best_value} (F1={best_score:.3f})')\n",
    "\n",
    "            # 设置图表属性\n",
    "            axes[i].set_title(f'{display_name} 对模型性能的影响', fontsize=14, fontweight='bold', pad=20)\n",
    "            axes[i].set_xlabel('参数值', fontsize=12)\n",
    "            axes[i].set_ylabel('F1分数', fontsize=12)\n",
    "            axes[i].set_xticks(x_positions)\n",
    "            axes[i].set_xticklabels(x_labels, rotation=45)\n",
    "            axes[i].legend(loc='lower right' if param_name in ['n_estimators', 'max_depth'] else 'upper right')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "            # 设置y轴范围，突出差异\n",
    "            y_min = max(0.8, param_data['Mean_Score'].min() - 0.05)\n",
    "            y_max = min(1.0, param_data['Mean_Score'].max() + 0.05)\n",
    "            axes[i].set_ylim(y_min, y_max)\n",
    "\n",
    "    plt.suptitle('随机森林参数对模型性能的影响趋势分析', fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('parameter_trend_analysis_detailed.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"详细参数趋势分析图已保存至: parameter_trend_analysis_detailed.png\")\n",
    "\n",
    "    # 5. 创建参数热力图\n",
    "    print(\"\\n=== 生成参数组合热力图 ===\")\n",
    "\n",
    "    # 提取前20个最佳参数组合进行热力分析\n",
    "    top_results = results_df.nlargest(20, 'mean_test_score')[['params', 'mean_test_score']].copy()\n",
    "\n",
    "    # 解析参数\n",
    "    param_values_heatmap = []\n",
    "    for _, row in top_results.iterrows():\n",
    "        params = eval(row['params'])  # 将字符串转换为字典\n",
    "        params['score'] = row['mean_test_score']\n",
    "        param_values_heatmap.append(params)\n",
    "\n",
    "    heatmap_df = pd.DataFrame(param_values_heatmap)\n",
    "\n",
    "    # 创建热力图数据\n",
    "    heatmap_data = heatmap_df.pivot_table(\n",
    "        index='n_estimators',\n",
    "        columns='max_depth',\n",
    "        values='score',\n",
    "        aggfunc='mean'\n",
    "    ).fillna(0)\n",
    "\n",
    "    # 绘制热力图\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "                cbar_kws={'label': 'F1分数'}, linewidths=0.5)\n",
    "    plt.title('不同参数组合的性能热力图\\n(n_estimators vs max_depth)', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.xlabel('最大深度 (max_depth)', fontsize=12)\n",
    "    plt.ylabel('树的数量 (n_estimators)', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('parameter_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"参数组合热力图已保存至: parameter_heatmap.png\")\n",
    "\n",
    "    # 6. 使用最佳参数训练新模型\n",
    "    print(\"\\n=== 使用最佳参数训练新模型 ===\")\n",
    "    training_start = time.time()\n",
    "\n",
    "    # 创建使用最佳参数的新模型\n",
    "    optimized_rf_model = RandomForestClassifier(\n",
    "        **best_params,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # 训练模型\n",
    "    optimized_rf_model.fit(X_train, y_train)\n",
    "    training_time = time.time() - training_start\n",
    "    print(f\"模型训练完成，耗时: {training_time:.2f}秒\")\n",
    "\n",
    "    # 7. 评估优化后的模型\n",
    "    print(\"\\n=== 优化后模型性能评估 ===\")\n",
    "    predict_start_time = time.time()\n",
    "\n",
    "    # 在训练集和测试集上进行预测\n",
    "    y_train_pred_opt = optimized_rf_model.predict(X_train)\n",
    "    y_test_pred_opt = optimized_rf_model.predict(X_test)\n",
    "\n",
    "    # 获取概率预测\n",
    "    y_train_pred_proba_opt = optimized_rf_model.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred_proba_opt = optimized_rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    predict_time = time.time() - predict_start_time\n",
    "    print(f\"预测完成，耗时: {predict_time:.2f}秒\")\n",
    "\n",
    "    # 计算训练集指标\n",
    "    train_accuracy_opt = accuracy_score(y_train, y_train_pred_opt)\n",
    "    train_precision_opt = precision_score(y_train, y_train_pred_opt, zero_division=0)\n",
    "    train_recall_opt = recall_score(y_train, y_train_pred_opt, zero_division=0)\n",
    "    train_f1_opt = f1_score(y_train, y_train_pred_opt, zero_division=0)\n",
    "    train_roc_auc_opt = roc_auc_score(y_train, y_train_pred_proba_opt)\n",
    "\n",
    "    # 计算测试集指标\n",
    "    test_accuracy_opt = accuracy_score(y_test, y_test_pred_opt)\n",
    "    test_precision_opt = precision_score(y_test, y_test_pred_opt, zero_division=0)\n",
    "    test_recall_opt = recall_score(y_test, y_test_pred_opt, zero_division=0)\n",
    "    test_f1_opt = f1_score(y_test, y_test_pred_opt, zero_division=0)\n",
    "    test_roc_auc_opt = roc_auc_score(y_test, y_test_pred_proba_opt)\n",
    "\n",
    "    # 计算混淆矩阵\n",
    "    cm_train_opt = confusion_matrix(y_train, y_train_pred_opt)\n",
    "    cm_test_opt = confusion_matrix(y_test, y_test_pred_opt)\n",
    "\n",
    "    total_time = load_time + cv_time + param_search_time + training_time + predict_time\n",
    "\n",
    "    # 存储优化后模型的结果\n",
    "    metrics_results.append({\n",
    "        'Dataset': '优化后-训练集',\n",
    "        'Accuracy': train_accuracy_opt,\n",
    "        'Precision': train_precision_opt,\n",
    "        'Recall': train_recall_opt,\n",
    "        'F1-Score': train_f1_opt,\n",
    "        'ROC-AUC': train_roc_auc_opt,\n",
    "        'Parameters': str(best_params)\n",
    "    })\n",
    "\n",
    "    metrics_results.append({\n",
    "        'Dataset': '优化后-测试集',\n",
    "        'Accuracy': test_accuracy_opt,\n",
    "        'Precision': test_precision_opt,\n",
    "        'Recall': test_recall_opt,\n",
    "        'F1-Score': test_f1_opt,\n",
    "        'ROC-AUC': test_roc_auc_opt,\n",
    "        'Parameters': str(best_params)\n",
    "    })\n",
    "\n",
    "    execution_times.append({\n",
    "        '模型加载时间': load_time,\n",
    "        '交叉验证时间': cv_time,\n",
    "        '参数搜索时间': param_search_time,\n",
    "        '模型训练时间': training_time,\n",
    "        '预测时间': predict_time,\n",
    "        '总时间': total_time\n",
    "    })\n",
    "\n",
    "    # 打印优化后模型结果\n",
    "    print(\"\\n优化后RF_balanced模型评估结果:\")\n",
    "    print(\"训练集性能:\")\n",
    "    print(f\"  准确率: {train_accuracy_opt:.4f}\")\n",
    "    print(f\"  精确率: {train_precision_opt:.4f}\")\n",
    "    print(f\"  召回率: {train_recall_opt:.4f}\")\n",
    "    print(f\"  F1分数: {train_f1_opt:.4f}\")\n",
    "    print(f\"  ROC-AUC: {train_roc_auc_opt:.4f}\")\n",
    "\n",
    "    print(\"\\n测试集性能:\")\n",
    "    print(f\"  准确率: {test_accuracy_opt:.4f}\")\n",
    "    print(f\"  精确率: {test_precision_opt:.4f}\")\n",
    "    print(f\"  召回率: {test_recall_opt:.4f}\")\n",
    "    print(f\"  F1分数: {test_f1_opt:.4f}\")\n",
    "    print(f\"  ROC-AUC: {test_roc_auc_opt:.4f}\")\n",
    "\n",
    "    print(\"\\n混淆矩阵:\")\n",
    "    print(\"训练集混淆矩阵:\")\n",
    "    print(cm_train_opt)\n",
    "    print(\"测试集混淆矩阵:\")\n",
    "    print(cm_test_opt)\n",
    "\n",
    "    print(\"\\n详细分类报告 - 测试集:\")\n",
    "    print(classification_report(y_test, y_test_pred_opt, target_names=['正常', '断纱']))\n",
    "\n",
    "    # 8. 保存优化后的模型\n",
    "    print(\"\\n=== 保存优化后的模型 ===\")\n",
    "    optimized_model_info = {\n",
    "        'model': optimized_rf_model,\n",
    "        'best_params': best_params,\n",
    "        'class_weights': class_weights,\n",
    "        'train_break_count': train_break_count,\n",
    "        'train_break_ratio': train_break_ratio,\n",
    "        'feature_names': feature_names,\n",
    "        'cv_results': cv_results,\n",
    "        'param_analysis': param_analysis,\n",
    "        'optimization_time': param_search_time\n",
    "    }\n",
    "\n",
    "    joblib.dump(optimized_model_info, 'rf_balanced_optimized.pkl')\n",
    "    print(\"优化后的模型已保存至: rf_balanced_optimized.pkl\")\n",
    "\n",
    "    # 9. 最终结果汇总\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RF_balanced模型优化总结\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 创建结果DataFrame\n",
    "    results_df_final = pd.DataFrame(metrics_results)\n",
    "    print(\"\\n模型性能总结:\")\n",
    "    print(results_df_final.round(4))\n",
    "\n",
    "    # 创建时间统计DataFrame\n",
    "    time_df = pd.DataFrame(execution_times)\n",
    "    print(\"\\n时间统计:\")\n",
    "    print(time_df.round(2))\n",
    "\n",
    "    # 保存所有结果到CSV文件\n",
    "    results_df_final.to_csv('RF_balanced_optimized_performance.csv', index=False)\n",
    "    time_df.to_csv('RF_balanced_optimized_timing.csv', index=False)\n",
    "    cv_df.to_csv('RF_balanced_cross_validation.csv', index=False)\n",
    "    param_analysis_df.to_csv('RF_balanced_parameter_analysis.csv', index=False)\n",
    "\n",
    "    print(f\"\\n所有结果已保存至CSV文件\")\n",
    "\n",
    "    # 特征重要性分析\n",
    "    print(\"\\n=== 优化后模型特征重要性分析 ===\")\n",
    "    feature_importances_opt = optimized_rf_model.feature_importances_\n",
    "\n",
    "    # 创建特征重要性DataFrame\n",
    "    importance_df_opt = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances_opt\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    print(\"前10个最重要的特征:\")\n",
    "    print(importance_df_opt.head(10).round(4))\n",
    "\n",
    "    # 保存特征重要性\n",
    "    importance_df_opt.to_csv('RF_balanced_optimized_feature_importance.csv', index=False)\n",
    "    print(f\"特征重要性已保存至: RF_balanced_optimized_feature_importance.csv\")\n",
    "\n",
    "    print(\"\\n优化流程完成！\")\n",
    "    print(f\"总执行时间: {total_time:.2f}秒\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"错误: 未找到保存的RF_balanced模型文件 'rf_balanced_updated.pkl'\")\n",
    "    print(\"请先运行模型创建和保存代码\")\n",
    "except Exception as e:\n",
    "    print(f\"模型加载或评估过程中出错: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n所有评估和优化完成！\")"
   ],
   "id": "f52e46329d4e078",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置中文字体和样式\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"=== RF_balanced模型参数调优与性能评估 ===\")\n",
    "print(\"包含交叉验证和超参数优化\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 设置绘图样式 - 英文专业格式\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.linewidth': 2.5,\n",
    "    'font.size': 16,\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 17,\n",
    "    'xtick.labelsize': 15,\n",
    "    'ytick.labelsize': 15,\n",
    "    'legend.fontsize': 15,\n",
    "    'grid.linewidth': 1.2,\n",
    "    'lines.linewidth': 3.5,\n",
    "    'figure.dpi': 1200,\n",
    "    'savefig.dpi': 1200,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.unicode_minus': False\n",
    "})\n",
    "\n",
    "# 初始化存储结果的列表\n",
    "metrics_results = []\n",
    "execution_times = []\n",
    "cv_results = []\n",
    "param_analysis = []\n",
    "\n",
    "try:\n",
    "    # 加载之前保存的更新后RF_balanced模型\n",
    "    print(\"加载已保存的RF_balanced模型...\")\n",
    "    load_start_time = time.time()\n",
    "    model_info = joblib.load('rf_balanced_updated.pkl')\n",
    "    load_time = time.time() - load_start_time\n",
    "\n",
    "    # 获取模型和相关信息\n",
    "    rf_balanced_model = model_info['model']\n",
    "    class_weights = model_info['class_weights']\n",
    "    train_break_count = model_info['train_break_count']\n",
    "    train_break_ratio = model_info['train_break_ratio']\n",
    "    feature_names = model_info['feature_names']\n",
    "\n",
    "    print(f\"模型加载完成，耗时: {load_time:.2f}秒\")\n",
    "    print(f\"使用的类别权重: {class_weights}\")\n",
    "    print(f\"训练集断纱事件: {train_break_count} (占比: {train_break_ratio:.2%})\")\n",
    "\n",
    "    # 1. 交叉验证评估 - 输出所有指标\n",
    "    print(\"\\n=== 交叉验证评估 ===\")\n",
    "    cv_start_time = time.time()\n",
    "\n",
    "    # 使用分层K折交叉验证\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # 评估多个指标\n",
    "    scoring_metrics = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'precision': 'precision',\n",
    "        'recall': 'recall',\n",
    "        'f1': 'f1',\n",
    "        'roc_auc': 'roc_auc'\n",
    "    }\n",
    "\n",
    "    cv_scores = {}\n",
    "    cv_detailed_results = []\n",
    "\n",
    "    print(\"进行5折交叉验证，评估多个指标...\")\n",
    "    for metric_name, metric_scorer in scoring_metrics.items():\n",
    "        scores = cross_val_score(rf_balanced_model, X_train, y_train,\n",
    "                               cv=cv, scoring=metric_scorer, n_jobs=-1)\n",
    "        cv_scores[metric_name] = scores\n",
    "\n",
    "        # 存储详细结果\n",
    "        for fold_idx, score in enumerate(scores):\n",
    "            cv_detailed_results.append({\n",
    "                'Fold': fold_idx + 1,\n",
    "                'Metric': metric_name,\n",
    "                'Score': score\n",
    "            })\n",
    "\n",
    "        # 输出每个指标的统计信息\n",
    "        mean_score = np.mean(scores)\n",
    "        std_score = np.std(scores)\n",
    "        cv_results.append({\n",
    "            'Metric': metric_name,\n",
    "            'Mean_Score': mean_score,\n",
    "            'Std_Score': std_score,\n",
    "            'Scores': scores\n",
    "        })\n",
    "\n",
    "        print(f\"  {metric_name}: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "\n",
    "    cv_time = time.time() - cv_start_time\n",
    "    print(f\"交叉验证完成，耗时: {cv_time:.2f}秒\")\n",
    "\n",
    "    # 创建交叉验证详细结果DataFrame\n",
    "    cv_detailed_df = pd.DataFrame(cv_detailed_results)\n",
    "\n",
    "    # 打印每个折的详细结果\n",
    "    print(\"\\n交叉验证详细结果 (5折):\")\n",
    "    pivot_cv = cv_detailed_df.pivot_table(index='Fold', columns='Metric', values='Score')\n",
    "    print(pivot_cv.round(4))\n",
    "\n",
    "    # 打印总体统计\n",
    "    print(\"\\n交叉验证总体统计:\")\n",
    "    cv_df = pd.DataFrame(cv_results)\n",
    "    print(cv_df[['Metric', 'Mean_Score', 'Std_Score']].round(4))\n",
    "\n",
    "    # 2. 超参数优化\n",
    "    print(\"\\n=== 超参数优化 ===\")\n",
    "    param_search_start = time.time()\n",
    "\n",
    "    # 定义参数网格\n",
    "    param_dist = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30, 40],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # 使用随机搜索进行参数优化\n",
    "    random_search = RandomizedSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,  # 随机搜索的迭代次数\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"开始随机搜索参数优化...\")\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    param_search_time = time.time() - param_search_start\n",
    "    print(f\"参数优化完成，耗时: {param_search_time:.2f}秒\")\n",
    "\n",
    "    # 输出最佳参数\n",
    "    print(\"\\n最佳参数组合:\")\n",
    "    best_params = random_search.best_params_\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print(f\"最佳交叉验证分数 (F1): {random_search.best_score_:.4f}\")\n",
    "\n",
    "    # 3. 参数影响分析\n",
    "    print(\"\\n=== 参数影响分析 ===\")\n",
    "\n",
    "    # 分析不同参数对性能的影响\n",
    "    results_df = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "    # 分析主要参数的影响\n",
    "    key_params = ['param_n_estimators', 'param_max_depth',\n",
    "                 'param_min_samples_split', 'param_min_samples_leaf']\n",
    "\n",
    "    for param in key_params:\n",
    "        if param in results_df.columns:\n",
    "            param_data = results_df.groupby(param)['mean_test_score'].agg(['mean', 'std', 'count']).reset_index()\n",
    "            param_data = param_data.sort_values('mean', ascending=False)\n",
    "\n",
    "            print(f\"\\n{param} 对性能的影响:\")\n",
    "            for _, row in param_data.head(10).iterrows():\n",
    "                param_value = row[param]\n",
    "                if hasattr(param_value, '__len__') and not isinstance(param_value, str):\n",
    "                    param_value = str(param_value)\n",
    "                print(f\"  {param_value}: {row['mean']:.4f} ± {row['std']:.4f}\")\n",
    "\n",
    "            # 存储参数分析结果\n",
    "            param_analysis.extend([\n",
    "                {\n",
    "                    'Parameter': param.replace('param_', ''),\n",
    "                    'Value': row[param],\n",
    "                    'Mean_Score': row['mean'],\n",
    "                    'Std_Score': row['std'],\n",
    "                    'Count': row['count']\n",
    "                } for _, row in param_data.iterrows()\n",
    "            ])\n",
    "\n",
    "    # 4. 使用最佳参数训练新模型\n",
    "    print(\"\\n=== 使用最佳参数训练新模型 ===\")\n",
    "    training_start = time.time()\n",
    "\n",
    "    # 创建使用最佳参数的新模型\n",
    "    optimized_rf_model = RandomForestClassifier(\n",
    "        **best_params,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # 训练模型\n",
    "    optimized_rf_model.fit(X_train, y_train)\n",
    "    training_time = time.time() - training_start\n",
    "    print(f\"模型训练完成，耗时: {training_time:.2f}秒\")\n",
    "\n",
    "    # 5. 评估优化后的模型\n",
    "    print(\"\\n=== 优化后模型性能评估 ===\")\n",
    "    predict_start_time = time.time()\n",
    "\n",
    "    # 在训练集和测试集上进行预测\n",
    "    y_train_pred_opt = optimized_rf_model.predict(X_train)\n",
    "    y_test_pred_opt = optimized_rf_model.predict(X_test)\n",
    "\n",
    "    # 获取概率预测\n",
    "    y_train_pred_proba_opt = optimized_rf_model.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred_proba_opt = optimized_rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    predict_time = time.time() - predict_start_time\n",
    "    print(f\"预测完成，耗时: {predict_time:.2f}秒\")\n",
    "\n",
    "    # 计算训练集指标\n",
    "    train_accuracy_opt = accuracy_score(y_train, y_train_pred_opt)\n",
    "    train_precision_opt = precision_score(y_train, y_train_pred_opt, zero_division=0)\n",
    "    train_recall_opt = recall_score(y_train, y_train_pred_opt, zero_division=0)\n",
    "    train_f1_opt = f1_score(y_train, y_train_pred_opt, zero_division=0)\n",
    "    train_roc_auc_opt = roc_auc_score(y_train, y_train_pred_proba_opt)\n",
    "\n",
    "    # 计算测试集指标\n",
    "    test_accuracy_opt = accuracy_score(y_test, y_test_pred_opt)\n",
    "    test_precision_opt = precision_score(y_test, y_test_pred_opt, zero_division=0)\n",
    "    test_recall_opt = recall_score(y_test, y_test_pred_opt, zero_division=0)\n",
    "    test_f1_opt = f1_score(y_test, y_test_pred_opt, zero_division=0)\n",
    "    test_roc_auc_opt = roc_auc_score(y_test, y_test_pred_proba_opt)\n",
    "\n",
    "    # 计算混淆矩阵\n",
    "    cm_train_opt = confusion_matrix(y_train, y_train_pred_opt)\n",
    "    cm_test_opt = confusion_matrix(y_test, y_test_pred_opt)\n",
    "\n",
    "    total_time = load_time + cv_time + param_search_time + training_time + predict_time\n",
    "\n",
    "    # 6. 可视化混淆矩阵\n",
    "    print(\"\\n=== 生成混淆矩阵可视化 ===\")\n",
    "\n",
    "    # 创建训练集混淆矩阵热力图\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_train_opt, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['正常', '断纱'],\n",
    "                yticklabels=['正常', '断纱'])\n",
    "    plt.title('训练集混淆矩阵 - 优化后RF_balanced模型', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('预测标签', fontsize=12)\n",
    "    plt.ylabel('真实标签', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"训练集混淆矩阵已保存至: training_confusion_matrix.png\")\n",
    "\n",
    "    # 创建测试集混淆矩阵热力图\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_test_opt, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['正常', '断纱'],\n",
    "                yticklabels=['正常', '断纱'])\n",
    "    plt.title('测试集混淆矩阵 - 优化后RF_balanced模型', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('预测标签', fontsize=12)\n",
    "    plt.ylabel('真实标签', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('test_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"测试集混淆矩阵已保存至: test_confusion_matrix.png\")\n",
    "\n",
    "    # 7. 存储优化后模型的结果\n",
    "    metrics_results.append({\n",
    "        'Dataset': '优化后-训练集',\n",
    "        'Accuracy': train_accuracy_opt,\n",
    "        'Precision': train_precision_opt,\n",
    "        'Recall': train_recall_opt,\n",
    "        'F1-Score': train_f1_opt,\n",
    "        'ROC-AUC': train_roc_auc_opt,\n",
    "        'Parameters': str(best_params)\n",
    "    })\n",
    "\n",
    "    metrics_results.append({\n",
    "        'Dataset': '优化后-测试集',\n",
    "        'Accuracy': test_accuracy_opt,\n",
    "        'Precision': test_precision_opt,\n",
    "        'Recall': test_recall_opt,\n",
    "        'F1-Score': test_f1_opt,\n",
    "        'ROC-AUC': test_roc_auc_opt,\n",
    "        'Parameters': str(best_params)\n",
    "    })\n",
    "\n",
    "    execution_times.append({\n",
    "        '模型加载时间': load_time,\n",
    "        '交叉验证时间': cv_time,\n",
    "        '参数搜索时间': param_search_time,\n",
    "        '模型训练时间': training_time,\n",
    "        '预测时间': predict_time,\n",
    "        '总时间': total_time\n",
    "    })\n",
    "\n",
    "    # 8. 打印优化后模型结果\n",
    "    print(\"\\n优化后RF_balanced模型评估结果:\")\n",
    "    print(\"训练集性能:\")\n",
    "    print(f\"  准确率: {train_accuracy_opt:.4f}\")\n",
    "    print(f\"  精确率: {train_precision_opt:.4f}\")\n",
    "    print(f\"  召回率: {train_recall_opt:.4f}\")\n",
    "    print(f\"  F1分数: {train_f1_opt:.4f}\")\n",
    "    print(f\"  ROC-AUC: {train_roc_auc_opt:.4f}\")\n",
    "\n",
    "    print(\"\\n测试集性能:\")\n",
    "    print(f\"  准确率: {test_accuracy_opt:.4f}\")\n",
    "    print(f\"  精确率: {test_precision_opt:.4f}\")\n",
    "    print(f\"  召回率: {test_recall_opt:.4f}\")\n",
    "    print(f\"  F1分数: {test_f1_opt:.4f}\")\n",
    "    print(f\"  ROC-AUC: {test_roc_auc_opt:.4f}\")\n",
    "\n",
    "    print(\"\\n混淆矩阵:\")\n",
    "    print(\"训练集混淆矩阵:\")\n",
    "    print(cm_train_opt)\n",
    "    print(\"测试集混淆矩阵:\")\n",
    "    print(cm_test_opt)\n",
    "\n",
    "    print(\"\\n详细分类报告 - 训练集:\")\n",
    "    print(classification_report(y_train, y_train_pred_opt, target_names=['正常', '断纱']))\n",
    "\n",
    "    print(\"\\n详细分类报告 - 测试集:\")\n",
    "    print(classification_report(y_test, y_test_pred_opt, target_names=['正常', '断纱']))\n",
    "\n",
    "    # 9. 保存优化后的模型\n",
    "    print(\"\\n=== 保存优化后的模型 ===\")\n",
    "    optimized_model_info = {\n",
    "        'model': optimized_rf_model,\n",
    "        'best_params': best_params,\n",
    "        'class_weights': class_weights,\n",
    "        'train_break_count': train_break_count,\n",
    "        'train_break_ratio': train_break_ratio,\n",
    "        'feature_names': feature_names,\n",
    "        'cv_results': cv_results,\n",
    "        'param_analysis': param_analysis,\n",
    "        'optimization_time': param_search_time\n",
    "    }\n",
    "\n",
    "    # 更新模型文件\n",
    "    joblib.dump(optimized_model_info, 'rf_balanced_updated.pkl')\n",
    "    print(\"优化后的模型已更新至: rf_balanced_updated.pkl\")\n",
    "\n",
    "    # 同时保存一个优化版本的副本\n",
    "    joblib.dump(optimized_model_info, 'rf_balanced_optimized.pkl')\n",
    "    print(\"优化后的模型副本已保存至: rf_balanced_optimized.pkl\")\n",
    "\n",
    "    # 10. 参数趋势可视化\n",
    "    print(\"\\n=== 生成参数趋势可视化 ===\")\n",
    "\n",
    "    # 创建参数分析DataFrame\n",
    "    param_analysis_df = pd.DataFrame(param_analysis)\n",
    "\n",
    "    # 为每个参数创建趋势图\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # 颜色设置\n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "\n",
    "    # 参数显示名称映射\n",
    "    param_display_names = {\n",
    "        'n_estimators': '树的数量 (n_estimators)',\n",
    "        'max_depth': '最大深度 (max_depth)',\n",
    "        'min_samples_split': '分裂最小样本数 (min_samples_split)',\n",
    "        'min_samples_leaf': '叶节点最小样本数 (min_samples_leaf)'\n",
    "    }\n",
    "\n",
    "    for i, (param_name, display_name) in enumerate(param_display_names.items()):\n",
    "        param_data = param_analysis_df[param_analysis_df['Parameter'] == param_name]\n",
    "\n",
    "        if not param_data.empty:\n",
    "            # 确保数值类型排序\n",
    "            param_data = param_data.copy()\n",
    "\n",
    "            # 处理max_depth中的None值\n",
    "            if param_name == 'max_depth':\n",
    "                param_data['Sort_Key'] = param_data['Value'].apply(lambda x: 1000 if x is None else x)\n",
    "                param_data = param_data.sort_values('Sort_Key')\n",
    "                x_labels = [str(x) if x is not None else 'None' for x in param_data['Value']]\n",
    "            else:\n",
    "                param_data = param_data.sort_values('Value')\n",
    "                x_labels = [str(x) for x in param_data['Value']]\n",
    "\n",
    "            # 绘制趋势线\n",
    "            x_positions = range(len(param_data))\n",
    "            axes[i].plot(x_positions, param_data['Mean_Score'],\n",
    "                        'o-', linewidth=3, markersize=8, color=colors[i],\n",
    "                        label='平均F1分数', markerfacecolor='white', markeredgewidth=2)\n",
    "\n",
    "            # 添加误差线\n",
    "            axes[i].fill_between(x_positions,\n",
    "                               param_data['Mean_Score'] - param_data['Std_Score'],\n",
    "                               param_data['Mean_Score'] + param_data['Std_Score'],\n",
    "                               alpha=0.2, color=colors[i], label='标准差范围')\n",
    "\n",
    "            # 标记最佳值\n",
    "            best_idx = param_data['Mean_Score'].idxmax()\n",
    "            best_x = list(x_positions)[list(param_data.index).index(best_idx)]\n",
    "            best_score = param_data.loc[best_idx, 'Mean_Score']\n",
    "            best_value = param_data.loc[best_idx, 'Value']\n",
    "\n",
    "            axes[i].axvline(x=best_x, color='red', linestyle='--', alpha=0.8, linewidth=2)\n",
    "            axes[i].plot(best_x, best_score, 'o', markersize=10, color='red',\n",
    "                       label=f'最佳值: {best_value} (F1={best_score:.3f})')\n",
    "\n",
    "            # 设置图表属性\n",
    "            axes[i].set_title(f'{display_name} 对模型性能的影响', fontsize=14, fontweight='bold', pad=20)\n",
    "            axes[i].set_xlabel('参数值', fontsize=12)\n",
    "            axes[i].set_ylabel('F1分数', fontsize=12)\n",
    "            axes[i].set_xticks(x_positions)\n",
    "            axes[i].set_xticklabels(x_labels, rotation=45)\n",
    "            axes[i].legend(loc='lower right' if param_name in ['n_estimators', 'max_depth'] else 'upper right')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "            # 设置y轴范围，突出差异\n",
    "            y_min = max(0.8, param_data['Mean_Score'].min() - 0.05)\n",
    "            y_max = min(1.0, param_data['Mean_Score'].max() + 0.05)\n",
    "            axes[i].set_ylim(y_min, y_max)\n",
    "\n",
    "    plt.suptitle('随机森林参数对模型性能的影响趋势分析', fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('parameter_trend_analysis_detailed.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"详细参数趋势分析图已保存至: parameter_trend_analysis_detailed.png\")\n",
    "\n",
    "    # 11. 最终结果汇总\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RF_balanced模型优化总结\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 创建结果DataFrame\n",
    "    results_df_final = pd.DataFrame(metrics_results)\n",
    "    print(\"\\n模型性能总结:\")\n",
    "    print(results_df_final.round(4))\n",
    "\n",
    "    # 创建时间统计DataFrame\n",
    "    time_df = pd.DataFrame(execution_times)\n",
    "    print(\"\\n时间统计:\")\n",
    "    print(time_df.round(2))\n",
    "\n",
    "    # 保存所有结果到CSV文件\n",
    "    results_df_final.to_csv('RF_balanced_optimized_performance.csv', index=False)\n",
    "    time_df.to_csv('RF_balanced_optimized_timing.csv', index=False)\n",
    "    cv_df.to_csv('RF_balanced_cross_validation.csv', index=False)\n",
    "    param_analysis_df.to_csv('RF_balanced_parameter_analysis.csv', index=False)\n",
    "    cv_detailed_df.to_csv('RF_balanced_cross_validation_detailed.csv', index=False)\n",
    "\n",
    "    print(f\"\\n所有结果已保存至CSV文件\")\n",
    "\n",
    "    # 特征重要性分析\n",
    "    print(\"\\n=== 优化后模型特征重要性分析 ===\")\n",
    "    feature_importances_opt = optimized_rf_model.feature_importances_\n",
    "\n",
    "    # 创建特征重要性DataFrame\n",
    "    importance_df_opt = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances_opt\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    print(\"前10个最重要的特征:\")\n",
    "    print(importance_df_opt.head(10).round(4))\n",
    "\n",
    "    # 保存特征重要性\n",
    "    importance_df_opt.to_csv('RF_balanced_optimized_feature_importance.csv', index=False)\n",
    "    print(f\"特征重要性已保存至: RF_balanced_optimized_feature_importance.csv\")\n",
    "\n",
    "    print(\"\\n优化流程完成！\")\n",
    "    print(f\"总执行时间: {total_time:.2f}秒\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"错误: 未找到保存的RF_balanced模型文件 'rf_balanced_updated.pkl'\")\n",
    "    print(\"请先运行模型创建和保存代码\")\n",
    "except Exception as e:\n",
    "    print(f\"模型加载或评估过程中出错: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n所有评估和优化完成！\")"
   ],
   "id": "d7c0322c021d8b76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# 设置绘图样式 - 英文专业格式\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.linewidth': 2.5,\n",
    "    'font.size': 16,\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 17,\n",
    "    'xtick.labelsize': 15,\n",
    "    'ytick.labelsize': 15,\n",
    "    'legend.fontsize': 15,\n",
    "    'grid.linewidth': 1.2,\n",
    "    'lines.linewidth': 3.5,\n",
    "    'figure.dpi': 1200,\n",
    "    'savefig.dpi': 1200,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.unicode_minus': False\n",
    "})\n",
    "\n",
    "print(\"=== RF_balanced Model Hyperparameter Tuning and Performance Evaluation ===\")\n",
    "print(\"Includes Cross-Validation and Hyperparameter Optimization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize lists to store results\n",
    "metrics_results = []\n",
    "execution_times = []\n",
    "cv_results = []\n",
    "param_analysis = []\n",
    "\n",
    "try:\n",
    "    # Load the previously saved updated RF_balanced model\n",
    "    print(\"Loading saved RF_balanced model...\")\n",
    "    load_start_time = time.time()\n",
    "    model_info = joblib.load('rf_balanced_updated.pkl')\n",
    "    load_time = time.time() - load_start_time\n",
    "\n",
    "    # Get model and related information\n",
    "    rf_balanced_model = model_info['model']\n",
    "    class_weights = model_info['class_weights']\n",
    "    train_break_count = model_info['train_break_count']\n",
    "    train_break_ratio = model_info['train_break_ratio']\n",
    "    feature_names = model_info['feature_names']\n",
    "\n",
    "    print(f\"Model loaded successfully, time: {load_time:.2f} seconds\")\n",
    "    print(f\"Class weights used: {class_weights}\")\n",
    "    print(f\"Training set yarn break events: {train_break_count} (ratio: {train_break_ratio:.2%})\")\n",
    "\n",
    "    # 1. Cross-validation evaluation - Output all metrics\n",
    "    print(\"\\n=== Cross-Validation Evaluation ===\")\n",
    "    cv_start_time = time.time()\n",
    "\n",
    "    # Use stratified K-fold cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Evaluate multiple metrics\n",
    "    scoring_metrics = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'precision': 'precision',\n",
    "        'recall': 'recall',\n",
    "        'f1': 'f1',\n",
    "        'roc_auc': 'roc_auc'\n",
    "    }\n",
    "\n",
    "    cv_scores = {}\n",
    "    cv_detailed_results = []\n",
    "\n",
    "    print(\"Performing 5-fold cross-validation, evaluating multiple metrics...\")\n",
    "    for metric_name, metric_scorer in scoring_metrics.items():\n",
    "        scores = cross_val_score(rf_balanced_model, X_train, y_train,\n",
    "                               cv=cv, scoring=metric_scorer, n_jobs=-1)\n",
    "        cv_scores[metric_name] = scores\n",
    "\n",
    "        # Store detailed results\n",
    "        for fold_idx, score in enumerate(scores):\n",
    "            cv_detailed_results.append({\n",
    "                'Fold': fold_idx + 1,\n",
    "                'Metric': metric_name,\n",
    "                'Score': score\n",
    "            })\n",
    "\n",
    "        # Output statistics for each metric\n",
    "        mean_score = np.mean(scores)\n",
    "        std_score = np.std(scores)\n",
    "        cv_results.append({\n",
    "            'Metric': metric_name,\n",
    "            'Mean_Score': mean_score,\n",
    "            'Std_Score': std_score,\n",
    "            'Scores': scores\n",
    "        })\n",
    "\n",
    "        print(f\"  {metric_name}: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "\n",
    "    cv_time = time.time() - cv_start_time\n",
    "    print(f\"Cross-validation completed, time: {cv_time:.2f} seconds\")\n",
    "\n",
    "    # Create cross-validation detailed results DataFrame\n",
    "    cv_detailed_df = pd.DataFrame(cv_detailed_results)\n",
    "\n",
    "    # Print detailed results for each fold\n",
    "    print(\"\\nCross-validation detailed results (5 folds):\")\n",
    "    pivot_cv = cv_detailed_df.pivot_table(index='Fold', columns='Metric', values='Score')\n",
    "    print(pivot_cv.round(4))\n",
    "\n",
    "    # Print overall statistics\n",
    "    print(\"\\nCross-validation overall statistics:\")\n",
    "    cv_df = pd.DataFrame(cv_results)\n",
    "    print(cv_df[['Metric', 'Mean_Score', 'Std_Score']].round(4))\n",
    "\n",
    "    # 2. Hyperparameter optimization\n",
    "    print(\"\\n=== Hyperparameter Optimization ===\")\n",
    "    param_search_start = time.time()\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_dist = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30, 40],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Use randomized search for parameter optimization\n",
    "    random_search = RandomizedSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,  # Number of iterations for random search\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Starting randomized search parameter optimization...\")\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    param_search_time = time.time() - param_search_start\n",
    "    print(f\"Parameter optimization completed, time: {param_search_time:.2f} seconds\")\n",
    "\n",
    "    # Output best parameters\n",
    "    print(\"\\nBest parameter combination:\")\n",
    "    best_params = random_search.best_params_\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print(f\"Best cross-validation score (F1): {random_search.best_score_:.4f}\")\n",
    "\n",
    "    # 3. Parameter impact analysis\n",
    "    print(\"\\n=== Parameter Impact Analysis ===\")\n",
    "\n",
    "    # Analyze the impact of different parameters on performance\n",
    "    results_df = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "    # Analyze impact of main parameters\n",
    "    key_params = ['param_n_estimators', 'param_max_depth',\n",
    "                 'param_min_samples_split', 'param_min_samples_leaf']\n",
    "\n",
    "    for param in key_params:\n",
    "        if param in results_df.columns:\n",
    "            param_data = results_df.groupby(param)['mean_test_score'].agg(['mean', 'std', 'count']).reset_index()\n",
    "            param_data = param_data.sort_values('mean', ascending=False)\n",
    "\n",
    "            print(f\"\\nImpact of {param} on performance:\")\n",
    "            for _, row in param_data.head(10).iterrows():\n",
    "                param_value = row[param]\n",
    "                if hasattr(param_value, '__len__') and not isinstance(param_value, str):\n",
    "                    param_value = str(param_value)\n",
    "                print(f\"  {param_value}: {row['mean']:.4f} ± {row['std']:.4f}\")\n",
    "\n",
    "            # Store parameter analysis results\n",
    "            param_analysis.extend([\n",
    "                {\n",
    "                    'Parameter': param.replace('param_', ''),\n",
    "                    'Value': row[param],\n",
    "                    'Mean_Score': row['mean'],\n",
    "                    'Std_Score': row['std'],\n",
    "                    'Count': row['count']\n",
    "                } for _, row in param_data.iterrows()\n",
    "            ])\n",
    "\n",
    "    # 4. Train new model with best parameters\n",
    "    print(\"\\n=== Training New Model with Best Parameters ===\")\n",
    "    training_start = time.time()\n",
    "\n",
    "    # Create new model with best parameters\n",
    "    optimized_rf_model = RandomForestClassifier(\n",
    "        **best_params,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    optimized_rf_model.fit(X_train, y_train)\n",
    "    training_time = time.time() - training_start\n",
    "    print(f\"Model training completed, time: {training_time:.2f} seconds\")\n",
    "\n",
    "    # 5. Evaluate the optimized model\n",
    "    print(\"\\n=== Optimized Model Performance Evaluation ===\")\n",
    "    predict_start_time = time.time()\n",
    "\n",
    "    # Make predictions on training and test sets\n",
    "    y_train_pred_opt = optimized_rf_model.predict(X_train)\n",
    "    y_test_pred_opt = optimized_rf_model.predict(X_test)\n",
    "\n",
    "    # Get probability predictions\n",
    "    y_train_pred_proba_opt = optimized_rf_model.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred_proba_opt = optimized_rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    predict_time = time.time() - predict_start_time\n",
    "    print(f\"Prediction completed, time: {predict_time:.2f} seconds\")\n",
    "\n",
    "    # Calculate training set metrics\n",
    "    train_accuracy_opt = accuracy_score(y_train, y_train_pred_opt)\n",
    "    train_precision_opt = precision_score(y_train, y_train_pred_opt, zero_division=0)\n",
    "    train_recall_opt = recall_score(y_train, y_train_pred_opt, zero_division=0)\n",
    "    train_f1_opt = f1_score(y_train, y_train_pred_opt, zero_division=0)\n",
    "    train_roc_auc_opt = roc_auc_score(y_train, y_train_pred_proba_opt)\n",
    "\n",
    "    # Calculate test set metrics\n",
    "    test_accuracy_opt = accuracy_score(y_test, y_test_pred_opt)\n",
    "    test_precision_opt = precision_score(y_test, y_test_pred_opt, zero_division=0)\n",
    "    test_recall_opt = recall_score(y_test, y_test_pred_opt, zero_division=0)\n",
    "    test_f1_opt = f1_score(y_test, y_test_pred_opt, zero_division=0)\n",
    "    test_roc_auc_opt = roc_auc_score(y_test, y_test_pred_proba_opt)\n",
    "\n",
    "    # Calculate confusion matrices\n",
    "    cm_train_opt = confusion_matrix(y_train, y_train_pred_opt)\n",
    "    cm_test_opt = confusion_matrix(y_test, y_test_pred_opt)\n",
    "\n",
    "    total_time = load_time + cv_time + param_search_time + training_time + predict_time\n",
    "\n",
    "    # 6. Visualize confusion matrices with enhanced styling\n",
    "    print(\"\\n=== Generating Confusion Matrix Visualizations ===\")\n",
    "\n",
    "    # Create training set confusion matrix heatmap with enhanced styling\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_train_opt, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Yarn Break'],\n",
    "                yticklabels=['Normal', 'Yarn Break'],\n",
    "                annot_kws={'size': 16, 'weight': 'bold'},  # Enlarge and bold annotation font\n",
    "                linewidths=2, linecolor='black')  # Bold cell borders\n",
    "\n",
    "    plt.title('Training Set Confusion Matrix - Optimized RF_balanced Model',\n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Bold the entire plot border\n",
    "    ax = plt.gca()\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Training set confusion matrix saved to: training_confusion_matrix.png\")\n",
    "\n",
    "    # Create test set confusion matrix heatmap with enhanced styling\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_test_opt, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Yarn Break'],\n",
    "                yticklabels=['Normal', 'Yarn Break'],\n",
    "                annot_kws={'size': 16, 'weight': 'bold'},  # Enlarge and bold annotation font\n",
    "                linewidths=2, linecolor='black')  # Bold cell borders\n",
    "\n",
    "    plt.title('Test Set Confusion Matrix - Optimized RF_balanced Model',\n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Bold the entire plot border\n",
    "    ax = plt.gca()\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('test_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Test set confusion matrix saved to: test_confusion_matrix.png\")\n",
    "\n",
    "    # 7. Store optimized model results\n",
    "    metrics_results.append({\n",
    "        'Dataset': 'Optimized-Training Set',\n",
    "        'Accuracy': train_accuracy_opt,\n",
    "        'Precision': train_precision_opt,\n",
    "        'Recall': train_recall_opt,\n",
    "        'F1-Score': train_f1_opt,\n",
    "        'ROC-AUC': train_roc_auc_opt,\n",
    "        'Parameters': str(best_params)\n",
    "    })\n",
    "\n",
    "    metrics_results.append({\n",
    "        'Dataset': 'Optimized-Test Set',\n",
    "        'Accuracy': test_accuracy_opt,\n",
    "        'Precision': test_precision_opt,\n",
    "        'Recall': test_recall_opt,\n",
    "        'F1-Score': test_f1_opt,\n",
    "        'ROC-AUC': test_roc_auc_opt,\n",
    "        'Parameters': str(best_params)\n",
    "    })\n",
    "\n",
    "    execution_times.append({\n",
    "        'Model Loading Time': load_time,\n",
    "        'Cross-Validation Time': cv_time,\n",
    "        'Parameter Search Time': param_search_time,\n",
    "        'Model Training Time': training_time,\n",
    "        'Prediction Time': predict_time,\n",
    "        'Total Time': total_time\n",
    "    })\n",
    "\n",
    "    # 8. Print optimized model results\n",
    "    print(\"\\nOptimized RF_balanced Model Evaluation Results:\")\n",
    "    print(\"Training Set Performance:\")\n",
    "    print(f\"  Accuracy: {train_accuracy_opt:.4f}\")\n",
    "    print(f\"  Precision: {train_precision_opt:.4f}\")\n",
    "    print(f\"  Recall: {train_recall_opt:.4f}\")\n",
    "    print(f\"  F1-Score: {train_f1_opt:.4f}\")\n",
    "    print(f\"  ROC-AUC: {train_roc_auc_opt:.4f}\")\n",
    "\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(f\"  Accuracy: {test_accuracy_opt:.4f}\")\n",
    "    print(f\"  Precision: {test_precision_opt:.4f}\")\n",
    "    print(f\"  Recall: {test_recall_opt:.4f}\")\n",
    "    print(f\"  F1-Score: {test_f1_opt:.4f}\")\n",
    "    print(f\"  ROC-AUC: {test_roc_auc_opt:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrices:\")\n",
    "    print(\"Training Set Confusion Matrix:\")\n",
    "    print(cm_train_opt)\n",
    "    print(\"Test Set Confusion Matrix:\")\n",
    "    print(cm_test_opt)\n",
    "\n",
    "    print(\"\\nDetailed Classification Report - Training Set:\")\n",
    "    print(classification_report(y_train, y_train_pred_opt, target_names=['Normal', 'Yarn Break']))\n",
    "\n",
    "    print(\"\\nDetailed Classification Report - Test Set:\")\n",
    "    print(classification_report(y_test, y_test_pred_opt, target_names=['Normal', 'Yarn Break']))\n",
    "\n",
    "    # 9. Save the optimized model\n",
    "    print(\"\\n=== Saving Optimized Model ===\")\n",
    "    optimized_model_info = {\n",
    "        'model': optimized_rf_model,\n",
    "        'best_params': best_params,\n",
    "        'class_weights': class_weights,\n",
    "        'train_break_count': train_break_count,\n",
    "        'train_break_ratio': train_break_ratio,\n",
    "        'feature_names': feature_names,\n",
    "        'cv_results': cv_results,\n",
    "        'param_analysis': param_analysis,\n",
    "        'optimization_time': param_search_time\n",
    "    }\n",
    "\n",
    "    # Update model file\n",
    "    joblib.dump(optimized_model_info, 'rf_balanced_updated.pkl')\n",
    "    print(\"Optimized model updated to: rf_balanced_updated.pkl\")\n",
    "\n",
    "    # Also save an optimized version copy\n",
    "    joblib.dump(optimized_model_info, 'rf_balanced_optimized.pkl')\n",
    "    print(\"Optimized model copy saved to: rf_balanced_optimized.pkl\")\n",
    "\n",
    "    # 10. Parameter trend visualization\n",
    "    print(\"\\n=== Generating Parameter Trend Visualization ===\")\n",
    "\n",
    "    # Create parameter analysis DataFrame\n",
    "    param_analysis_df = pd.DataFrame(param_analysis)\n",
    "\n",
    "    # Create trend charts for each parameter\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # Color settings\n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "\n",
    "    # Parameter display name mapping\n",
    "    param_display_names = {\n",
    "        'n_estimators': 'Number of Trees (n_estimators)',\n",
    "        'max_depth': 'Maximum Depth (max_depth)',\n",
    "        'min_samples_split': 'Minimum Samples Split (min_samples_split)',\n",
    "        'min_samples_leaf': 'Minimum Samples Leaf (min_samples_leaf)'\n",
    "    }\n",
    "\n",
    "    for i, (param_name, display_name) in enumerate(param_display_names.items()):\n",
    "        param_data = param_analysis_df[param_analysis_df['Parameter'] == param_name]\n",
    "\n",
    "        if not param_data.empty:\n",
    "            # Ensure numeric type sorting\n",
    "            param_data = param_data.copy()\n",
    "\n",
    "            # Handle None values in max_depth\n",
    "            if param_name == 'max_depth':\n",
    "                param_data['Sort_Key'] = param_data['Value'].apply(lambda x: 1000 if x is None else x)\n",
    "                param_data = param_data.sort_values('Sort_Key')\n",
    "                x_labels = [str(x) if x is not None else 'None' for x in param_data['Value']]\n",
    "            else:\n",
    "                param_data = param_data.sort_values('Value')\n",
    "                x_labels = [str(x) for x in param_data['Value']]\n",
    "\n",
    "            # Plot trend line\n",
    "            x_positions = range(len(param_data))\n",
    "            axes[i].plot(x_positions, param_data['Mean_Score'],\n",
    "                        'o-', linewidth=3, markersize=8, color=colors[i],\n",
    "                        label='Mean F1-Score', markerfacecolor='white', markeredgewidth=2)\n",
    "\n",
    "            # Add error bars\n",
    "            axes[i].fill_between(x_positions,\n",
    "                               param_data['Mean_Score'] - param_data['Std_Score'],\n",
    "                               param_data['Mean_Score'] + param_data['Std_Score'],\n",
    "                               alpha=0.2, color=colors[i], label='Standard Deviation Range')\n",
    "\n",
    "            # Mark best value\n",
    "            best_idx = param_data['Mean_Score'].idxmax()\n",
    "            best_x = list(x_positions)[list(param_data.index).index(best_idx)]\n",
    "            best_score = param_data.loc[best_idx, 'Mean_Score']\n",
    "            best_value = param_data.loc[best_idx, 'Value']\n",
    "\n",
    "            axes[i].axvline(x=best_x, color='red', linestyle='--', alpha=0.8, linewidth=2)\n",
    "            axes[i].plot(best_x, best_score, 'o', markersize=10, color='red',\n",
    "                       label=f'Best Value: {best_value} (F1={best_score:.3f})')\n",
    "\n",
    "            # Set chart properties\n",
    "            axes[i].set_title(f'Impact of {display_name} on Model Performance',\n",
    "                             fontsize=18, fontweight='bold', pad=20)\n",
    "            axes[i].set_xlabel('Parameter Value', fontsize=18, fontweight='bold')\n",
    "            axes[i].set_ylabel('F1-Score', fontsize=18, fontweight='bold')\n",
    "            axes[i].set_xticks(x_positions)\n",
    "            axes[i].set_xticklabels(x_labels, rotation=45)\n",
    "            axes[i].legend(loc='lower right' if param_name in ['n_estimators', 'max_depth'] else 'upper right')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "            # Set y-axis range to highlight differences\n",
    "            y_min = max(0.8, param_data['Mean_Score'].min() - 0.05)\n",
    "            y_max = min(1.0, param_data['Mean_Score'].max() + 0.05)\n",
    "            axes[i].set_ylim(y_min, y_max)\n",
    "\n",
    "    plt.suptitle('Random Forest Parameter Impact on Model Performance Trend Analysis',\n",
    "                 fontsize=18, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('parameter_trend_analysis_detailed.png', dpi=1200, bbox_inches='tight')\n",
    "    print(\"Detailed parameter trend analysis chart saved to: parameter_trend_analysis_detailed.png\")\n",
    "\n",
    "    # 11. Final results summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RF_balanced Model Optimization Summary\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results_df_final = pd.DataFrame(metrics_results)\n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(results_df_final.round(4))\n",
    "\n",
    "    # Create time statistics DataFrame\n",
    "    time_df = pd.DataFrame(execution_times)\n",
    "    print(\"\\nTime Statistics:\")\n",
    "    print(time_df.round(2))\n",
    "\n",
    "    # Save all results to CSV files\n",
    "    results_df_final.to_csv('RF_balanced_optimized_performance.csv', index=False)\n",
    "    time_df.to_csv('RF_balanced_optimized_timing.csv', index=False)\n",
    "    cv_df.to_csv('RF_balanced_cross_validation.csv', index=False)\n",
    "    param_analysis_df.to_csv('RF_balanced_parameter_analysis.csv', index=False)\n",
    "    cv_detailed_df.to_csv('RF_balanced_cross_validation_detailed.csv', index=False)\n",
    "\n",
    "    print(f\"\\nAll results saved to CSV files\")\n",
    "\n",
    "    # Feature importance analysis\n",
    "    print(\"\\n=== Optimized Model Feature Importance Analysis ===\")\n",
    "    feature_importances_opt = optimized_rf_model.feature_importances_\n",
    "\n",
    "    # Create feature importance DataFrame\n",
    "    importance_df_opt = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances_opt\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(importance_df_opt.head(10).round(4))\n",
    "\n",
    "    # Save feature importance\n",
    "    importance_df_opt.to_csv('RF_balanced_optimized_feature_importance.csv', index=False)\n",
    "    print(f\"Feature importance saved to: RF_balanced_optimized_feature_importance.csv\")\n",
    "\n",
    "    print(\"\\nOptimization process completed!\")\n",
    "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Saved RF_balanced model file 'rf_balanced_updated.pkl' not found\")\n",
    "    print(\"Please run the model creation and saving code first\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model loading or evaluation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nAll evaluation and optimization completed!\")"
   ],
   "id": "e3eeb74f2f9d7e13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d8c5bd1fd1500c06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# 设置绘图样式 - 英文专业格式\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# 设置绘图样式 - 英文专业格式\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.linewidth': 2.5,\n",
    "    'font.size': 16,\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 17,\n",
    "    'xtick.labelsize': 15,\n",
    "    'ytick.labelsize': 15,\n",
    "    'legend.fontsize': 15,\n",
    "    'grid.linewidth': 1.2,\n",
    "    'lines.linewidth': 3.5,\n",
    "    'figure.dpi': 1200,\n",
    "    'savefig.dpi': 1200,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.unicode_minus': False\n",
    "})\n",
    "\n",
    "print(\"=== RF_balanced Model Hyperparameter Tuning and Performance Evaluation ===\")\n",
    "print(\"Includes Cross-Validation and Hyperparameter Optimization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize lists to store results\n",
    "metrics_results = []\n",
    "execution_times = []\n",
    "cv_results = []\n",
    "param_analysis = []\n",
    "\n",
    "try:\n",
    "    # Load the previously saved updated RF_balanced model\n",
    "    print(\"Loading saved RF_balanced model...\")\n",
    "    load_start_time = time.time()\n",
    "    model_info = joblib.load('rf_balanced_updated.pkl')\n",
    "    load_time = time.time() - load_start_time\n",
    "\n",
    "    # Get model and related information\n",
    "    rf_balanced_model = model_info['model']\n",
    "    class_weights = model_info['class_weights']\n",
    "    train_break_count = model_info['train_break_count']\n",
    "    train_break_ratio = model_info['train_break_ratio']\n",
    "    feature_names = model_info['feature_names']\n",
    "\n",
    "    print(f\"Model loaded successfully, time: {load_time:.2f} seconds\")\n",
    "    print(f\"Class weights used: {class_weights}\")\n",
    "    print(f\"Training set yarn break events: {train_break_count} (ratio: {train_break_ratio:.2%})\")\n",
    "\n",
    "    # 1. Cross-validation evaluation - Output all metrics\n",
    "    print(\"\\n=== Cross-Validation Evaluation ===\")\n",
    "    cv_start_time = time.time()\n",
    "\n",
    "    # Use stratified K-fold cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Evaluate multiple metrics\n",
    "    scoring_metrics = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'precision': 'precision',\n",
    "        'recall': 'recall',\n",
    "        'f1': 'f1',\n",
    "        'roc_auc': 'roc_auc'\n",
    "    }\n",
    "\n",
    "    cv_scores = {}\n",
    "    cv_detailed_results = []\n",
    "\n",
    "    print(\"Performing 5-fold cross-validation, evaluating multiple metrics...\")\n",
    "    for metric_name, metric_scorer in scoring_metrics.items():\n",
    "        scores = cross_val_score(rf_balanced_model, X_train, y_train,\n",
    "                               cv=cv, scoring=metric_scorer, n_jobs=-1)\n",
    "        cv_scores[metric_name] = scores\n",
    "\n",
    "        # Store detailed results\n",
    "        for fold_idx, score in enumerate(scores):\n",
    "            cv_detailed_results.append({\n",
    "                'Fold': fold_idx + 1,\n",
    "                'Metric': metric_name,\n",
    "                'Score': score\n",
    "            })\n",
    "\n",
    "        # Output statistics for each metric\n",
    "        mean_score = np.mean(scores)\n",
    "        std_score = np.std(scores)\n",
    "        cv_results.append({\n",
    "            'Metric': metric_name,\n",
    "            'Mean_Score': mean_score,\n",
    "            'Std_Score': std_score,\n",
    "            'Scores': scores\n",
    "        })\n",
    "\n",
    "        print(f\"  {metric_name}: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "\n",
    "    cv_time = time.time() - cv_start_time\n",
    "    print(f\"Cross-validation completed, time: {cv_time:.2f} seconds\")\n",
    "\n",
    "    # Create cross-validation detailed results DataFrame\n",
    "    cv_detailed_df = pd.DataFrame(cv_detailed_results)\n",
    "\n",
    "    # Print detailed results for each fold\n",
    "    print(\"\\nCross-validation detailed results (5 folds):\")\n",
    "    pivot_cv = cv_detailed_df.pivot_table(index='Fold', columns='Metric', values='Score')\n",
    "    print(pivot_cv.round(4))\n",
    "\n",
    "    # Print overall statistics\n",
    "    print(\"\\nCross-validation overall statistics:\")\n",
    "    cv_df = pd.DataFrame(cv_results)\n",
    "    print(cv_df[['Metric', 'Mean_Score', 'Std_Score']].round(4))\n",
    "\n",
    "    # 2. Hyperparameter optimization\n",
    "    print(\"\\n=== Hyperparameter Optimization ===\")\n",
    "    param_search_start = time.time()\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_dist = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30, 40],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Use randomized search for parameter optimization\n",
    "    random_search = RandomizedSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,  # Number of iterations for random search\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Starting randomized search parameter optimization...\")\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    param_search_time = time.time() - param_search_start\n",
    "    print(f\"Parameter optimization completed, time: {param_search_time:.2f} seconds\")\n",
    "\n",
    "    # Output best parameters\n",
    "    print(\"\\nBest parameter combination:\")\n",
    "    best_params = random_search.best_params_\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print(f\"Best cross-validation score (F1): {random_search.best_score_:.4f}\")\n",
    "\n",
    "    # 3. Parameter impact analysis\n",
    "    print(\"\\n=== Parameter Impact Analysis ===\")\n",
    "\n",
    "    # Analyze the impact of different parameters on performance\n",
    "    results_df = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "    # Analyze impact of main parameters\n",
    "    key_params = ['param_n_estimators', 'param_max_depth',\n",
    "                 'param_min_samples_split', 'param_min_samples_leaf']\n",
    "\n",
    "    for param in key_params:\n",
    "        if param in results_df.columns:\n",
    "            param_data = results_df.groupby(param)['mean_test_score'].agg(['mean', 'std', 'count']).reset_index()\n",
    "            param_data = param_data.sort_values('mean', ascending=False)\n",
    "\n",
    "            print(f\"\\nImpact of {param} on performance:\")\n",
    "            for _, row in param_data.head(10).iterrows():\n",
    "                param_value = row[param]\n",
    "                if hasattr(param_value, '__len__') and not isinstance(param_value, str):\n",
    "                    param_value = str(param_value)\n",
    "                print(f\"  {param_value}: {row['mean']:.4f} ± {row['std']:.4f}\")\n",
    "\n",
    "            # Store parameter analysis results\n",
    "            param_analysis.extend([\n",
    "                {\n",
    "                    'Parameter': param.replace('param_', ''),\n",
    "                    'Value': row[param],\n",
    "                    'Mean_Score': row['mean'],\n",
    "                    'Std_Score': row['std'],\n",
    "                    'Count': row['count']\n",
    "                } for _, row in param_data.iterrows()\n",
    "            ])\n",
    "\n",
    "    # 4. Train new model with best parameters\n",
    "    print(\"\\n=== Training New Model with Best Parameters ===\")\n",
    "    training_start = time.time()\n",
    "\n",
    "    # Create new model with best parameters\n",
    "    optimized_rf_model = RandomForestClassifier(\n",
    "        **best_params,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    optimized_rf_model.fit(X_train, y_train)\n",
    "    training_time = time.time() - training_start\n",
    "    print(f\"Model training completed, time: {training_time:.2f} seconds\")\n",
    "\n",
    "    # 5. Evaluate the optimized model\n",
    "    print(\"\\n=== Optimized Model Performance Evaluation ===\")\n",
    "    predict_start_time = time.time()\n",
    "\n",
    "    # Make predictions on training and test sets\n",
    "    y_train_pred_opt = optimized_rf_model.predict(X_train)\n",
    "    y_test_pred_opt = optimized_rf_model.predict(X_test)\n",
    "\n",
    "    # Get probability predictions\n",
    "    y_train_pred_proba_opt = optimized_rf_model.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred_proba_opt = optimized_rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    predict_time = time.time() - predict_start_time\n",
    "    print(f\"Prediction completed, time: {predict_time:.2f} seconds\")\n",
    "\n",
    "    # Calculate training set metrics\n",
    "    train_accuracy_opt = accuracy_score(y_train, y_train_pred_opt)\n",
    "    train_precision_opt = precision_score(y_train, y_train_pred_opt, zero_division=0)\n",
    "    train_recall_opt = recall_score(y_train, y_train_pred_opt, zero_division=0)\n",
    "    train_f1_opt = f1_score(y_train, y_train_pred_opt, zero_division=0)\n",
    "    train_roc_auc_opt = roc_auc_score(y_train, y_train_pred_proba_opt)\n",
    "\n",
    "    # Calculate test set metrics\n",
    "    test_accuracy_opt = accuracy_score(y_test, y_test_pred_opt)\n",
    "    test_precision_opt = precision_score(y_test, y_test_pred_opt, zero_division=0)\n",
    "    test_recall_opt = recall_score(y_test, y_test_pred_opt, zero_division=0)\n",
    "    test_f1_opt = f1_score(y_test, y_test_pred_opt, zero_division=0)\n",
    "    test_roc_auc_opt = roc_auc_score(y_test, y_test_pred_proba_opt)\n",
    "\n",
    "    # Calculate confusion matrices\n",
    "    cm_train_opt = confusion_matrix(y_train, y_train_pred_opt)\n",
    "    cm_test_opt = confusion_matrix(y_test, y_test_pred_opt)\n",
    "\n",
    "    total_time = load_time + cv_time + param_search_time + training_time + predict_time\n",
    "\n",
    "    # 6. Visualize confusion matrices in one combined figure with enhanced styling\n",
    "    print(\"\\n=== Generating Combined Confusion Matrix Visualization ===\")\n",
    "\n",
    "    # Create a combined figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    # Training set confusion matrix\n",
    "    sns.heatmap(cm_train_opt, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Yarn Break'],\n",
    "                yticklabels=['Normal', 'Yarn Break'],\n",
    "                annot_kws={'size': 18, 'weight': 'bold'},\n",
    "                linewidths=2, linecolor='black', ax=ax1)\n",
    "    ax1.set_title('Training Set Confusion Matrix\\nOptimized RF_balanced Model',\n",
    "                 fontsize=18, fontweight='bold', pad=20)\n",
    "    ax1.set_xlabel('Predicted Label', fontsize=18, fontweight='bold')\n",
    "    ax1.set_ylabel('True Label', fontsize=18, fontweight='bold')\n",
    "\n",
    "    # Test set confusion matrix\n",
    "    sns.heatmap(cm_test_opt, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Yarn Break'],\n",
    "                yticklabels=['Normal', 'Yarn Break'],\n",
    "                annot_kws={'size': 18, 'weight': 'bold'},\n",
    "                linewidths=2, linecolor='black', ax=ax2)\n",
    "    ax2.set_title('Test Set Confusion Matrix\\nOptimized RF_balanced Model',\n",
    "                 fontsize=18, fontweight='bold', pad=20)\n",
    "    ax2.set_xlabel('Predicted Label', fontsize=18, fontweight='bold')\n",
    "    ax2.set_ylabel('True Label', fontsize=18, fontweight='bold')\n",
    "\n",
    "    # Bold the borders for both subplots\n",
    "    for ax in [ax1, ax2]:\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(3)\n",
    "\n",
    "    plt.suptitle('Optimized RF_balanced Model - Confusion Matrix Analysis',\n",
    "                fontsize=18, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('combined_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Combined confusion matrix saved to: combined_confusion_matrix.png\")\n",
    "\n",
    "    # 7. Store optimized model results\n",
    "    metrics_results.append({\n",
    "        'Dataset': 'Optimized-Training Set',\n",
    "        'Accuracy': train_accuracy_opt,\n",
    "        'Precision': train_precision_opt,\n",
    "        'Recall': train_recall_opt,\n",
    "        'F1-Score': train_f1_opt,\n",
    "        'ROC-AUC': train_roc_auc_opt,\n",
    "        'Parameters': str(best_params)\n",
    "    })\n",
    "\n",
    "    metrics_results.append({\n",
    "        'Dataset': 'Optimized-Test Set',\n",
    "        'Accuracy': test_accuracy_opt,\n",
    "        'Precision': test_precision_opt,\n",
    "        'Recall': test_recall_opt,\n",
    "        'F1-Score': test_f1_opt,\n",
    "        'ROC-AUC': test_roc_auc_opt,\n",
    "        'Parameters': str(best_params)\n",
    "    })\n",
    "\n",
    "    execution_times.append({\n",
    "        'Model Loading Time': load_time,\n",
    "        'Cross-Validation Time': cv_time,\n",
    "        'Parameter Search Time': param_search_time,\n",
    "        'Model Training Time': training_time,\n",
    "        'Prediction Time': predict_time,\n",
    "        'Total Time': total_time\n",
    "    })\n",
    "\n",
    "    # 8. Print optimized model results\n",
    "    print(\"\\nOptimized RF_balanced Model Evaluation Results:\")\n",
    "    print(\"Training Set Performance:\")\n",
    "    print(f\"  Accuracy: {train_accuracy_opt:.4f}\")\n",
    "    print(f\"  Precision: {train_precision_opt:.4f}\")\n",
    "    print(f\"  Recall: {train_recall_opt:.4f}\")\n",
    "    print(f\"  F1-Score: {train_f1_opt:.4f}\")\n",
    "    print(f\"  ROC-AUC: {train_roc_auc_opt:.4f}\")\n",
    "\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(f\"  Accuracy: {test_accuracy_opt:.4f}\")\n",
    "    print(f\"  Precision: {test_precision_opt:.4f}\")\n",
    "    print(f\"  Recall: {test_recall_opt:.4f}\")\n",
    "    print(f\"  F1-Score: {test_f1_opt:.4f}\")\n",
    "    print(f\"  ROC-AUC: {test_roc_auc_opt:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrices:\")\n",
    "    print(\"Training Set Confusion Matrix:\")\n",
    "    print(cm_train_opt)\n",
    "    print(\"Test Set Confusion Matrix:\")\n",
    "    print(cm_test_opt)\n",
    "\n",
    "    print(\"\\nDetailed Classification Report - Training Set:\")\n",
    "    print(classification_report(y_train, y_train_pred_opt, target_names=['Normal', 'Yarn Break']))\n",
    "\n",
    "    print(\"\\nDetailed Classification Report - Test Set:\")\n",
    "    print(classification_report(y_test, y_test_pred_opt, target_names=['Normal', 'Yarn Break']))\n",
    "\n",
    "    # 9. Save the optimized model with comprehensive information\n",
    "    print(\"\\n=== Saving Optimized Model ===\")\n",
    "    optimized_model_info = {\n",
    "        'model': optimized_rf_model,\n",
    "        'best_params': best_params,\n",
    "        'class_weights': class_weights,\n",
    "        'train_break_count': train_break_count,\n",
    "        'train_break_ratio': train_break_ratio,\n",
    "        'feature_names': feature_names,\n",
    "        'cv_results': cv_results,\n",
    "        'param_analysis': param_analysis,\n",
    "        'optimization_time': param_search_time,\n",
    "        'training_metrics': {\n",
    "            'accuracy': train_accuracy_opt,\n",
    "            'precision': train_precision_opt,\n",
    "            'recall': train_recall_opt,\n",
    "            'f1_score': train_f1_opt,\n",
    "            'roc_auc': train_roc_auc_opt\n",
    "        },\n",
    "        'test_metrics': {\n",
    "            'accuracy': test_accuracy_opt,\n",
    "            'precision': test_precision_opt,\n",
    "            'recall': test_recall_opt,\n",
    "            'f1_score': test_f1_opt,\n",
    "            'roc_auc': test_roc_auc_opt\n",
    "        },\n",
    "        'confusion_matrices': {\n",
    "            'train': cm_train_opt,\n",
    "            'test': cm_test_opt\n",
    "        },\n",
    "        'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'model_version': 'optimized_v1.0'\n",
    "    }\n",
    "\n",
    "    # Save the optimized model with timestamp in filename\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    optimized_filename = f'rf_balanced_optimized_{timestamp}.pkl'\n",
    "    joblib.dump(optimized_model_info, optimized_filename)\n",
    "    print(f\"Optimized model saved to: {optimized_filename}\")\n",
    "\n",
    "    # Also update the main model file for easy access\n",
    "    joblib.dump(optimized_model_info, 'rf_balanced_optimized_latest.pkl')\n",
    "    print(\"Latest optimized model saved to: rf_balanced_optimized_latest.pkl\")\n",
    "\n",
    "    # 10. Parameter trend visualization\n",
    "    print(\"\\n=== Generating Parameter Trend Visualization ===\")\n",
    "\n",
    "    # Create parameter analysis DataFrame\n",
    "    param_analysis_df = pd.DataFrame(param_analysis)\n",
    "\n",
    "    # Create trend charts for each parameter\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # Color settings\n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "\n",
    "    # Parameter display name mapping\n",
    "    param_display_names = {\n",
    "        'n_estimators': 'Number of Trees (n_estimators)',\n",
    "        'max_depth': 'Maximum Depth (max_depth)',\n",
    "        'min_samples_split': 'Minimum Samples Split (min_samples_split)',\n",
    "        'min_samples_leaf': 'Minimum Samples Leaf (min_samples_leaf)'\n",
    "    }\n",
    "\n",
    "    for i, (param_name, display_name) in enumerate(param_display_names.items()):\n",
    "        param_data = param_analysis_df[param_analysis_df['Parameter'] == param_name]\n",
    "\n",
    "        if not param_data.empty:\n",
    "            # Ensure numeric type sorting\n",
    "            param_data = param_data.copy()\n",
    "\n",
    "            # Handle None values in max_depth\n",
    "            if param_name == 'max_depth':\n",
    "                param_data['Sort_Key'] = param_data['Value'].apply(lambda x: 1000 if x is None else x)\n",
    "                param_data = param_data.sort_values('Sort_Key')\n",
    "                x_labels = [str(x) if x is not None else 'None' for x in param_data['Value']]\n",
    "            else:\n",
    "                param_data = param_data.sort_values('Value')\n",
    "                x_labels = [str(x) for x in param_data['Value']]\n",
    "\n",
    "            # Plot trend line\n",
    "            x_positions = range(len(param_data))\n",
    "            axes[i].plot(x_positions, param_data['Mean_Score'],\n",
    "                        'o-', linewidth=3, markersize=8, color=colors[i],\n",
    "                        label='Mean F1-Score', markerfacecolor='white', markeredgewidth=2)\n",
    "\n",
    "            # Add error bars\n",
    "            axes[i].fill_between(x_positions,\n",
    "                               param_data['Mean_Score'] - param_data['Std_Score'],\n",
    "                               param_data['Mean_Score'] + param_data['Std_Score'],\n",
    "                               alpha=0.2, color=colors[i], label='Standard Deviation Range')\n",
    "\n",
    "            # Mark best value\n",
    "            best_idx = param_data['Mean_Score'].idxmax()\n",
    "            best_x = list(x_positions)[list(param_data.index).index(best_idx)]\n",
    "            best_score = param_data.loc[best_idx, 'Mean_Score']\n",
    "            best_value = param_data.loc[best_idx, 'Value']\n",
    "\n",
    "            axes[i].axvline(x=best_x, color='red', linestyle='--', alpha=0.8, linewidth=2)\n",
    "            axes[i].plot(best_x, best_score, 'o', markersize=10, color='red',\n",
    "                       label=f'Best Value: {best_value} (F1={best_score:.3f})')\n",
    "\n",
    "            # Set chart properties\n",
    "            axes[i].set_title(f'Impact of {display_name} on Model Performance',\n",
    "                             fontsize=14, fontweight='bold', pad=20)\n",
    "            axes[i].set_xlabel('Parameter Value', fontsize=12, fontweight='bold')\n",
    "            axes[i].set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "            axes[i].set_xticks(x_positions)\n",
    "            axes[i].set_xticklabels(x_labels, rotation=45)\n",
    "            axes[i].legend(loc='lower right' if param_name in ['n_estimators', 'max_depth'] else 'upper right')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "            # Set y-axis range to highlight differences\n",
    "            y_min = max(0.8, param_data['Mean_Score'].min() - 0.05)\n",
    "            y_max = min(1.0, param_data['Mean_Score'].max() + 0.05)\n",
    "            axes[i].set_ylim(y_min, y_max)\n",
    "\n",
    "    plt.suptitle('Random Forest Parameter Impact on Model Performance Trend Analysis',\n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('parameter_trend_analysis_detailed.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Detailed parameter trend analysis chart saved to: parameter_trend_analysis_detailed.png\")\n",
    "\n",
    "    # 11. Final results summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RF_balanced Model Optimization Summary\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results_df_final = pd.DataFrame(metrics_results)\n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(results_df_final.round(4))\n",
    "\n",
    "    # Create time statistics DataFrame\n",
    "    time_df = pd.DataFrame(execution_times)\n",
    "    print(\"\\nTime Statistics:\")\n",
    "    print(time_df.round(2))\n",
    "\n",
    "    # Save all results to CSV files\n",
    "    results_df_final.to_csv('RF_balanced_optimized_performance.csv', index=False)\n",
    "    time_df.to_csv('RF_balanced_optimized_timing.csv', index=False)\n",
    "    cv_df.to_csv('RF_balanced_cross_validation.csv', index=False)\n",
    "    param_analysis_df.to_csv('RF_balanced_parameter_analysis.csv', index=False)\n",
    "    cv_detailed_df.to_csv('RF_balanced_cross_validation_detailed.csv', index=False)\n",
    "\n",
    "    print(f\"\\nAll results saved to CSV files\")\n",
    "\n",
    "    # Feature importance analysis\n",
    "    print(\"\\n=== Optimized Model Feature Importance Analysis ===\")\n",
    "    feature_importances_opt = optimized_rf_model.feature_importances_\n",
    "\n",
    "    # Create feature importance DataFrame\n",
    "    importance_df_opt = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances_opt\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(importance_df_opt.head(10).round(4))\n",
    "\n",
    "    # Save feature importance\n",
    "    importance_df_opt.to_csv('RF_balanced_optimized_feature_importance.csv', index=False)\n",
    "    print(f\"Feature importance saved to: RF_balanced_optimized_feature_importance.csv\")\n",
    "\n",
    "    print(\"\\nOptimization process completed!\")\n",
    "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "    print(f\"Optimized model ready for deployment: {optimized_filename}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Saved RF_balanced model file 'rf_balanced_updated.pkl' not found\")\n",
    "    print(\"Please run the model creation and saving code first\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model loading or evaluation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nAll evaluation and optimization completed!\")\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.linewidth': 2.5,\n",
    "    'font.size': 16,\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 17,\n",
    "    'xtick.labelsize': 15,\n",
    "    'ytick.labelsize': 15,\n",
    "    'legend.fontsize': 15,\n",
    "    'grid.linewidth': 1.2,\n",
    "    'lines.linewidth': 3.5,\n",
    "    'figure.dpi': 1200,\n",
    "    'savefig.dpi': 1200,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.unicode_minus': False\n",
    "})\n",
    "\n",
    "print(\"=== RF_balanced Model Hyperparameter Tuning and Performance Evaluation ===\")\n",
    "print(\"Includes Cross-Validation and Hyperparameter Optimization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize lists to store results\n",
    "metrics_results = []\n",
    "execution_times = []\n",
    "cv_results = []\n",
    "param_analysis = []\n",
    "\n",
    "try:\n",
    "    # Load the previously saved updated RF_balanced model\n",
    "    print(\"Loading saved RF_balanced model...\")\n",
    "    load_start_time = time.time()\n",
    "    model_info = joblib.load('rf_balanced_updated.pkl')\n",
    "    load_time = time.time() - load_start_time\n",
    "\n",
    "    # Get model and related information\n",
    "    rf_balanced_model = model_info['model']\n",
    "    class_weights = model_info['class_weights']\n",
    "    train_break_count = model_info['train_break_count']\n",
    "    train_break_ratio = model_info['train_break_ratio']\n",
    "    feature_names = model_info['feature_names']\n",
    "\n",
    "    print(f\"Model loaded successfully, time: {load_time:.2f} seconds\")\n",
    "    print(f\"Class weights used: {class_weights}\")\n",
    "    print(f\"Training set yarn break events: {train_break_count} (ratio: {train_break_ratio:.2%})\")\n",
    "\n",
    "    # 1. Cross-validation evaluation - Output all metrics\n",
    "    print(\"\\n=== Cross-Validation Evaluation ===\")\n",
    "    cv_start_time = time.time()\n",
    "\n",
    "    # Use stratified K-fold cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Evaluate multiple metrics\n",
    "    scoring_metrics = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'precision': 'precision',\n",
    "        'recall': 'recall',\n",
    "        'f1': 'f1',\n",
    "        'roc_auc': 'roc_auc'\n",
    "    }\n",
    "\n",
    "    cv_scores = {}\n",
    "    cv_detailed_results = []\n",
    "\n",
    "    print(\"Performing 5-fold cross-validation, evaluating multiple metrics...\")\n",
    "    for metric_name, metric_scorer in scoring_metrics.items():\n",
    "        scores = cross_val_score(rf_balanced_model, X_train, y_train,\n",
    "                               cv=cv, scoring=metric_scorer, n_jobs=-1)\n",
    "        cv_scores[metric_name] = scores\n",
    "\n",
    "        # Store detailed results\n",
    "        for fold_idx, score in enumerate(scores):\n",
    "            cv_detailed_results.append({\n",
    "                'Fold': fold_idx + 1,\n",
    "                'Metric': metric_name,\n",
    "                'Score': score\n",
    "            })\n",
    "\n",
    "        # Output statistics for each metric\n",
    "        mean_score = np.mean(scores)\n",
    "        std_score = np.std(scores)\n",
    "        cv_results.append({\n",
    "            'Metric': metric_name,\n",
    "            'Mean_Score': mean_score,\n",
    "            'Std_Score': std_score,\n",
    "            'Scores': scores\n",
    "        })\n",
    "\n",
    "        print(f\"  {metric_name}: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "\n",
    "    cv_time = time.time() - cv_start_time\n",
    "    print(f\"Cross-validation completed, time: {cv_time:.2f} seconds\")\n",
    "\n",
    "    # Create cross-validation detailed results DataFrame\n",
    "    cv_detailed_df = pd.DataFrame(cv_detailed_results)\n",
    "\n",
    "    # Print detailed results for each fold\n",
    "    print(\"\\nCross-validation detailed results (5 folds):\")\n",
    "    pivot_cv = cv_detailed_df.pivot_table(index='Fold', columns='Metric', values='Score')\n",
    "    print(pivot_cv.round(4))\n",
    "\n",
    "    # Print overall statistics\n",
    "    print(\"\\nCross-validation overall statistics:\")\n",
    "    cv_df = pd.DataFrame(cv_results)\n",
    "    print(cv_df[['Metric', 'Mean_Score', 'Std_Score']].round(4))\n",
    "\n",
    "    # 2. Hyperparameter optimization\n",
    "    print(\"\\n=== Hyperparameter Optimization ===\")\n",
    "    param_search_start = time.time()\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_dist = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30, 40],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Use randomized search for parameter optimization\n",
    "    random_search = RandomizedSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,  # Number of iterations for random search\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Starting randomized search parameter optimization...\")\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    param_search_time = time.time() - param_search_start\n",
    "    print(f\"Parameter optimization completed, time: {param_search_time:.2f} seconds\")\n",
    "\n",
    "    # Output best parameters\n",
    "    print(\"\\nBest parameter combination:\")\n",
    "    best_params = random_search.best_params_\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print(f\"Best cross-validation score (F1): {random_search.best_score_:.4f}\")\n",
    "\n",
    "    # 3. Parameter impact analysis\n",
    "    print(\"\\n=== Parameter Impact Analysis ===\")\n",
    "\n",
    "    # Analyze the impact of different parameters on performance\n",
    "    results_df = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "    # Analyze impact of main parameters\n",
    "    key_params = ['param_n_estimators', 'param_max_depth',\n",
    "                 'param_min_samples_split', 'param_min_samples_leaf']\n",
    "\n",
    "    for param in key_params:\n",
    "        if param in results_df.columns:\n",
    "            param_data = results_df.groupby(param)['mean_test_score'].agg(['mean', 'std', 'count']).reset_index()\n",
    "            param_data = param_data.sort_values('mean', ascending=False)\n",
    "\n",
    "            print(f\"\\nImpact of {param} on performance:\")\n",
    "            for _, row in param_data.head(10).iterrows():\n",
    "                param_value = row[param]\n",
    "                if hasattr(param_value, '__len__') and not isinstance(param_value, str):\n",
    "                    param_value = str(param_value)\n",
    "                print(f\"  {param_value}: {row['mean']:.4f} ± {row['std']:.4f}\")\n",
    "\n",
    "            # Store parameter analysis results\n",
    "            param_analysis.extend([\n",
    "                {\n",
    "                    'Parameter': param.replace('param_', ''),\n",
    "                    'Value': row[param],\n",
    "                    'Mean_Score': row['mean'],\n",
    "                    'Std_Score': row['std'],\n",
    "                    'Count': row['count']\n",
    "                } for _, row in param_data.iterrows()\n",
    "            ])\n",
    "\n",
    "    # 4. Train new model with best parameters\n",
    "    print(\"\\n=== Training New Model with Best Parameters ===\")\n",
    "    training_start = time.time()\n",
    "\n",
    "    # Create new model with best parameters\n",
    "    optimized_rf_model = RandomForestClassifier(\n",
    "        **best_params,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    optimized_rf_model.fit(X_train, y_train)\n",
    "    training_time = time.time() - training_start\n",
    "    print(f\"Model training completed, time: {training_time:.2f} seconds\")\n",
    "\n",
    "    # 5. Evaluate the optimized model\n",
    "    print(\"\\n=== Optimized Model Performance Evaluation ===\")\n",
    "    predict_start_time = time.time()\n",
    "\n",
    "    # Make predictions on training and test sets\n",
    "    y_train_pred_opt = optimized_rf_model.predict(X_train)\n",
    "    y_test_pred_opt = optimized_rf_model.predict(X_test)\n",
    "\n",
    "    # Get probability predictions\n",
    "    y_train_pred_proba_opt = optimized_rf_model.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred_proba_opt = optimized_rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    predict_time = time.time() - predict_start_time\n",
    "    print(f\"Prediction completed, time: {predict_time:.2f} seconds\")\n",
    "\n",
    "    # Calculate training set metrics\n",
    "    train_accuracy_opt = accuracy_score(y_train, y_train_pred_opt)\n",
    "    train_precision_opt = precision_score(y_train, y_train_pred_opt, zero_division=0)\n",
    "    train_recall_opt = recall_score(y_train, y_train_pred_opt, zero_division=0)\n",
    "    train_f1_opt = f1_score(y_train, y_train_pred_opt, zero_division=0)\n",
    "    train_roc_auc_opt = roc_auc_score(y_train, y_train_pred_proba_opt)\n",
    "\n",
    "    # Calculate test set metrics\n",
    "    test_accuracy_opt = accuracy_score(y_test, y_test_pred_opt)\n",
    "    test_precision_opt = precision_score(y_test, y_test_pred_opt, zero_division=0)\n",
    "    test_recall_opt = recall_score(y_test, y_test_pred_opt, zero_division=0)\n",
    "    test_f1_opt = f1_score(y_test, y_test_pred_opt, zero_division=0)\n",
    "    test_roc_auc_opt = roc_auc_score(y_test, y_test_pred_proba_opt)\n",
    "\n",
    "    # Calculate confusion matrices\n",
    "    cm_train_opt = confusion_matrix(y_train, y_train_pred_opt)\n",
    "    cm_test_opt = confusion_matrix(y_test, y_test_pred_opt)\n",
    "\n",
    "    total_time = load_time + cv_time + param_search_time + training_time + predict_time\n",
    "\n",
    "    # 6. Visualize confusion matrices in one combined figure with enhanced styling\n",
    "    print(\"\\n=== Generating Combined Confusion Matrix Visualization ===\")\n",
    "\n",
    "    # Create a combined figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    # ==================== 修改位置1：混淆矩阵轴标签字体大小 ====================\n",
    "    # Training set confusion matrix\n",
    "    sns.heatmap(cm_train_opt, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Yarn Break'],\n",
    "                yticklabels=['Normal', 'Yarn Break'],\n",
    "                annot_kws={'size': 18, 'weight': 'bold'},\n",
    "                linewidths=2, linecolor='black', ax=ax1)\n",
    "    ax1.set_title('Training Set Confusion Matrix\\nOptimized RF_balanced Model',\n",
    "                 fontsize=18, fontweight='bold', pad=20)\n",
    "    ax1.set_xlabel('Predicted Label', fontsize=20, fontweight='bold')  # 从18改为20\n",
    "    ax1.set_ylabel('True Label', fontsize=20, fontweight='bold')       # 从18改为20\n",
    "\n",
    "    # Test set confusion matrix\n",
    "    sns.heatmap(cm_test_opt, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Yarn Break'],\n",
    "                yticklabels=['Normal', 'Yarn Break'],\n",
    "                annot_kws={'size': 18, 'weight': 'bold'},\n",
    "                linewidths=2, linecolor='black', ax=ax2)\n",
    "    ax2.set_title('Test Set Confusion Matrix\\nOptimized RF_balanced Model',\n",
    "                 fontsize=18, fontweight='bold', pad=20)\n",
    "    ax2.set_xlabel('Predicted Label', fontsize=20, fontweight='bold')  # 从18改为20\n",
    "    ax2.set_ylabel('True Label', fontsize=20, fontweight='bold')       # 从18改为20\n",
    "\n",
    "    # Bold the borders for both subplots\n",
    "    for ax in [ax1, ax2]:\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(3)\n",
    "\n",
    "    plt.suptitle('Optimized RF_balanced Model - Confusion Matrix Analysis',\n",
    "                fontsize=18, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('combined_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Combined confusion matrix saved to: combined_confusion_matrix.png\")\n",
    "\n",
    "    # 7. Store optimized model results\n",
    "    metrics_results.append({\n",
    "        'Dataset': 'Optimized-Training Set',\n",
    "        'Accuracy': train_accuracy_opt,\n",
    "        'Precision': train_precision_opt,\n",
    "        'Recall': train_recall_opt,\n",
    "        'F1-Score': train_f1_opt,\n",
    "        'ROC-AUC': train_roc_auc_opt,\n",
    "        'Parameters': str(best_params)\n",
    "    })\n",
    "\n",
    "    metrics_results.append({\n",
    "        'Dataset': 'Optimized-Test Set',\n",
    "        'Accuracy': test_accuracy_opt,\n",
    "        'Precision': test_precision_opt,\n",
    "        'Recall': test_recall_opt,\n",
    "        'F1-Score': test_f1_opt,\n",
    "        'ROC-AUC': test_roc_auc_opt,\n",
    "        'Parameters': str(best_params)\n",
    "    })\n",
    "\n",
    "    execution_times.append({\n",
    "        'Model Loading Time': load_time,\n",
    "        'Cross-Validation Time': cv_time,\n",
    "        'Parameter Search Time': param_search_time,\n",
    "        'Model Training Time': training_time,\n",
    "        'Prediction Time': predict_time,\n",
    "        'Total Time': total_time\n",
    "    })\n",
    "\n",
    "    # 8. Print optimized model results\n",
    "    print(\"\\nOptimized RF_balanced Model Evaluation Results:\")\n",
    "    print(\"Training Set Performance:\")\n",
    "    print(f\"  Accuracy: {train_accuracy_opt:.4f}\")\n",
    "    print(f\"  Precision: {train_precision_opt:.4f}\")\n",
    "    print(f\"  Recall: {train_recall_opt:.4f}\")\n",
    "    print(f\"  F1-Score: {train_f1_opt:.4f}\")\n",
    "    print(f\"  ROC-AUC: {train_roc_auc_opt:.4f}\")\n",
    "\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(f\"  Accuracy: {test_accuracy_opt:.4f}\")\n",
    "    print(f\"  Precision: {test_precision_opt:.4f}\")\n",
    "    print(f\"  Recall: {test_recall_opt:.4f}\")\n",
    "    print(f\"  F1-Score: {test_f1_opt:.4f}\")\n",
    "    print(f\"  ROC-AUC: {test_roc_auc_opt:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrices:\")\n",
    "    print(\"Training Set Confusion Matrix:\")\n",
    "    print(cm_train_opt)\n",
    "    print(\"Test Set Confusion Matrix:\")\n",
    "    print(cm_test_opt)\n",
    "\n",
    "    print(\"\\nDetailed Classification Report - Training Set:\")\n",
    "    print(classification_report(y_train, y_train_pred_opt, target_names=['Normal', 'Yarn Break']))\n",
    "\n",
    "    print(\"\\nDetailed Classification Report - Test Set:\")\n",
    "    print(classification_report(y_test, y_test_pred_opt, target_names=['Normal', 'Yarn Break']))\n",
    "\n",
    "    # 9. Save the optimized model with comprehensive information\n",
    "    print(\"\\n=== Saving Optimized Model ===\")\n",
    "    optimized_model_info = {\n",
    "        'model': optimized_rf_model,\n",
    "        'best_params': best_params,\n",
    "        'class_weights': class_weights,\n",
    "        'train_break_count': train_break_count,\n",
    "        'train_break_ratio': train_break_ratio,\n",
    "        'feature_names': feature_names,\n",
    "        'cv_results': cv_results,\n",
    "        'param_analysis': param_analysis,\n",
    "        'optimization_time': param_search_time,\n",
    "        'training_metrics': {\n",
    "            'accuracy': train_accuracy_opt,\n",
    "            'precision': train_precision_opt,\n",
    "            'recall': train_recall_opt,\n",
    "            'f1_score': train_f1_opt,\n",
    "            'roc_auc': train_roc_auc_opt\n",
    "        },\n",
    "        'test_metrics': {\n",
    "            'accuracy': test_accuracy_opt,\n",
    "            'precision': test_precision_opt,\n",
    "            'recall': test_recall_opt,\n",
    "            'f1_score': test_f1_opt,\n",
    "            'roc_auc': test_roc_auc_opt\n",
    "        },\n",
    "        'confusion_matrices': {\n",
    "            'train': cm_train_opt,\n",
    "            'test': cm_test_opt\n",
    "        },\n",
    "        'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'model_version': 'optimized_v1.0'\n",
    "    }\n",
    "\n",
    "    # Save the optimized model with timestamp in filename\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    optimized_filename = f'rf_balanced_optimized_{timestamp}.pkl'\n",
    "    joblib.dump(optimized_model_info, optimized_filename)\n",
    "    print(f\"Optimized model saved to: {optimized_filename}\")\n",
    "\n",
    "    # Also update the main model file for easy access\n",
    "    joblib.dump(optimized_model_info, 'rf_balanced_optimized_latest.pkl')\n",
    "    print(\"Latest optimized model saved to: rf_balanced_optimized_latest.pkl\")\n",
    "\n",
    "    # 10. Parameter trend visualization (移除图例)\n",
    "    print(\"\\n=== Generating Parameter Trend Visualization ===\")\n",
    "\n",
    "    # Create parameter analysis DataFrame\n",
    "    param_analysis_df = pd.DataFrame(param_analysis)\n",
    "\n",
    "    # Create trend charts for each parameter\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # Color settings\n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "\n",
    "    # Parameter display name mapping\n",
    "    param_display_names = {\n",
    "        'n_estimators': 'Number of Trees (n_estimators)',\n",
    "        'max_depth': 'Maximum Depth (max_depth)',\n",
    "        'min_samples_split': 'Minimum Samples Split (min_samples_split)',\n",
    "        'min_samples_leaf': 'Minimum Samples Leaf (min_samples_leaf)'\n",
    "    }\n",
    "\n",
    "    for i, (param_name, display_name) in enumerate(param_display_names.items()):\n",
    "        param_data = param_analysis_df[param_analysis_df['Parameter'] == param_name]\n",
    "\n",
    "        if not param_data.empty:\n",
    "            # Ensure numeric type sorting\n",
    "            param_data = param_data.copy()\n",
    "\n",
    "            # Handle None values in max_depth\n",
    "            if param_name == 'max_depth':\n",
    "                param_data['Sort_Key'] = param_data['Value'].apply(lambda x: 1000 if x is None else x)\n",
    "                param_data = param_data.sort_values('Sort_Key')\n",
    "                x_labels = [str(x) if x is not None else 'None' for x in param_data['Value']]\n",
    "            else:\n",
    "                param_data = param_data.sort_values('Value')\n",
    "                x_labels = [str(x) for x in param_data['Value']]\n",
    "\n",
    "            # Plot trend line\n",
    "            x_positions = range(len(param_data))\n",
    "            axes[i].plot(x_positions, param_data['Mean_Score'],\n",
    "                        'o-', linewidth=3, markersize=8, color=colors[i],\n",
    "                        label='Mean F1-Score', markerfacecolor='white', markeredgewidth=2)\n",
    "\n",
    "            # Add error bars\n",
    "            axes[i].fill_between(x_positions,\n",
    "                               param_data['Mean_Score'] - param_data['Std_Score'],\n",
    "                               param_data['Mean_Score'] + param_data['Std_Score'],\n",
    "                               alpha=0.2, color=colors[i])\n",
    "\n",
    "            # Mark best value\n",
    "            best_idx = param_data['Mean_Score'].idxmax()\n",
    "            best_x = list(x_positions)[list(param_data.index).index(best_idx)]\n",
    "            best_score = param_data.loc[best_idx, 'Mean_Score']\n",
    "            best_value = param_data.loc[best_idx, 'Value']\n",
    "\n",
    "            axes[i].axvline(x=best_x, color='red', linestyle='--', alpha=0.8, linewidth=2)\n",
    "            axes[i].plot(best_x, best_score, 'o', markersize=10, color='red')\n",
    "\n",
    "            # ==================== 修改位置2：参数趋势图轴标签字体大小 ====================\n",
    "            # Set chart properties\n",
    "            axes[i].set_title(f'Impact of {display_name} on Model Performance',\n",
    "                             fontsize=16, fontweight='bold', pad=20)\n",
    "            axes[i].set_xlabel('Parameter Value', fontsize=20, fontweight='bold')  # 从12改为20\n",
    "            axes[i].set_ylabel('F1-Score', fontsize=20, fontweight='bold')         # 从12改为20\n",
    "            axes[i].set_xticks(x_positions)\n",
    "            axes[i].set_xticklabels(x_labels, rotation=45, fontsize=14)\n",
    "            # 移除图例以避免重叠\n",
    "            # axes[i].legend(loc='lower right' if param_name in ['n_estimators', 'max_depth'] else 'upper right')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "            # Set y-axis range to highlight differences\n",
    "            y_min = max(0.8, param_data['Mean_Score'].min() - 0.05)\n",
    "            y_max = min(1.0, param_data['Mean_Score'].max() + 0.05)\n",
    "            axes[i].set_ylim(y_min, y_max)\n",
    "\n",
    "    plt.suptitle('Random Forest Parameter Impact on Model Performance Trend Analysis',\n",
    "                 fontsize=18, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('parameter_trend_analysis_detailed.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Detailed parameter trend analysis chart saved to: parameter_trend_analysis_detailed.png\")\n",
    "\n",
    "    # 11. Final results summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RF_balanced Model Optimization Summary\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results_df_final = pd.DataFrame(metrics_results)\n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(results_df_final.round(4))\n",
    "\n",
    "    # Create time statistics DataFrame\n",
    "    time_df = pd.DataFrame(execution_times)\n",
    "    print(\"\\nTime Statistics:\")\n",
    "    print(time_df.round(2))\n",
    "\n",
    "    # Save all results to CSV files\n",
    "    results_df_final.to_csv('RF_balanced_optimized_performance.csv', index=False)\n",
    "    time_df.to_csv('RF_balanced_optimized_timing.csv', index=False)\n",
    "    cv_df.to_csv('RF_balanced_cross_validation.csv', index=False)\n",
    "    param_analysis_df.to_csv('RF_balanced_parameter_analysis.csv', index=False)\n",
    "    cv_detailed_df.to_csv('RF_balanced_cross_validation_detailed.csv', index=False)\n",
    "\n",
    "    print(f\"\\nAll results saved to CSV files\")\n",
    "\n",
    "    # Feature importance analysis\n",
    "    print(\"\\n=== Optimized Model Feature Importance Analysis ===\")\n",
    "    feature_importances_opt = optimized_rf_model.feature_importances_\n",
    "\n",
    "    # Create feature importance DataFrame\n",
    "    importance_df_opt = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances_opt\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(importance_df_opt.head(10).round(4))\n",
    "\n",
    "    # Save feature importance\n",
    "    importance_df_opt.to_csv('RF_balanced_optimized_feature_importance.csv', index=False)\n",
    "    print(f\"Feature importance saved to: RF_balanced_optimized_feature_importance.csv\")\n",
    "\n",
    "    print(\"\\nOptimization process completed!\")\n",
    "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "    print(f\"Optimized model ready for deployment: {optimized_filename}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Saved RF_balanced model file 'rf_balanced_updated.pkl' not found\")\n",
    "    print(\"Please run the model creation and saving code first\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model loading or evaluation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nAll evaluation and optimization completed!\")"
   ],
   "id": "80b291490357411b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 数据准备\n",
    "data = {\n",
    "    'feature': ['V$_{1}$', 'P$_{1}$', 'R$_{3}$_380','R$_{3}$_375', 'R$_{6}$_600', 'R$_{2}$_9200',\n",
    "                'R$_{4}$_300', 'R$_{2}$_9500', 'R$_{5}$_8.2', 'R$_{1}$_1300', 'R$_{2}$_9000',\n",
    "                'R$_{3}$_443', 'R$_{5}$_9.1', 'R$_{1}$_2000', 'R$_{6}$_1100', 'R$_{5}$_11.2', 'R$_{3}$_365'],\n",
    "    'importance': [0.414, 0.391, 0.054, 0.032, 0.019, 0.018,\n",
    "                   0.015, 0.011, 0.010, 0.009, 0.008,\n",
    "                   0.006, 0.005, 0.004, 0.002, 0.001, 0.001]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 按照重要性从高到低排序\n",
    "df = df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# 设置绘图风格\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.size': 20,\n",
    "    'axes.labelsize': 26,\n",
    "    'axes.titlesize': 26,\n",
    "    'xtick.labelsize': 18,\n",
    "    'ytick.labelsize': 18,\n",
    "    'figure.dpi': 1200,\n",
    "    'savefig.dpi': 1200,\n",
    "    'figure.figsize': (12, 8),\n",
    "    'axes.grid': True,\n",
    "    'grid.linestyle': '--',\n",
    "    'grid.alpha': 0.3,\n",
    "    'grid.color': 'gray',\n",
    "    # 'axes.facecolor': 'white', 可以保留也可以去掉这一行，下面会显式设置\n",
    "})\n",
    "\n",
    "# 创建颜色映射 (蓝色渐变，颜色更鲜明)\n",
    "colors = plt.cm.Blues(np.linspace(0.3, 0.9, len(df)))\n",
    "\n",
    "# 创建图形\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# 显式设置绘图区域（坐标轴内）的背景颜色为白色\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# 绘制横向柱状图，反转特征顺序\n",
    "bars = ax.barh(\n",
    "    df['feature'][::-1],\n",
    "    df['importance'][::-1],\n",
    "    color=colors[::-1],\n",
    "    height=0.8,\n",
    "    edgecolor='black',\n",
    "    linewidth=2  # 加粗边框线\n",
    ")\n",
    "\n",
    "# 添加数据标签\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(\n",
    "        width + 0.001,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f'{width:.3f}',\n",
    "        va='center',\n",
    "        ha='left',\n",
    "        fontsize=18,\n",
    "        fontweight='bold',  # 加粗标签字体\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "# 设置坐标轴\n",
    "ax.set_xlabel('Feature Importance Score', fontweight='bold', labelpad=24)\n",
    "ax.set_ylabel('Features', fontweight='bold', labelpad=2)\n",
    "# ax.set_title('Feature Importance Ranking', pad=30, fontweight='bold')\n",
    "\n",
    "# 调整 x 轴范围\n",
    "ax.set_xlim(0, df['importance'].max() * 1.2)\n",
    "\n",
    "# 隐藏上、右边框\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# 调整下、左边框样式\n",
    "ax.spines['bottom'].set_linewidth(3)  # 加粗边框线\n",
    "ax.spines['left'].set_linewidth(3)    # 加粗边框线\n",
    "ax.spines['bottom'].set_color('black')\n",
    "ax.spines['left'].set_color('black')\n",
    "\n",
    "# 调整刻度标签颜色和粗细\n",
    "ax.tick_params(axis='x', colors='black', width=2, length=6)\n",
    "ax.tick_params(axis='y', colors='black', width=2, length=6)\n",
    "\n",
    "# 设置 y 轴标签字体为斜体\n",
    "for tick in ax.get_yticklabels():\n",
    "    tick.set_fontstyle('italic')\n",
    "\n",
    "# 调整子图布局\n",
    "plt.subplots_adjust(left=0.15, right=0.95, top=0.9, bottom=0.1)\n",
    "\n",
    "# 调整布局\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存图像\n",
    "plt.savefig('feature_importance_bold_clear.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ],
   "id": "c08b982e81b1c44f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from joblib import dump  # 用于保存模型\n",
    "\n",
    "# 初始化列表存储分类指标\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "execution_times = []\n",
    "model_names = []\n",
    "best_params_list = []  # 存储最佳参数\n",
    "feature_importances = []  # 存储特征重要性\n",
    "\n",
    "# 定义分类模型\n",
    "models = [\n",
    "    ('RF', RandomForestClassifier()),\n",
    "    ('XGBoost', XGBClassifier()),\n",
    "    ('LightGBM', LGBMClassifier())\n",
    "]\n",
    "\n",
    "# 定义超参数网格\n",
    "param_grids = {\n",
    "    'RF': {'n_estimators': [10], 'max_depth': [None], 'class_weight': ['balanced']},\n",
    "    'XGBoost': {'n_estimators': [10, 30, 50, 70, 100], 'learning_rate': [0.005, 0.01, 0.05, 0.1], 'scale_pos_weight': [1, 10, 100]},\n",
    "    'LightGBM': {'n_estimators': [10, 30, 50, 70, 100], 'learning_rate': [0.005, 0.01, 0.05, 0.1], 'is_unbalance': [True, False]}\n",
    "}\n",
    "\n",
    "# 主循环\n",
    "for name, classifier in models:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 超参数调优\n",
    "    if param_grids.get(name):\n",
    "        grid_search = GridSearchCV(classifier, param_grid=param_grids[name], cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_  # 获取最佳参数\n",
    "    else:\n",
    "        best_model = classifier\n",
    "        best_model.fit(X_train, y_train)\n",
    "        best_params = {}  # 如果没有调优，最佳参数为空字典\n",
    "\n",
    "    # 预测\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # 计算分类指标\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "\n",
    "    # 记录执行时间\n",
    "    execution_time = time.time() - start_time\n",
    "    execution_times.append(execution_time)\n",
    "\n",
    "    # 记录模型名称和最佳参数\n",
    "    model_names.append(name)\n",
    "    best_params_list.append(best_params)\n",
    "\n",
    "    # 获取特征重要性\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importances = best_model.feature_importances_\n",
    "        feature_importances.append(importances)\n",
    "    elif hasattr(best_model, 'coef_'):\n",
    "        importances = best_model.coef_[0]  # 对于线性模型，取第一个类的系数\n",
    "        feature_importances.append(importances)\n",
    "    else:\n",
    "        feature_importances.append(None)\n",
    "\n",
    "    # 保存 Random Forest 模型\n",
    "    if name == 'RF':\n",
    "        dump(best_model, 'rf_model.joblib')\n",
    "\n",
    "# 保存结果\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores,\n",
    "    'ROC-AUC': roc_auc_scores,\n",
    "    'Execution Time (s)': execution_times,\n",
    "    'Best Parameters': best_params_list\n",
    "})\n",
    "\n",
    "print(\"All Models Performance:\")\n",
    "display(results_df)\n",
    "\n",
    "# 打印特征重要性\n",
    "print(\"\\n=== Feature Importances ===\")\n",
    "for i, name in enumerate(model_names):\n",
    "    if feature_importances[i] is not None:\n",
    "        print(f\"\\nModel: {name}\")\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': feature_importances[i]\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        display(importance_df.head(20))  # 显示前 10 个重要特征"
   ],
   "id": "588e16c71d249c80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from joblib import dump, load  # 用于保存和加载模型\n",
    "\n",
    "# 初始化列表存储分类指标\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "execution_times = []\n",
    "model_names = []\n",
    "best_params_list = []  # 存储最佳参数\n",
    "feature_importances = []  # 存储特征重要性\n",
    "\n",
    "# 定义分类模型\n",
    "models = [\n",
    "    ('RF', RandomForestClassifier()),\n",
    "    ('XGBoost', XGBClassifier()),\n",
    "    ('LightGBM', LGBMClassifier())\n",
    "]\n",
    "\n",
    "# 定义超参数网格\n",
    "param_grids = {\n",
    "    'RF': {'n_estimators': [10], 'max_depth': [None], 'class_weight': ['balanced']},\n",
    "    'XGBoost': {'n_estimators': [10, 30, 50, 70, 100], 'learning_rate': [0.005, 0.01, 0.05, 0.1], 'scale_pos_weight': [1, 10, 100]},\n",
    "    'LightGBM': {'n_estimators': [10, 30, 50, 70, 100], 'learning_rate': [0.005, 0.01, 0.05, 0.1], 'is_unbalance': [True, False]}\n",
    "}\n",
    "\n",
    "# 主循环\n",
    "for name, classifier in models:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 超参数调优\n",
    "    if param_grids.get(name):\n",
    "        grid_search = GridSearchCV(classifier, param_grid=param_grids[name], cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_  # 获取最佳参数\n",
    "    else:\n",
    "        best_model = classifier\n",
    "        best_model.fit(X_train, y_train)\n",
    "        best_params = {}  # 如果没有调优，最佳参数为空字典\n",
    "\n",
    "    # 预测\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # 计算分类指标\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "\n",
    "    # 记录执行时间\n",
    "    execution_time = time.time() - start_time\n",
    "    execution_times.append(execution_time)\n",
    "\n",
    "    # 记录模型名称和最佳参数\n",
    "    model_names.append(name)\n",
    "    best_params_list.append(best_params)\n",
    "\n",
    "    # 获取特征重要性\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importances = best_model.feature_importances_\n",
    "        feature_importances.append(importances)\n",
    "    elif hasattr(best_model, 'coef_'):\n",
    "        importances = best_model.coef_[0]  # 对于线性模型，取第一个类的系数\n",
    "        feature_importances.append(importances)\n",
    "    else:\n",
    "        feature_importances.append(None)\n",
    "\n",
    "    # 保存模型和特征重要性\n",
    "    if name == 'RF':\n",
    "        dump(best_model, 'rf_model.joblib')\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': feature_importances[-1]\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        importance_df.to_csv('D:/code/junma/600000/0424/rf_feature_importance.csv', index=False)  # 保存特征重要性到CSV文件\n",
    "\n",
    "# 保存结果\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores,\n",
    "    'ROC-AUC': roc_auc_scores,\n",
    "    'Execution Time (s)': execution_times,\n",
    "    'Best Parameters': best_params_list\n",
    "})\n",
    "\n",
    "results_df.to_csv('D:/code/junma/600000/0424/model_performance.csv', index=False)  # 保存模型性能结果到CSV文件\n",
    "\n",
    "print(\"All Models Performance:\")\n",
    "display(results_df)\n",
    "\n",
    "# 打印特征重要性\n",
    "print(\"\\n=== Feature Importances ===\")\n",
    "for i, name in enumerate(model_names):\n",
    "    if feature_importances[i] is not None:\n",
    "        print(f\"\\nModel: {name}\")\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': feature_importances[i]\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        display(importance_df.head(20))"
   ],
   "id": "50bb7f2ed3a38293",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from joblib import dump, load  # 用于保存和加载模型\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置绘图样式\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.linewidth': 2.5,\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "})\n",
    "\n",
    "# 初始化列表存储分类指标\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "execution_times = []\n",
    "model_names = []\n",
    "best_params_list = []  # 存储最佳参数\n",
    "feature_importances = []  # 存储特征重要性\n",
    "\n",
    "# 定义分类模型\n",
    "models = [\n",
    "    ('RF', RandomForestClassifier()),\n",
    "    ('XGBoost', XGBClassifier()),\n",
    "    ('LightGBM', LGBMClassifier())\n",
    "]\n",
    "\n",
    "# 定义超参数网格\n",
    "param_grids = {\n",
    "    'RF': {'n_estimators': [10], 'max_depth': [None], 'class_weight': ['balanced']},\n",
    "    'XGBoost': {'n_estimators': [10, 30, 50, 70, 100], 'learning_rate': [0.005, 0.01, 0.05, 0.1], 'scale_pos_weight': [1, 10, 100]},\n",
    "    'LightGBM': {'n_estimators': [10, 30, 50, 70, 100], 'learning_rate': [0.005, 0.01, 0.05, 0.1], 'is_unbalance': [True, False]}\n",
    "}\n",
    "\n",
    "# 尝试加载优化后的RF模型\n",
    "try:\n",
    "    print(\"尝试加载优化后的RF模型...\")\n",
    "    optimized_model_info = load('rf_balanced_optimized_latest.pkl')\n",
    "    optimized_rf_model = optimized_model_info['model']\n",
    "    rf_best_params = optimized_model_info['best_params']\n",
    "    print(\"优化后的RF模型加载成功！\")\n",
    "    print(\"最佳参数:\", rf_best_params)\n",
    "\n",
    "    # 使用优化后的RF模型进行预测和评估\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 预测\n",
    "    y_pred_optimized = optimized_rf_model.predict(X_test)\n",
    "    y_pred_proba_optimized = optimized_rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # 计算分类指标\n",
    "    accuracy_opt = accuracy_score(y_test, y_pred_optimized)\n",
    "    precision_opt = precision_score(y_test, y_pred_optimized)\n",
    "    recall_opt = recall_score(y_test, y_pred_optimized)\n",
    "    f1_opt = f1_score(y_test, y_pred_optimized)\n",
    "    roc_auc_opt = roc_auc_score(y_test, y_pred_proba_optimized)\n",
    "\n",
    "    execution_time_opt = time.time() - start_time\n",
    "\n",
    "    # 存储优化模型的指标\n",
    "    model_names.append('RF_Optimized')\n",
    "    accuracy_scores.append(accuracy_opt)\n",
    "    precision_scores.append(precision_opt)\n",
    "    recall_scores.append(recall_opt)\n",
    "    f1_scores.append(f1_opt)\n",
    "    roc_auc_scores.append(roc_auc_opt)\n",
    "    execution_times.append(execution_time_opt)\n",
    "    best_params_list.append(rf_best_params)\n",
    "\n",
    "    # 获取优化模型的特征重要性\n",
    "    feature_importances_opt = optimized_rf_model.feature_importances_\n",
    "    feature_importances.append(feature_importances_opt)\n",
    "\n",
    "    print(\"优化后的RF模型评估完成！\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"优化后的RF模型文件未找到，将使用标准RF模型...\")\n",
    "    optimized_rf_model = None\n",
    "\n",
    "# 主循环 - 训练其他模型\n",
    "for name, classifier in models:\n",
    "    # 如果是RF模型且已经加载了优化版本，则跳过标准训练\n",
    "    if name == 'RF' and optimized_rf_model is not None:\n",
    "        print(\"跳过标准RF模型训练，使用优化版本...\")\n",
    "        continue\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 超参数调优\n",
    "    if param_grids.get(name):\n",
    "        print(f\"正在对{name}进行超参数调优...\")\n",
    "        grid_search = GridSearchCV(classifier, param_grid=param_grids[name], cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_  # 获取最佳参数\n",
    "    else:\n",
    "        best_model = classifier\n",
    "        best_model.fit(X_train, y_train)\n",
    "        best_params = {}  # 如果没有调优，最佳参数为空字典\n",
    "\n",
    "    # 预测\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # 计算分类指标\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "\n",
    "    # 记录执行时间\n",
    "    execution_time = time.time() - start_time\n",
    "    execution_times.append(execution_time)\n",
    "\n",
    "    # 记录模型名称和最佳参数\n",
    "    model_names.append(name)\n",
    "    best_params_list.append(best_params)\n",
    "\n",
    "    # 获取特征重要性\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importances = best_model.feature_importances_\n",
    "        feature_importances.append(importances)\n",
    "    elif hasattr(best_model, 'coef_'):\n",
    "        importances = best_model.coef_[0]  # 对于线性模型，取第一个类的系数\n",
    "        feature_importances.append(importances)\n",
    "    else:\n",
    "        feature_importances.append(None)\n",
    "\n",
    "    print(f\"{name}模型训练和评估完成！\")\n",
    "\n",
    "# 保存结果\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores,\n",
    "    'ROC-AUC': roc_auc_scores,\n",
    "    'Execution Time (s)': execution_times,\n",
    "    'Best Parameters': best_params_list\n",
    "})\n",
    "\n",
    "results_df.to_csv('D:/code/junma/600000/0424/model_performance.csv', index=False)  # 保存模型性能结果到CSV文件\n",
    "\n",
    "print(\"\\n所有模型性能比较:\")\n",
    "display(results_df)\n",
    "\n",
    "# 特征重要性分析和可视化\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"特征重要性分析\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 为每个模型创建特征重要性分析\n",
    "for i, name in enumerate(model_names):\n",
    "    if feature_importances[i] is not None:\n",
    "        print(f\"\\n--- {name} 模型特征重要性 ---\")\n",
    "\n",
    "        # 创建特征重要性DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': feature_importances[i]\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "\n",
    "        # 显示前20个最重要的特征\n",
    "        print(f\"前20个最重要特征:\")\n",
    "        display(importance_df.head(20))\n",
    "\n",
    "        # 保存特征重要性到CSV\n",
    "        csv_filename = f'D:/code/junma/600000/0424/{name.lower()}_feature_importance.csv'\n",
    "        importance_df.to_csv(csv_filename, index=False)\n",
    "        print(f\"特征重要性已保存到: {csv_filename}\")\n",
    "\n",
    "        # 创建特征重要性可视化\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = importance_df.head(15)\n",
    "\n",
    "        # 水平条形图\n",
    "        plt.barh(range(len(top_features)), top_features['Importance'],\n",
    "                color='skyblue', edgecolor='black', linewidth=1.2)\n",
    "        plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "        plt.xlabel('特征重要性', fontweight='bold', fontsize=12)\n",
    "        plt.title(f'{name}模型 - 前15个最重要特征', fontweight='bold', fontsize=14)\n",
    "        plt.gca().invert_yaxis()  # 最重要的特征在顶部\n",
    "\n",
    "        # 添加数值标签\n",
    "        for j, v in enumerate(top_features['Importance']):\n",
    "            plt.text(v + 0.001, j, f'{v:.4f}', va='center', fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # 保存图像\n",
    "        img_filename = f'D:/code/junma/600000/0424/{name.lower()}_feature_importance.png'\n",
    "        plt.savefig(img_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"特征重要性图已保存到: {img_filename}\")\n",
    "        plt.show()\n",
    "\n",
    "# 创建所有模型特征重要性比较（如果特征重要性可用）\n",
    "print(\"\\n--- 所有模型特征重要性比较 ---\")\n",
    "\n",
    "# 找出所有模型共同的重要特征\n",
    "common_important_features = {}\n",
    "for i, name in enumerate(model_names):\n",
    "    if feature_importances[i] is not None:\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': feature_importances[i]\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "\n",
    "        # 获取每个模型的前10个重要特征\n",
    "        top_features = importance_df.head(10)['Feature'].tolist()\n",
    "        common_important_features[name] = set(top_features)\n",
    "\n",
    "        print(f\"{name} 前10重要特征: {top_features}\")\n",
    "\n",
    "# 找出共同的重要特征\n",
    "if len(common_important_features) > 1:\n",
    "    common_features = set.intersection(*common_important_features.values())\n",
    "    print(f\"\\n所有模型共同的重要特征: {list(common_features)}\")\n",
    "\n",
    "# 创建特征重要性比较热图（如果多个模型都有特征重要性）\n",
    "models_with_importance = [name for i, name in enumerate(model_names)\n",
    "                         if feature_importances[i] is not None]\n",
    "\n",
    "if len(models_with_importance) >= 2:\n",
    "    print(f\"\\n创建特征重要性比较热图...\")\n",
    "\n",
    "    # 选择前15个特征（基于第一个模型的重要性）\n",
    "    first_model_idx = model_names.index(models_with_importance[0])\n",
    "    top_features_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': feature_importances[first_model_idx]\n",
    "    }).sort_values('Importance', ascending=False).head(15)\n",
    "    top_features = top_features_df['Feature'].tolist()\n",
    "\n",
    "    # 创建比较DataFrame\n",
    "    comparison_data = []\n",
    "    for feature in top_features:\n",
    "        row = {'Feature': feature}\n",
    "        for model_name in models_with_importance:\n",
    "            model_idx = model_names.index(model_name)\n",
    "            feature_idx = list(X_train.columns).index(feature)\n",
    "            importance_value = feature_importances[model_idx][feature_idx]\n",
    "            row[model_name] = importance_value\n",
    "        comparison_data.append(row)\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.set_index('Feature')\n",
    "\n",
    "    # 创建热图\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(comparison_df, annot=True, cmap='YlOrRd', fmt='.4f',\n",
    "                linewidths=1, linecolor='black', cbar_kws={'label': '特征重要性'})\n",
    "    plt.title('不同模型间特征重要性比较', fontweight='bold', fontsize=16, pad=20)\n",
    "    plt.xlabel('模型', fontweight='bold', fontsize=12)\n",
    "    plt.ylabel('特征', fontweight='bold', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存热图\n",
    "    heatmap_filename = 'D:/code/junma/600000/0424/feature_importance_comparison_heatmap.png'\n",
    "    plt.savefig(heatmap_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"特征重要性比较热图已保存到: {heatmap_filename}\")\n",
    "    plt.show()\n",
    "\n",
    "    # 保存比较数据\n",
    "    comparison_df.to_csv('D:/code/junma/600000/0424/feature_importance_comparison.csv')\n",
    "    print(\"特征重要性比较数据已保存到CSV文件\")\n",
    "\n",
    "print(\"\\n所有模型训练和特征重要性分析完成！\")"
   ],
   "id": "228152e035138399",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 加载优化后的模型进行最终预测\n",
    "try:\n",
    "    print(\"加载优化后的模型进行最终预测...\")\n",
    "\n",
    "    # 尝试加载优化后的模型\n",
    "    optimized_model_info = joblib.load('rf_balanced_optimized_latest.pkl')\n",
    "    final_model = optimized_model_info['model']\n",
    "    best_params = optimized_model_info['best_params']\n",
    "\n",
    "    print(\"优化模型加载成功！\")\n",
    "    print(f\"使用的最佳参数: {best_params}\")\n",
    "\n",
    "    # 使用优化后的模型进行预测\n",
    "    y_final_pred = final_model.predict(X_test)\n",
    "    y_final_pred_proba = final_model.predict_proba(X_test)[:, 1]  # 预测概率\n",
    "\n",
    "    # 确保数据类型一致\n",
    "    final_y_pred = y_final_pred.astype(int)\n",
    "    final_y_test = y_test.astype(int)\n",
    "\n",
    "    # 创建完整的预测结果DataFrame\n",
    "    results_full = pd.DataFrame({\n",
    "        'Predicted_Value': final_y_pred,\n",
    "        'True_Value': final_y_test,\n",
    "        'Prediction_Probability': y_final_pred_proba,\n",
    "        'Prediction_Correct': final_y_pred == final_y_test\n",
    "    })\n",
    "\n",
    "    # 添加预测状态描述\n",
    "    results_full['Prediction_Status'] = results_full['Prediction_Correct'].map({\n",
    "        True: '正确预测',\n",
    "        False: '错误预测'\n",
    "    })\n",
    "\n",
    "    # 添加类别描述\n",
    "    results_full['True_Label'] = results_full['True_Value'].map({0: '正常', 1: '断纱'})\n",
    "    results_full['Predicted_Label'] = results_full['Predicted_Value'].map({0: '正常', 1: '断纱'})\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"优化模型预测结果总览\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 总体统计\n",
    "    total_samples = len(results_full)\n",
    "    correct_predictions = results_full['Prediction_Correct'].sum()\n",
    "    overall_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f\"总样本数: {total_samples}\")\n",
    "    print(f\"正确预测数: {correct_predictions}\")\n",
    "    print(f\"总体准确率: {overall_accuracy:.4f} ({overall_accuracy:.2%})\")\n",
    "\n",
    "    # 1. 断纱预测结果分析 (预测为1的样本)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"断纱预测结果分析 (预测值 = 1)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    results_break = results_full[results_full['Predicted_Value'] == 1].copy()\n",
    "    results_break['Difference'] = results_break['True_Value'] - results_break['Predicted_Value']\n",
    "\n",
    "    break_total = len(results_break)\n",
    "    break_correct = (results_break['True_Value'] == 1).sum()\n",
    "    break_incorrect = (results_break['True_Value'] == 0).sum()\n",
    "    break_accuracy = break_correct / break_total if break_total > 0 else 0\n",
    "\n",
    "    print(f\"预测为断纱的样本总数: {break_total}\")\n",
    "    print(f\"其中实际为断纱(正确预测): {break_correct}\")\n",
    "    print(f\"其中实际为正常(错误预测 - 误报): {break_incorrect}\")\n",
    "    print(f\"断纱预测准确率: {break_accuracy:.4f} ({break_accuracy:.2%})\")\n",
    "\n",
    "    # 显示断纱预测的前后各5行\n",
    "    print(f\"\\n前5个断纱预测样本:\")\n",
    "    break_display_cols = ['Predicted_Label', 'True_Label', 'Prediction_Probability', 'Prediction_Status']\n",
    "    display(results_break[break_display_cols].head())\n",
    "\n",
    "    print(f\"\\n后5个断纱预测样本:\")\n",
    "    display(results_break[break_display_cols].tail())\n",
    "\n",
    "    # 2. 正常预测结果分析 (预测为0的样本)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"正常预测结果分析 (预测值 = 0)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    results_normal = results_full[results_full['Predicted_Value'] == 0].copy()\n",
    "    results_normal['Difference'] = results_normal['True_Value'] - results_normal['Predicted_Value']\n",
    "\n",
    "    normal_total = len(results_normal)\n",
    "    normal_correct = (results_normal['True_Value'] == 0).sum()\n",
    "    normal_incorrect = (results_normal['True_Value'] == 1).sum()\n",
    "    normal_accuracy = normal_correct / normal_total if normal_total > 0 else 0\n",
    "\n",
    "    print(f\"预测为正常的样本总数: {normal_total}\")\n",
    "    print(f\"其中实际为正常(正确预测): {normal_correct}\")\n",
    "    print(f\"其中实际为断纱(错误预测 - 漏报): {normal_incorrect}\")\n",
    "    print(f\"正常预测准确率: {normal_accuracy:.4f} ({normal_accuracy:.2%})\")\n",
    "\n",
    "    # 显示正常预测的前后各5行\n",
    "    print(f\"\\n前5个正常预测样本:\")\n",
    "    normal_display_cols = ['Predicted_Label', 'True_Label', 'Prediction_Probability', 'Prediction_Status']\n",
    "    display(results_normal[normal_display_cols].head())\n",
    "\n",
    "    print(f\"\\n后5个正常预测样本:\")\n",
    "    display(results_normal[normal_display_cols].tail())\n",
    "\n",
    "    # 3. 详细错误分析\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"详细错误分析\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 误报分析 (预测为1但实际为0)\n",
    "    false_positives = results_break[results_break['True_Value'] == 0]\n",
    "    print(f\"误报数量 (预测断纱但实际正常): {len(false_positives)}\")\n",
    "    if len(false_positives) > 0:\n",
    "        print(\"误报样本详情:\")\n",
    "        display(false_positives[['Prediction_Probability', 'True_Label', 'Predicted_Label']].head(10))\n",
    "\n",
    "    # 漏报分析 (预测为0但实际为1)\n",
    "    false_negatives = results_normal[results_normal['True_Value'] == 1]\n",
    "    print(f\"\\n漏报数量 (预测正常但实际断纱): {len(false_negatives)}\")\n",
    "    if len(false_negatives) > 0:\n",
    "        print(\"漏报样本详情:\")\n",
    "        display(false_negatives[['Prediction_Probability', 'True_Label', 'Predicted_Label']].head(10))\n",
    "\n",
    "    # 4. 预测概率分布分析\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"预测概率分布分析\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 正确预测的概率分布\n",
    "    correct_probabilities = results_full[results_full['Prediction_Correct'] == True]['Prediction_Probability']\n",
    "    incorrect_probabilities = results_full[results_full['Prediction_Correct'] == False]['Prediction_Probability']\n",
    "\n",
    "    print(f\"正确预测的平均概率: {correct_probabilities.mean():.4f}\")\n",
    "    print(f\"错误预测的平均概率: {incorrect_probabilities.mean():.4f}\")\n",
    "    print(f\"正确预测的概率标准差: {correct_probabilities.std():.4f}\")\n",
    "    print(f\"错误预测的概率标准差: {incorrect_probabilities.std():.4f}\")\n",
    "\n",
    "    # 按类别统计概率\n",
    "    for true_class in [0, 1]:\n",
    "        class_data = results_full[results_full['True_Value'] == true_class]\n",
    "        class_name = '正常' if true_class == 0 else '断纱'\n",
    "        print(f\"\\n{class_name}样本的预测概率统计:\")\n",
    "        print(f\"  平均概率: {class_data['Prediction_Probability'].mean():.4f}\")\n",
    "        print(f\"  概率中位数: {class_data['Prediction_Probability'].median():.4f}\")\n",
    "        print(f\"  概率标准差: {class_data['Prediction_Probability'].std():.4f}\")\n",
    "\n",
    "    # 5. 保存详细结果\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"保存预测结果\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 保存完整结果\n",
    "    results_full.to_csv('optimized_model_complete_predictions.csv', index=False)\n",
    "    print(\"完整预测结果已保存至: optimized_model_complete_predictions.csv\")\n",
    "\n",
    "    # 保存断纱预测结果\n",
    "    results_break.to_csv('optimized_model_break_predictions.csv', index=False)\n",
    "    print(\"断纱预测结果已保存至: optimized_model_break_predictions.csv\")\n",
    "\n",
    "    # 保存正常预测结果\n",
    "    results_normal.to_csv('optimized_model_normal_predictions.csv', index=False)\n",
    "    print(\"正常预测结果已保存至: optimized_model_normal_predictions.csv\")\n",
    "\n",
    "    # 保存错误分析结果\n",
    "    error_analysis = pd.concat([false_positives, false_negatives])\n",
    "    if len(error_analysis) > 0:\n",
    "        error_analysis.to_csv('optimized_model_error_analysis.csv', index=False)\n",
    "        print(\"错误分析结果已保存至: optimized_model_error_analysis.csv\")\n",
    "\n",
    "    # 6. 生成预测结果汇总报告\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"预测结果汇总报告\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    summary_report = {\n",
    "        '总样本数': total_samples,\n",
    "        '正确预测数': correct_predictions,\n",
    "        '总体准确率': f\"{overall_accuracy:.4f} ({overall_accuracy:.2%})\",\n",
    "        '断纱预测总数': break_total,\n",
    "        '断纱正确预测数': break_correct,\n",
    "        '断纱误报数': break_incorrect,\n",
    "        '断纱预测准确率': f\"{break_accuracy:.4f} ({break_accuracy:.2%})\",\n",
    "        '正常预测总数': normal_total,\n",
    "        '正常正确预测数': normal_correct,\n",
    "        '正常漏报数': normal_incorrect,\n",
    "        '正常预测准确率': f\"{normal_accuracy:.4f} ({normal_accuracy:.2%})\",\n",
    "        '误报率 (False Positive Rate)': f\"{break_incorrect/break_total:.4f} ({break_incorrect/break_total:.2%})\" if break_total > 0 else \"N/A\",\n",
    "        '漏报率 (False Negative Rate)': f\"{normal_incorrect/normal_total:.4f} ({normal_incorrect/normal_total:.2%})\" if normal_total > 0 else \"N/A\",\n",
    "        '使用模型': 'RF_Optimized',\n",
    "        '最佳参数': str(best_params)\n",
    "    }\n",
    "\n",
    "    summary_df = pd.DataFrame(list(summary_report.items()), columns=['指标', '值'])\n",
    "    print(\"预测结果汇总:\")\n",
    "    display(summary_df)\n",
    "\n",
    "    # 保存汇总报告\n",
    "    summary_df.to_csv('optimized_model_prediction_summary.csv', index=False)\n",
    "    print(\"\\n预测汇总报告已保存至: optimized_model_prediction_summary.csv\")\n",
    "\n",
    "    # 7. 可视化预测结果 (可选)\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "\n",
    "        print(\"\\n生成预测结果可视化...\")\n",
    "\n",
    "        # 设置绘图样式\n",
    "        plt.style.use('default')\n",
    "        plt.rcParams.update({\n",
    "            'font.family': 'Arial',\n",
    "            'font.weight': 'bold',\n",
    "            'axes.labelweight': 'bold',\n",
    "            'axes.titleweight': 'bold',\n",
    "            'font.size': 10\n",
    "        })\n",
    "\n",
    "        # 创建预测结果分布图\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "        # 1. 预测概率分布\n",
    "        axes[0,0].hist(results_full['Prediction_Probability'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0,0].set_xlabel('预测概率')\n",
    "        axes[0,0].set_ylabel('频数')\n",
    "        axes[0,0].set_title('预测概率分布')\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "        # 2. 正确与错误预测的概率分布\n",
    "        axes[0,1].hist(correct_probabilities, bins=30, alpha=0.7, label='正确预测', color='green')\n",
    "        axes[0,1].hist(incorrect_probabilities, bins=30, alpha=0.7, label='错误预测', color='red')\n",
    "        axes[0,1].set_xlabel('预测概率')\n",
    "        axes[0,1].set_ylabel('频数')\n",
    "        axes[0,1].set_title('正确vs错误预测的概率分布')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "        # 3. 类别分布饼图\n",
    "        prediction_counts = results_full['Predicted_Label'].value_counts()\n",
    "        axes[1,0].pie(prediction_counts.values, labels=prediction_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "        axes[1,0].set_title('预测类别分布')\n",
    "\n",
    "        # 4. 准确率条形图\n",
    "        accuracy_data = [overall_accuracy, break_accuracy, normal_accuracy]\n",
    "        accuracy_labels = ['总体准确率', '断纱预测准确率', '正常预测准确率']\n",
    "        bars = axes[1,1].bar(accuracy_labels, accuracy_data, color=['blue', 'red', 'green'])\n",
    "        axes[1,1].set_ylabel('准确率')\n",
    "        axes[1,1].set_title('各类别预测准确率')\n",
    "        axes[1,1].set_ylim(0, 1)\n",
    "\n",
    "        # 在条形图上添加数值标签\n",
    "        for bar, acc in zip(bars, accuracy_data):\n",
    "            height = bar.get_height()\n",
    "            axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                          f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('optimized_model_prediction_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"预测分析图已保存至: optimized_model_prediction_analysis.png\")\n",
    "        plt.show()\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Matplotlib/Seaborn 不可用，跳过可视化部分\")\n",
    "\n",
    "    print(\"\\n优化模型预测分析完成！\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"错误: 优化模型文件 'rf_balanced_optimized_latest.pkl' 未找到\")\n",
    "    print(\"请先运行模型优化代码\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"预测过程中发生错误: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ],
   "id": "6928fc8698f8b3af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from joblib import dump, load  # 用于保存和加载模型\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置绘图样式\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.linewidth': 2.5,\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "})\n",
    "\n",
    "# 初始化列表存储分类指标\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "execution_times = []\n",
    "model_names = []\n",
    "best_params_list = []  # 存储最佳参数\n",
    "feature_importances = []  # 存储特征重要性\n",
    "\n",
    "# 定义分类模型\n",
    "models = [\n",
    "    ('RF', RandomForestClassifier()),\n",
    "    ('XGBoost', XGBClassifier()),\n",
    "    ('LightGBM', LGBMClassifier())\n",
    "]\n",
    "\n",
    "# 定义超参数网格\n",
    "param_grids = {\n",
    "    'RF': {'n_estimators': [10], 'max_depth': [None], 'class_weight': ['balanced']},\n",
    "    'XGBoost': {'n_estimators': [10, 30, 50, 70, 100], 'learning_rate': [0.005, 0.01, 0.05, 0.1], 'scale_pos_weight': [1, 10, 100]},\n",
    "    'LightGBM': {'n_estimators': [10, 30, 50, 70, 100], 'learning_rate': [0.005, 0.01, 0.05, 0.1], 'is_unbalance': [True, False]}\n",
    "}\n",
    "\n",
    "# 尝试加载优化后的RF模型\n",
    "try:\n",
    "    print(\"尝试加载优化后的RF模型...\")\n",
    "    optimized_model_info = load('rf_balanced_optimized_latest.pkl')\n",
    "    optimized_rf_model = optimized_model_info['model']\n",
    "    rf_best_params = optimized_model_info['best_params']\n",
    "    print(\"优化后的RF模型加载成功！\")\n",
    "    print(\"最佳参数:\", rf_best_params)\n",
    "\n",
    "    # 使用优化后的RF模型进行预测和评估\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 预测\n",
    "    y_pred_optimized = optimized_rf_model.predict(X_test)\n",
    "    y_pred_proba_optimized = optimized_rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # 计算分类指标\n",
    "    accuracy_opt = accuracy_score(y_test, y_pred_optimized)\n",
    "    precision_opt = precision_score(y_test, y_pred_optimized)\n",
    "    recall_opt = recall_score(y_test, y_pred_optimized)\n",
    "    f1_opt = f1_score(y_test, y_pred_optimized)\n",
    "    roc_auc_opt = roc_auc_score(y_test, y_pred_proba_optimized)\n",
    "\n",
    "    execution_time_opt = time.time() - start_time\n",
    "\n",
    "    # 存储优化模型的指标\n",
    "    model_names.append('RF_Optimized')\n",
    "    accuracy_scores.append(accuracy_opt)\n",
    "    precision_scores.append(precision_opt)\n",
    "    recall_scores.append(recall_opt)\n",
    "    f1_scores.append(f1_opt)\n",
    "    roc_auc_scores.append(roc_auc_opt)\n",
    "    execution_times.append(execution_time_opt)\n",
    "    best_params_list.append(rf_best_params)\n",
    "\n",
    "    # 获取优化模型的特征重要性\n",
    "    feature_importances_opt = optimized_rf_model.feature_importances_\n",
    "    feature_importances.append(feature_importances_opt)\n",
    "\n",
    "    print(\"优化后的RF模型评估完成！\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"优化后的RF模型文件未找到，将使用标准RF模型...\")\n",
    "    optimized_rf_model = None\n",
    "\n",
    "# 主循环 - 训练其他模型\n",
    "for name, classifier in models:\n",
    "    # 如果是RF模型且已经加载了优化版本，则跳过标准训练\n",
    "    if name == 'RF' and optimized_rf_model is not None:\n",
    "        print(\"跳过标准RF模型训练，使用优化版本...\")\n",
    "        continue\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 超参数调优\n",
    "    if param_grids.get(name):\n",
    "        print(f\"正在对{name}进行超参数调优...\")\n",
    "        grid_search = GridSearchCV(classifier, param_grid=param_grids[name], cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_  # 获取最佳参数\n",
    "    else:\n",
    "        best_model = classifier\n",
    "        best_model.fit(X_train, y_train)\n",
    "        best_params = {}  # 如果没有调优，最佳参数为空字典\n",
    "\n",
    "    # 预测\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # 计算分类指标\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "\n",
    "    # 记录执行时间\n",
    "    execution_time = time.time() - start_time\n",
    "    execution_times.append(execution_time)\n",
    "\n",
    "    # 记录模型名称和最佳参数\n",
    "    model_names.append(name)\n",
    "    best_params_list.append(best_params)\n",
    "\n",
    "    # 获取特征重要性\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importances = best_model.feature_importances_\n",
    "        feature_importances.append(importances)\n",
    "    elif hasattr(best_model, 'coef_'):\n",
    "        importances = best_model.coef_[0]  # 对于线性模型，取第一个类的系数\n",
    "        feature_importances.append(importances)\n",
    "    else:\n",
    "        feature_importances.append(None)\n",
    "\n",
    "    print(f\"{name}模型训练和评估完成！\")\n",
    "\n",
    "# 保存结果\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores,\n",
    "    'ROC-AUC': roc_auc_scores,\n",
    "    'Execution Time (s)': execution_times,\n",
    "    'Best Parameters': best_params_list\n",
    "})\n",
    "\n",
    "results_df.to_csv('D:/code/junma/600000/0424/model_performance.csv', index=False)  # 保存模型性能结果到CSV文件\n",
    "\n",
    "print(\"\\n所有模型性能比较:\")\n",
    "display(results_df)\n",
    "\n",
    "# 特征重要性分析和可视化\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"特征重要性分析\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 为每个模型创建特征重要性分析\n",
    "for i, name in enumerate(model_names):\n",
    "    if feature_importances[i] is not None:\n",
    "        print(f\"\\n--- {name} 模型特征重要性 ---\")\n",
    "\n",
    "        # 创建特征重要性DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': feature_importances[i]\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "\n",
    "        # 显示前20个最重要的特征\n",
    "        print(f\"前20个最重要特征:\")\n",
    "        display(importance_df.head(20))\n",
    "\n",
    "        # 保存特征重要性到CSV\n",
    "        csv_filename = f'D:/code/junma/600000/0424/{name.lower()}_feature_importance.csv'\n",
    "        importance_df.to_csv(csv_filename, index=False)\n",
    "        print(f\"特征重要性已保存到: {csv_filename}\")\n",
    "\n",
    "        # 创建特征重要性可视化\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = importance_df.head(15)\n",
    "\n",
    "        # 水平条形图\n",
    "        plt.barh(range(len(top_features)), top_features['Importance'],\n",
    "                color='skyblue', edgecolor='black', linewidth=1.2)\n",
    "        plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "        plt.xlabel('特征重要性', fontweight='bold', fontsize=12)\n",
    "        plt.title(f'{name}模型 - 前15个最重要特征', fontweight='bold', fontsize=14)\n",
    "        plt.gca().invert_yaxis()  # 最重要的特征在顶部\n",
    "\n",
    "        # 添加数值标签\n",
    "        for j, v in enumerate(top_features['Importance']):\n",
    "            plt.text(v + 0.001, j, f'{v:.4f}', va='center', fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # 保存图像\n",
    "        img_filename = f'D:/code/junma/600000/0424/{name.lower()}_feature_importance.png'\n",
    "        plt.savefig(img_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"特征重要性图已保存到: {img_filename}\")\n",
    "        plt.show()\n",
    "\n",
    "# 创建所有模型特征重要性比较（如果特征重要性可用）\n",
    "print(\"\\n--- 所有模型特征重要性比较 ---\")\n",
    "\n",
    "# 找出所有模型共同的重要特征\n",
    "common_important_features = {}\n",
    "for i, name in enumerate(model_names):\n",
    "    if feature_importances[i] is not None:\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': feature_importances[i]\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "\n",
    "        # 获取每个模型的前10个重要特征\n",
    "        top_features = importance_df.head(10)['Feature'].tolist()\n",
    "        common_important_features[name] = set(top_features)\n",
    "\n",
    "        print(f\"{name} 前10重要特征: {top_features}\")\n",
    "\n",
    "# 找出共同的重要特征\n",
    "if len(common_important_features) > 1:\n",
    "    common_features = set.intersection(*common_important_features.values())\n",
    "    print(f\"\\n所有模型共同的重要特征: {list(common_features)}\")\n",
    "\n",
    "# 创建特征重要性比较热图（如果多个模型都有特征重要性）\n",
    "models_with_importance = [name for i, name in enumerate(model_names)\n",
    "                         if feature_importances[i] is not None]\n",
    "\n",
    "if len(models_with_importance) >= 2:\n",
    "    print(f\"\\n创建特征重要性比较热图...\")\n",
    "\n",
    "    # 选择前15个特征（基于第一个模型的重要性）\n",
    "    first_model_idx = model_names.index(models_with_importance[0])\n",
    "    top_features_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': feature_importances[first_model_idx]\n",
    "    }).sort_values('Importance', ascending=False).head(15)\n",
    "    top_features = top_features_df['Feature'].tolist()\n",
    "\n",
    "    # 创建比较DataFrame\n",
    "    comparison_data = []\n",
    "    for feature in top_features:\n",
    "        row = {'Feature': feature}\n",
    "        for model_name in models_with_importance:\n",
    "            model_idx = model_names.index(model_name)\n",
    "            feature_idx = list(X_train.columns).index(feature)\n",
    "            importance_value = feature_importances[model_idx][feature_idx]\n",
    "            row[model_name] = importance_value\n",
    "        comparison_data.append(row)\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.set_index('Feature')\n",
    "\n",
    "    # 创建热图\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(comparison_df, annot=True, cmap='YlOrRd', fmt='.4f',\n",
    "                linewidths=1, linecolor='black', cbar_kws={'label': '特征重要性'})\n",
    "    plt.title('不同模型间特征重要性比较', fontweight='bold', fontsize=16, pad=20)\n",
    "    plt.xlabel('模型', fontweight='bold', fontsize=12)\n",
    "    plt.ylabel('特征', fontweight='bold', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存热图\n",
    "    heatmap_filename = 'D:/code/junma/600000/0424/feature_importance_comparison_heatmap.png'\n",
    "    plt.savefig(heatmap_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"特征重要性比较热图已保存到: {heatmap_filename}\")\n",
    "    plt.show()\n",
    "\n",
    "    # 保存比较数据\n",
    "    comparison_df.to_csv('D:/code/junma/600000/0424/feature_importance_comparison.csv')\n",
    "    print(\"特征重要性比较数据已保存到CSV文件\")\n",
    "\n",
    "print(\"\\n所有模型训练和特征重要性分析完成！\")"
   ],
   "id": "a2a33aeb9b920cfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                            f1_score, roc_auc_score, confusion_matrix)\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import dump\n",
    "\n",
    "# 初始化列表存储分类指标\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "execution_times = []\n",
    "model_names = []\n",
    "best_params_list = []\n",
    "feature_importances = []\n",
    "confusion_matrices = []  # 存储混淆矩阵\n",
    "\n",
    "# 定义分类模型\n",
    "models = [\n",
    "    ('RF', RandomForestClassifier()),\n",
    "    ('XGBoost', XGBClassifier()),\n",
    "    ('LightGBM', LGBMClassifier())\n",
    "]\n",
    "\n",
    "# 定义超参数网格\n",
    "param_grids = {\n",
    "    'RF': {'n_estimators': [10], 'max_depth': [20], 'class_weight': ['balanced']},\n",
    "    'XGBoost': {'n_estimators': [10, 30, 50, 70, 100],\n",
    "                'learning_rate': [0.005, 0.01, 0.05, 0.1],\n",
    "                'scale_pos_weight': [1, 10, 100]},\n",
    "    'LightGBM': {'n_estimators': [10, 30, 50, 70, 100],\n",
    "                 'learning_rate': [0.005, 0.01, 0.05, 0.1],\n",
    "                 'is_unbalance': [True, False]}\n",
    "}\n",
    "\n",
    "# 主循环\n",
    "for name, classifier in models:\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "\n",
    "    # 超参数调优\n",
    "    if param_grids.get(name):\n",
    "        grid_search = GridSearchCV(classifier, param_grid=param_grids[name],\n",
    "                                  cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "    else:\n",
    "        best_model = classifier\n",
    "        best_model.fit(X_train, y_train)\n",
    "        best_params = {}\n",
    "\n",
    "    # 预测\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # 计算分类指标\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # 计算混淆矩阵\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    confusion_matrices.append(cm)\n",
    "\n",
    "    # 存储结果\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    execution_times.append(time.time() - start_time)\n",
    "    model_names.append(name)\n",
    "    best_params_list.append(best_params)\n",
    "\n",
    "    # 获取特征重要性\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importances = best_model.feature_importances_\n",
    "        feature_importances.append(importances)\n",
    "    elif hasattr(best_model, 'coef_'):\n",
    "        importances = best_model.coef_[0]\n",
    "        feature_importances.append(importances)\n",
    "    else:\n",
    "        feature_importances.append(None)\n",
    "\n",
    "    # 保存模型\n",
    "    dump(best_model, f'{name.lower()}_model.joblib')\n",
    "\n",
    "# 保存结果\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores,\n",
    "    'ROC-AUC': roc_auc_scores,\n",
    "    'Execution Time (s)': execution_times,\n",
    "    'Best Parameters': best_params_list\n",
    "})\n",
    "\n",
    "# 打印结果\n",
    "print(\"\\n=== Model Performance Comparison ===\")\n",
    "display(results_df)\n",
    "\n",
    "# 可视化混淆矩阵\n",
    "print(\"\\n=== Confusion Matrices ===\")\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, (name, cm) in enumerate(zip(model_names, confusion_matrices)):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title(f'{name} Confusion Matrix')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 打印特征重要性\n",
    "print(\"\\n=== Feature Importances ===\")\n",
    "for i, name in enumerate(model_names):\n",
    "    if feature_importances[i] is not None:\n",
    "        print(f\"\\nModel: {name}\")\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': feature_importances[i]\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "\n",
    "        # 可视化前10个重要特征\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='Importance', y='Feature',\n",
    "                   data=importance_df.head(20), palette='viridis')\n",
    "        plt.title(f'{name} - Top 20 Important Features')\n",
    "        plt.show()\n",
    "\n",
    "        display(importance_df.head(20))"
   ],
   "id": "32a5edf2777c450",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "# 设置随机种子保证可重复性\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. 准备交叉验证\n",
    "n_folds = 20\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# 2. 初始化存储特征重要性的数组\n",
    "feature_importances = np.zeros((n_folds, X_train.shape[1]))\n",
    "fold_scores = []\n",
    "\n",
    "# 3. 执行交叉验证\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"Processing fold {fold + 1}/{n_folds}\")\n",
    "\n",
    "    # 分割数据\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    # 使用明确的随机森林参数\n",
    "    rf_params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': None,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_leaf': 1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    rf = RandomForestRegressor(**rf_params)\n",
    "    rf.fit(X_tr, y_tr)\n",
    "\n",
    "    # 存储特征重要性\n",
    "    feature_importances[fold] = rf.feature_importances_\n",
    "\n",
    "    # 计算验证集分数\n",
    "    val_pred = rf.predict(X_val)\n",
    "    fold_score = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    fold_scores.append(fold_score)\n",
    "    print(f\"Fold {fold + 1} RMSE: {fold_score:.4f}\")\n",
    "\n",
    "# 4. 计算统计量\n",
    "mean_importance = feature_importances.mean(axis=0)\n",
    "std_importance = feature_importances.std(axis=0)\n",
    "cv_score = np.mean(fold_scores)\n",
    "cv_std = np.std(fold_scores)\n",
    "\n",
    "print(f\"\\nAverage CV RMSE: {cv_score:.4f} (±{cv_std:.4f})\")\n",
    "\n",
    "# 5. 创建特征重要性DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Mean Importance': mean_importance,\n",
    "    'Std Importance': std_importance,\n",
    "    'CV Rank': np.argsort(mean_importance)[::-1] + 1\n",
    "}).sort_values('Mean Importance', ascending=False)\n",
    "\n",
    "# 6. 可视化特征重要性（带标准差）\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = importance_df.head(20)\n",
    "sns.barplot(x='Mean Importance', y='Feature',\n",
    "            data=top_features,\n",
    "            palette='viridis',\n",
    "            hue='Feature',  # 添加hue参数避免警告\n",
    "            legend=False)   # 不显示图例\n",
    "# 手动添加误差条\n",
    "for i, (_, row) in enumerate(top_features.iterrows()):\n",
    "    plt.errorbar(x=row['Mean Importance'], y=i,\n",
    "                 xerr=row['Std Importance'],\n",
    "                 color='black', capsize=3)\n",
    "plt.title(f'Top 20 Feature Importances with {n_folds}-Fold CV Standard Deviation\\n(Random Forest)')\n",
    "plt.xlabel('Mean Importance ± Std Dev')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. 可视化特征重要性热图（所有折叠）\n",
    "plt.figure(figsize=(14, 20))\n",
    "sns.heatmap(feature_importances.T,\n",
    "            cmap='viridis',\n",
    "            yticklabels=X_train.columns,\n",
    "            xticklabels=[f'Fold {i+1}' for i in range(n_folds)])\n",
    "plt.title('Feature Importances Across All CV Folds')\n",
    "plt.xlabel('CV Fold')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. 输出稳定性分析结果\n",
    "print(\"\\n=== Feature Importance Stability Analysis ===\")\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "display(importance_df.head(20))\n",
    "\n",
    "print(\"\\nFeatures with Highest Variability (Std Dev > Mean):\")\n",
    "high_var_features = importance_df[importance_df['Std Importance'] > importance_df['Mean Importance']]\n",
    "if not high_var_features.empty:\n",
    "    display(high_var_features)\n",
    "else:\n",
    "    print(\"No features with std > mean importance\")\n",
    "\n",
    "# 9. 保存完整结果\n",
    "importance_df.to_csv('random_forest_feature_importance_cv_results.csv', index=False)\n",
    "\n",
    "# 10. 交叉验证性能分布可视化\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.boxplot(x=fold_scores)\n",
    "plt.title(f'RMSE Distribution Across {n_folds}-Fold CV\\nMean: {cv_score:.4f} (±{cv_std:.4f})')\n",
    "plt.xlabel('RMSE')\n",
    "plt.show()"
   ],
   "id": "54f434a84073fd07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d9c6da448c680172",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# 5. Analysis and Visualization ==========================================\n",
    "print(\"\\nLHS Experiment Results:\")\n",
    "print(experiment_df.head())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBreakage Rate Statistics:\")\n",
    "print(experiment_df['breakage_rate'].describe())\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(experiment_df['breakage_rate'], bins=20, edgecolor='k')\n",
    "plt.title('Distribution of Predicted Breakage Rates')\n",
    "plt.xlabel('Breakage Rate')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Pairplot for selected parameters\n",
    "plot_params = ['V_dw1', 'P_dw1', 'R_dw21', 'breakage_rate']\n",
    "sns.pairplot(experiment_df[plot_params], diag_kind='kde')\n",
    "plt.suptitle('Parameter Relationships with Breakage Rate', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 4. 帕累托前沿分析 =====================================================\n",
    "\n",
    "# Find optimal solution (minimum breakage rate)\n",
    "optimal_idx = experiment_df['breakage_rate'].idxmin()\n",
    "optimal_params = experiment_df.loc[optimal_idx].to_dict()\n",
    "optimal_rate = optimal_params.pop('breakage_rate')\n",
    "\n",
    "# Calculate gaps between fixed combinations and optimal solution\n",
    "for i, result in enumerate(fixed_results):\n",
    "    gap = result['breakage_rate'] - optimal_rate\n",
    "    print(f\"组合{i+1}与最优解的断纱率差距: {gap:.4f}\")\n",
    "\n",
    "# Plot Pareto front (visualizing two parameters)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=experiment_df, x='R_dw22', y='breakage_rate', hue='R_dw21')\n",
    "for i, combo in enumerate(fixed_combinations):\n",
    "    plt.scatter(combo['R_dw22'], fixed_results[i]['breakage_rate'],\n",
    "               s=200, marker='*', label=f'固定组合{i+1}')\n",
    "plt.scatter(optimal_params['R_dw22'], optimal_rate,\n",
    "           s=200, marker='X', c='red', label='最优解')\n",
    "plt.title('帕累托前沿分析')\n",
    "plt.xlabel('锭速(rpm)')\n",
    "plt.ylabel('断纱率')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 5. Sobol敏感性分析 ===================================================\n",
    "\n",
    "# Define Sobol analysis problem\n",
    "problem = {\n",
    "    'num_vars': len(param_ranges),\n",
    "    'names': list(param_ranges.keys()),\n",
    "    'bounds': [param_ranges[name] for name in param_ranges.keys()]\n",
    "}\n",
    "\n",
    "# Generate samples\n",
    "param_values = saltelli.sample(problem, 512)\n",
    "\n",
    "# Run model to get outputs\n",
    "Y = np.array([simulate_breakage_rate(dict(zip(problem['names'], values)))\n",
    "             for values in param_values])\n",
    "\n",
    "# Perform Sobol analysis\n",
    "Si = sobol.analyze(problem, Y)\n",
    "\n",
    "# Visualize sensitivity results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=Si['ST'], y=problem['names'])\n",
    "plt.title('Sobol总效应指数（参数敏感性）')\n",
    "plt.xlabel('敏感性指数')\n",
    "plt.ylabel('参数')\n",
    "plt.show()\n",
    "\n",
    "# 6. 稳健性评估（蒙特卡洛模拟） ========================================\n",
    "\n",
    "top3_combinations = experiment_df.nsmallest(3, 'breakage_rate')\n",
    "\n",
    "# Define noise injection function\n",
    "def add_noise(value, param_name, noise_level=0.05):\n",
    "    range_width = param_ranges[param_name][1] - param_ranges[param_name][0]\n",
    "    noise = np.random.normal(0, noise_level * range_width)\n",
    "    return max(param_ranges[param_name][0],\n",
    "              min(param_ranges[param_name][1], value + noise))\n",
    "\n",
    "# Monte Carlo simulation for TOP3 combinations\n",
    "n_simulations = 1000\n",
    "robustness_results = {i: [] for i in range(3)}\n",
    "\n",
    "for i, (_, combo) in enumerate(top3_combinations.iterrows()):\n",
    "    for _ in range(n_simulations):\n",
    "        noisy_params = {param: add_noise(combo[param], param)\n",
    "                       for param in param_ranges.keys()}\n",
    "        rate = simulate_breakage_rate(noisy_params)\n",
    "        robustness_results[i].append(rate)\n",
    "\n",
    "# Visualize robustness results\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(3):\n",
    "    sns.kdeplot(robustness_results[i], label=f'TOP{i+1}组合')\n",
    "plt.title('蒙特卡洛稳健性评估（5%噪声）')\n",
    "plt.xlabel('断纱率')\n",
    "plt.ylabel('概率密度')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 7. 生成验证报告 ======================================================\n",
    "\n",
    "print(\"\\n=== 验证报告 ===\")\n",
    "print(f\"\\n最优参数组合: {optimal_params}\")\n",
    "print(f\"最优断纱率: {optimal_rate:.4f}\")\n",
    "\n",
    "print(\"\\n固定组合验证结果:\")\n",
    "for i, result in enumerate(fixed_results):\n",
    "    print(f\"组合{i+1}: {result['params']}\")\n",
    "    print(f\"断纱率: {result['breakage_rate']:.4f} (与最优解差距: {result['breakage_rate']-optimal_rate:.4f})\")\n",
    "\n",
    "print(\"\\nSobol敏感性分析结果:\")\n",
    "for name, st in zip(problem['names'], Si['ST']):\n",
    "    print(f\"{name}: {st:.4f}\")\n",
    "\n",
    "print(\"\\nTOP3组合稳健性评估:\")\n",
    "for i in range(3):\n",
    "    mean_rate = np.mean(robustness_results[i])\n",
    "    std_rate = np.std(robustness_results[i])\n",
    "    print(f\"TOP{i+1}组合 - 平均断纱率: {mean_rate:.4f}, 标准差: {std_rate:.4f}\")"
   ],
   "id": "4ee945139ce81ed4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2dd74c748ea11360",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from joblib import dump, load\n",
    "import time\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "# 加载随机森林模型\n",
    "best_rf_model = load('rf_model.joblib')\n",
    "\n",
    "# 获取特征重要性\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# 打印特征重要性\n",
    "print(\"\\n=== Feature Importances for Random Forest ===\")\n",
    "display(feature_importance_df)\n",
    "\n",
    "# 定义参数范围（根据实际情况调整）\n",
    "param_ranges = {\n",
    "    'V_dw1': [1000, 2000],  # 速度1范围\n",
    "    'P_dw1': [0, 100],      # 满卷率1范围\n",
    "    'R_dw21': [100, 500],   # 半径1范围\n",
    "    # 添加更多参数范围\n",
    "}\n",
    "\n",
    "# 5. 敏感性分析（基于随机森林特征重要性）\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=feature_importance_df['Importance'], y=feature_importance_df['Feature'])\n",
    "plt.title('随机森林特征重要性（敏感性）')\n",
    "plt.xlabel('重要性指数')\n",
    "plt.ylabel('参数')\n",
    "plt.show()\n",
    "\n",
    "# 6. 稳定性评估（蒙特卡洛模拟）\n",
    "top3_combinations = experiment_df.nsmallest(3, 'breakage_rate')\n",
    "\n",
    "# 定义噪声注入函数\n",
    "def add_noise(value, param_name, noise_level=0.05):\n",
    "    range_width = param_ranges[param_name][1] - param_ranges[param_name][0]\n",
    "    noise = np.random.normal(0, noise_level * range_width)\n",
    "    return max(param_ranges[param_name][0],\n",
    "              min(param_ranges[param_name][1], value + noise))\n",
    "\n",
    "# 蒙特卡洛模拟\n",
    "n_simulations = 1000\n",
    "robustness_results = {i: [] for i in range(3)}\n",
    "\n",
    "for i, (_, combo) in enumerate(top3_combinations.iterrows()):\n",
    "    for _ in range(n_simulations):\n",
    "        noisy_params = {param: add_noise(combo[param], param)\n",
    "                       for param in param_ranges.keys()}\n",
    "\n",
    "        # 转换为模型输入格式\n",
    "        input_data = np.array([noisy_params[param] for param in X_train.columns]).reshape(1, -1)\n",
    "\n",
    "        # 预测断纱率\n",
    "        rate = best_rf_model.predict(input_data)[0]\n",
    "        robustness_results[i].append(rate)\n",
    "\n",
    "# 可视化稳健性结果\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(3):\n",
    "    sns.kdeplot(robustness_results[i], label=f'TOP{i+1}组合')\n",
    "plt.title('蒙特卡洛稳健性评估（5%噪声）')\n",
    "plt.xlabel('断纱率')\n",
    "plt.ylabel('概率密度')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 生成验证报告\n",
    "print(\"\\n=== 验证报告 ===\")\n",
    "print(f\"\\n最优参数组合: {optimal_params}\")\n",
    "print(f\"最优断纱率: {optimal_rate:.4f}\")\n",
    "\n",
    "print(\"\\n固定组合验证结果:\")\n",
    "for i, result in enumerate(fixed_results):\n",
    "    print(f\"组合{i+1}: {result['params']}\")\n",
    "    print(f\"断纱率: {result['breakage_rate']:.4f} (与最优解差距: {result['breakage_rate']-optimal_rate:.4f})\")\n",
    "\n",
    "print(\"\\n随机森林敏感性分析结果:\")\n",
    "for name, imp in zip(feature_importance_df['Feature'], feature_importance_df['Importance']):\n",
    "    print(f\"{name}: {imp:.4f}\")\n",
    "\n",
    "print(\"\\nTOP3组合稳健性评估:\")\n",
    "for i in range(3):\n",
    "    mean_rate = np.mean(robustness_results[i])\n",
    "    std_rate = np.std(robustness_results[i])\n",
    "    print(f\"TOP{i+1}组合 - 平均断纱率: {mean_rate:.4f}, 标准差: {std_rate:.4f}\")"
   ],
   "id": "a15367dc7c047ed6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "124aede8",
   "metadata": {
    "papermill": {
     "duration": 0.079777,
     "end_time": "2023-09-10T22:58:02.709433",
     "exception": false,
     "start_time": "2023-09-10T22:58:02.629656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>20 |</span></b> <b>Final Model Predictions and Comparison with True Prices</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "9df7fd13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:58:02.872028Z",
     "iopub.status.busy": "2023-09-10T22:58:02.870765Z",
     "iopub.status.idle": "2023-09-10T22:58:02.884679Z",
     "shell.execute_reply": "2023-09-10T22:58:02.883611Z"
    },
    "papermill": {
     "duration": 0.097736,
     "end_time": "2023-09-10T22:58:02.887043",
     "exception": false,
     "start_time": "2023-09-10T22:58:02.789307",
     "status": "completed"
    },
    "tags": []
   },
   "source": "best_global_model\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(\"Best global model hyperparameters:\", best_global_model.get_params())",
   "id": "b727441520559be5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from IPython.display import display  # 添加display函数\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "# 1. 数据预处理函数\n",
    "def preprocess_data(df, dw_number=1):\n",
    "    \"\"\"处理非数值型列并准备建模数据\"\"\"\n",
    "    # 创建断纱标志列\n",
    "    target_col = f'is_break_dw{dw_number}'\n",
    "    df[target_col] = df[f'D_dw{dw_number}'].apply(lambda x: 1 if x == 3 else 0)\n",
    "\n",
    "    # 删除时间戳等无关列（根据实际数据调整）\n",
    "    non_numeric_cols = df.select_dtypes(include=['object', 'datetime']).columns\n",
    "    df_clean = df.drop(columns=non_numeric_cols)\n",
    "\n",
    "    # 检查并处理剩余的非数值数据\n",
    "    non_numeric = df_clean.select_dtypes(exclude=['int', 'float', 'bool']).columns\n",
    "    if len(non_numeric) > 0:\n",
    "        print(f\"警告: 仍有非数值列 {list(non_numeric)}，将尝试自动转换\")\n",
    "        df_clean = pd.get_dummies(df_clean, columns=non_numeric)\n",
    "\n",
    "    # 分离特征和目标变量\n",
    "    X = df_clean.drop(columns=[target_col])\n",
    "    y = df_clean[target_col]\n",
    "\n",
    "    return X, y, target_col\n",
    "\n",
    "# 2. 训练最佳模型\n",
    "def train_best_model(X, y):\n",
    "    \"\"\"训练并返回最佳模型\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 评估模型\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"\\n模型评估报告:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return model\n",
    "\n",
    "# 3. 分析锭速偏离阈值\n",
    "def analyze_speed_threshold(model, data, speed_col, reference_col, target_col):\n",
    "    \"\"\"分析锭速偏离的安全阈值\"\"\"\n",
    "    results = []\n",
    "    speed_changes = np.arange(-30, 30.5, 0.5)  # -30%到+30%\n",
    "\n",
    "    # 确保只使用数值列\n",
    "    numeric_cols = data.select_dtypes(include=['int', 'float', 'bool']).columns\n",
    "    data = data[numeric_cols]\n",
    "\n",
    "    for change in tqdm(speed_changes, desc=\"分析速度偏离\"):\n",
    "        modified_data = data.copy()\n",
    "        modified_data[speed_col] = modified_data[reference_col] * (1 + change/100)\n",
    "\n",
    "        # 预测断纱概率\n",
    "        proba = model.predict_proba(modified_data.drop(columns=[target_col]))[:, 1]\n",
    "        breakage_rate = np.mean(proba)\n",
    "\n",
    "        results.append({\n",
    "            '速度变化(%)': change,\n",
    "            '断纱率(%)': breakage_rate * 100\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # 寻找断纱率突增的阈值点（使用导数变化）\n",
    "    results_df['斜率'] = results_df['断纱率(%)'].diff().abs()\n",
    "    threshold = results_df[results_df['斜率'] > results_df['斜率'].quantile(0.9)]\n",
    "\n",
    "    # 可视化\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.lineplot(data=results_df, x='速度变化(%)', y='断纱率(%)')\n",
    "    for t in threshold['速度变化(%)']:\n",
    "        plt.axvline(x=t, color='red', linestyle='--', alpha=0.3)\n",
    "    plt.title('锭速偏离对断纱率的影响')\n",
    "    plt.show()\n",
    "\n",
    "    return results_df, threshold\n",
    "\n",
    "# 4. 分析满卷率安全阈值\n",
    "def analyze_fullness_threshold(model, data, fullness_col, target_col):\n",
    "    \"\"\"分析满卷率的安全阈值\"\"\"\n",
    "    results = []\n",
    "    fullness_levels = np.arange(0, 101, 1)  # 0%到100%\n",
    "\n",
    "    # 确保只使用数值列\n",
    "    numeric_cols = data.select_dtypes(include=['int', 'float', 'bool']).columns\n",
    "    data = data[numeric_cols]\n",
    "\n",
    "    for level in tqdm(fullness_levels, desc=\"分析满卷率\"):\n",
    "        modified_data = data.copy()\n",
    "        modified_data[fullness_col] = level\n",
    "\n",
    "        # 预测断纱概率\n",
    "        proba = model.predict_proba(modified_data.drop(columns=[target_col]))[:, 1]\n",
    "        breakage_rate = np.mean(proba)\n",
    "\n",
    "        results.append({\n",
    "            '满卷率(%)': level,\n",
    "            '断纱率(%)': breakage_rate * 100\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # 寻找断纱率突增的阈值点\n",
    "    results_df['斜率'] = results_df['断纱率(%)'].diff().abs()\n",
    "    threshold = results_df[results_df['斜率'] > results_df['斜率'].quantile(0.9)]\n",
    "\n",
    "    # 可视化\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.lineplot(data=results_df, x='满卷率(%)', y='断纱率(%)')\n",
    "    for t in threshold['满卷率(%)']:\n",
    "        plt.axvline(x=t, color='red', linestyle='--', alpha=0.3)\n",
    "    plt.title('满卷率对断纱率的影响')\n",
    "    plt.show()\n",
    "\n",
    "    return results_df, threshold\n",
    "\n",
    "# 5. 分析不同满卷率下的最佳锭速范围\n",
    "def analyze_optimal_speed_range(model, data, speed_col, reference_col, fullness_col, target_col):\n",
    "    \"\"\"分析不同满卷率下的最佳锭速范围\"\"\"\n",
    "    results = []\n",
    "    speed_changes = np.arange(-10, 10.5, 0.5)  # -10%到+10%\n",
    "    fullness_levels = np.arange(10, 101, 10)   # 10%, 20%, ..., 100%\n",
    "\n",
    "    # 确保只使用数值列\n",
    "    numeric_cols = data.select_dtypes(include=['int', 'float', 'bool']).columns\n",
    "    data = data[numeric_cols]\n",
    "\n",
    "    for level in tqdm(fullness_levels, desc=\"分析满卷率\"):\n",
    "        for change in speed_changes:\n",
    "            modified_data = data.copy()\n",
    "            modified_data[speed_col] = modified_data[reference_col] * (1 + change/100)\n",
    "            modified_data[fullness_col] = level\n",
    "\n",
    "            # 预测断纱概率\n",
    "            proba = model.predict_proba(modified_data.drop(columns=[target_col]))[:, 1]\n",
    "            breakage_rate = np.mean(proba)\n",
    "\n",
    "            results.append({\n",
    "                '满卷率(%)': level,\n",
    "                '速度变化(%)': change,\n",
    "                '断纱率(%)': breakage_rate * 100\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # 找出每个满卷率下断纱率最低的速度范围\n",
    "    optimal_ranges = results_df.groupby('满卷率(%)').apply(\n",
    "        lambda x: x.nsmallest(3, '断纱率(%)')\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # 可视化\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=results_df, x='速度变化(%)', y='断纱率(%)', hue='满卷率(%)')\n",
    "    plt.title('不同满卷率下速度偏离对断纱率的影响')\n",
    "    plt.axvline(x=0, color='red', linestyle='--', label='标准速度')\n",
    "    plt.legend(title='满卷率(%)')\n",
    "    plt.show()\n",
    "\n",
    "    return results_df, optimal_ranges\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载数据（替换为您的实际数据）\n",
    "    df = pd.read_csv('D:/code/junma/600000/0424/2.csv')\n",
    "\n",
    "    # 配置参数（根据实际列名修改）\n",
    "    dw_number = 1  # 锭号\n",
    "    speed_col = 'V_dw1'      # 实际速度列\n",
    "    reference_col = 'R_dw22' # 参考速度列\n",
    "    fullness_col = 'P_dw1'   # 满卷率列\n",
    "\n",
    "    # 1. 数据预处理\n",
    "    print(\"正在进行数据预处理...\")\n",
    "    X, y, target_col = preprocess_data(df, dw_number=dw_number)\n",
    "\n",
    "    # 2. 训练模型\n",
    "    print(\"\\n训练模型中...\")\n",
    "    best_model = train_best_model(X, y)\n",
    "\n",
    "    # 3. 分析锭速偏离阈值\n",
    "    print(\"\\n分析锭速偏离阈值...\")\n",
    "    speed_results, speed_threshold = analyze_speed_threshold(\n",
    "        best_model, X.join(y), speed_col, reference_col, target_col\n",
    "    )\n",
    "    print(\"\\n锭速偏离安全阈值:\")\n",
    "    display(speed_threshold)\n",
    "\n",
    "    # 4. 分析满卷率安全阈值\n",
    "    print(\"\\n分析满卷率安全阈值...\")\n",
    "    fullness_results, fullness_threshold = analyze_fullness_threshold(\n",
    "        best_model, X.join(y), fullness_col, target_col\n",
    "    )\n",
    "    print(\"\\n满卷率安全阈值:\")\n",
    "    display(fullness_threshold)\n",
    "\n",
    "    # 5. 分析不同满卷率下的最佳锭速范围\n",
    "    print(\"\\n分析不同满卷率下的最佳锭速范围...\")\n",
    "    range_results, optimal_ranges = analyze_optimal_speed_range(\n",
    "        best_model, X.join(y), speed_col, reference_col, fullness_col, target_col\n",
    "    )\n",
    "    print(\"\\n各满卷率下的最佳速度范围:\")\n",
    "    display(optimal_ranges)\n",
    "\n",
    "    # 保存结果\n",
    "    speed_results.to_csv('speed_deviation_results.csv', index=False)\n",
    "    fullness_results.to_csv('fullness_threshold_results.csv', index=False)\n",
    "    range_results.to_csv('optimal_speed_ranges.csv', index=False)\n",
    "    optimal_ranges.to_csv('recommended_speed_ranges.csv', index=False)"
   ],
   "id": "93f4565e5c5b39f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 设置中文字体和负号显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "# 1. 数据预处理\n",
    "def preprocess_data(df, dw_number=1):\n",
    "    \"\"\"数据预处理函数\"\"\"\n",
    "    # 创建断纱标志列\n",
    "    target_col = f'is_break_dw{dw_number}'\n",
    "    df[target_col] = df[f'D_dw{dw_number}'].apply(lambda x: 1 if x == 3 else 0)\n",
    "\n",
    "    # 计算速度偏移率（当前锭子速度与所有锭子速度中位数的偏差百分比）\n",
    "    speed_col = f'V_dw{dw_number}'\n",
    "    reference_speed = df['R_dw22'].median()  # 使用 R_dw22 的中位数作为参考速度\n",
    "    df['速度偏移率(%)'] = ((df[speed_col] - reference_speed) / reference_speed) * 100\n",
    "\n",
    "    # 将满卷率和速度偏移率分箱（binning），并保留区间标签\n",
    "    df['满卷率分箱'], full_bins = pd.cut(df[f'P_dw{dw_number}'], bins=10, retbins=True)\n",
    "    df['速度偏移率分箱'], speed_bins = pd.cut(df['速度偏移率(%)'], bins=10, retbins=True)\n",
    "\n",
    "    # 处理设备名称和对应面\n",
    "    df['设备名称'] = df['name'].apply(lambda x: x.split('-')[0])  # 提取设备名称（如 NX78）\n",
    "    df['设备面'] = df['subsystem'].apply(lambda x: x.split('-')[1])  # 提取设备面（如 R103）\n",
    "\n",
    "    return df, full_bins, speed_bins\n",
    "\n",
    "# 2. 单因素分析\n",
    "def single_factor_analysis(df, factor_col, target_col, bins=None, factor_name=None):\n",
    "    \"\"\"单因素分析函数\"\"\"\n",
    "    # 计算每个区间的断纱率\n",
    "    factor_break_rate = df.groupby(factor_col)[target_col].agg(['mean', 'count']).reset_index()\n",
    "    factor_break_rate.rename(columns={'mean': '断纱率', 'count': '样本数'}, inplace=True)\n",
    "\n",
    "    # 获取区间范围\n",
    "    factor_break_rate['区间范围'] = factor_break_rate[factor_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\")\n",
    "\n",
    "    # 绘制柱状图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(data=factor_break_rate, x='区间范围', y='断纱率', palette='viridis')\n",
    "\n",
    "    # 添加样本数标注\n",
    "    for i, row in factor_break_rate.iterrows():\n",
    "        ax.text(i, row['断纱率']+0.01, f\"n={row['样本数']}\", ha='center', fontsize=9)\n",
    "\n",
    "    # 标注断纱率最高的区间\n",
    "    max_idx = factor_break_rate['断纱率'].idxmax()\n",
    "    max_row = factor_break_rate.loc[max_idx]\n",
    "    plt.axvline(x=max_idx, color='red', linestyle='--', alpha=0.5)\n",
    "    plt.text(max_idx, max_row['断纱率']+0.05,\n",
    "             f\"最高断纱率: {max_row['断纱率']:.2%}\\n区间: {max_row['区间范围']}\",\n",
    "             ha='center', color='red')\n",
    "\n",
    "    plt.title(f'{factor_name or factor_col} 对断纱率的影响')\n",
    "    plt.xlabel(factor_name or factor_col)\n",
    "    plt.ylabel('断纱率')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return factor_break_rate\n",
    "\n",
    "# 3. 双因素交互分析\n",
    "def interaction_analysis(df, factor1_col, factor2_col, target_col, factor1_name=None, factor2_name=None):\n",
    "    \"\"\"双因素交互分析函数\"\"\"\n",
    "    # 计算每个组合区间的断纱率\n",
    "    interaction_break_rate = df.groupby([factor1_col, factor2_col])[target_col].agg(['mean', 'count']).reset_index()\n",
    "    interaction_break_rate.rename(columns={'mean': '断纱率', 'count': '样本数'}, inplace=True)\n",
    "\n",
    "    # 创建区间范围标签\n",
    "    interaction_break_rate['factor1_range'] = interaction_break_rate[factor1_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\")\n",
    "    interaction_break_rate['factor2_range'] = interaction_break_rate[factor2_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\")\n",
    "\n",
    "    # 创建热力图\n",
    "    pivot_table = interaction_break_rate.pivot(index='factor1_range', columns='factor2_range', values='断纱率')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.heatmap(pivot_table, annot=True, fmt=\".2%\", cmap=\"YlGnBu\",\n",
    "                     annot_kws={\"size\": 9}, cbar_kws={'label': '断纱率'})\n",
    "\n",
    "    # 标注断纱率最高的组合区间\n",
    "    max_idx = interaction_break_rate['断纱率'].idxmax()\n",
    "    max_row = interaction_break_rate.loc[max_idx]\n",
    "    plt.title(f'{factor1_name or factor1_col} × {factor2_name or factor2_col} 对断纱率的影响\\n'\n",
    "              f\"断纱率最高的组合: {max_row['factor1_range']} × {max_row['factor2_range']} = {max_row['断纱率']:.2%}\",\n",
    "              pad=20)\n",
    "\n",
    "    plt.xlabel(factor2_name or factor2_col)\n",
    "    plt.ylabel(factor1_name or factor1_col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 统计显著性检验（卡方检验）\n",
    "    # 创建交叉表时使用原始数据而不是平均值\n",
    "    contingency_table = pd.crosstab(\n",
    "        df[factor1_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\"),\n",
    "        df[factor2_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\"),\n",
    "        values=df[target_col],\n",
    "        aggfunc='sum'  # 改为求和而不是平均值\n",
    "    )\n",
    "\n",
    "    # 检查是否有零频数\n",
    "    if (contingency_table == 0).any().any():\n",
    "        print(\"警告: 交叉表中存在零频数，卡方检验可能不准确\")\n",
    "        # 可以尝试合并一些类别或添加小常数\n",
    "        contingency_table = contingency_table + 0.5  # 添加0.5的连续性校正\n",
    "\n",
    "    try:\n",
    "        chi2, p, _, _ = chi2_contingency(contingency_table.fillna(0))\n",
    "        print(f\"卡方检验结果: p值 = {p:.4f}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"无法执行卡方检验: {str(e)}\")\n",
    "        p = None\n",
    "\n",
    "    return interaction_break_rate, p\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载数据（替换为你的实际数据路径）\n",
    "    df = pd.read_csv('D:/code/junma/600000/0424/2.csv')\n",
    "\n",
    "    # 配置参数（根据实际列名修改）\n",
    "    dw_number = 1  # 锭号\n",
    "\n",
    "    # 1. 数据预处理\n",
    "    print(\"正在进行数据预处理...\")\n",
    "    df_processed, full_bins, speed_bins = preprocess_data(df, dw_number=dw_number)\n",
    "\n",
    "    # 打印分箱边界\n",
    "    print(\"\\n满卷率分箱边界:\", full_bins)\n",
    "    print(\"速度偏移率分箱边界:\", speed_bins)\n",
    "\n",
    "    # 2. 单因素分析\n",
    "    print(\"\\n单因素分析...\")\n",
    "    fullness_break_rate = single_factor_analysis(\n",
    "        df_processed,\n",
    "        '满卷率分箱',\n",
    "        f'is_break_dw{dw_number}',\n",
    "        factor_name='满卷率'\n",
    "    )\n",
    "\n",
    "    speed_deviation_break_rate = single_factor_analysis(\n",
    "        df_processed,\n",
    "        '速度偏移率分箱',\n",
    "        f'is_break_dw{dw_number}',\n",
    "        factor_name='速度偏移率(%)'\n",
    "    )\n",
    "\n",
    "    # 3. 双因素交互分析\n",
    "    print(\"\\n双因素交互分析...\")\n",
    "    interaction_break_rate, p_value = interaction_analysis(\n",
    "        df_processed,\n",
    "        '满卷率分箱',\n",
    "        '速度偏移率分箱',\n",
    "        f'is_break_dw{dw_number}',\n",
    "        factor1_name='满卷率',\n",
    "        factor2_name='速度偏移率(%)'\n",
    "    )\n",
    "\n",
    "    # 保存结果\n",
    "    fullness_break_rate.to_csv('fullness_break_rate.csv', index=False)\n",
    "    speed_deviation_break_rate.to_csv('speed_deviation_break_rate.csv', index=False)\n",
    "    interaction_break_rate.to_csv('interaction_break_rate.csv', index=False)"
   ],
   "id": "13981698942f9270",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set font and display settings\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # Use SimHei for Chinese characters\n",
    "plt.rcParams['axes.unicode_minus'] = False  # Correct display of negative signs\n",
    "\n",
    "# 1. Data Preprocessing\n",
    "def preprocess_data(df, dw_number=1):\n",
    "    \"\"\"Data preprocessing function\"\"\"\n",
    "    # Create yarn break flag column\n",
    "    target_col = f'is_break_dw{dw_number}'\n",
    "    df[target_col] = df[f'D_dw{dw_number}'].apply(lambda x: 1 if x == 3 else 0)\n",
    "\n",
    "    # Calculate speed deviation rate (percentage deviation from median speed)\n",
    "    speed_col = f'V_dw{dw_number}'\n",
    "    reference_speed = df['R_dw22'].median()  # Use median of R_dw22 as reference speed\n",
    "    df['Speed_Deviation(%)'] = ((df[speed_col] - reference_speed) / reference_speed) * 100\n",
    "\n",
    "    # Bin fullness rate and speed deviation rate\n",
    "    df['Fullness_Bin'], full_bins = pd.cut(df[f'P_dw{dw_number}'], bins=10, retbins=True)\n",
    "    df['Speed_Deviation_Bin'], speed_bins = pd.cut(df['Speed_Deviation(%)'], bins=10, retbins=True)\n",
    "\n",
    "    # Process equipment name and side\n",
    "    df['Equipment_Name'] = df['name'].apply(lambda x: x.split('-')[0])  # Extract equipment name (e.g., NX78)\n",
    "    df['Equipment_Side'] = df['subsystem'].apply(lambda x: x.split('-')[1])  # Extract equipment side (e.g., R103)\n",
    "\n",
    "    return df, full_bins, speed_bins\n",
    "\n",
    "# 2. Single Factor Analysis\n",
    "def single_factor_analysis(df, factor_col, target_col, bins=None, factor_name=None):\n",
    "    \"\"\"Single factor analysis function\"\"\"\n",
    "    # Calculate break rate for each bin\n",
    "    factor_break_rate = df.groupby(factor_col)[target_col].agg(['mean', 'count']).reset_index()\n",
    "    factor_break_rate.rename(columns={'mean': 'Break_Rate', 'count': 'Sample_Count'}, inplace=True)\n",
    "\n",
    "    # Get bin ranges\n",
    "    factor_break_rate['Bin_Range'] = factor_break_rate[factor_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\")\n",
    "\n",
    "    # Plot bar chart with enhanced formatting for the first plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(data=factor_break_rate, x='Bin_Range', y='Break_Rate', palette='viridis')\n",
    "\n",
    "    # Add sample count annotations\n",
    "    for i, row in factor_break_rate.iterrows():\n",
    "        ax.text(i, row['Break_Rate']+0.01, f\"n={row['Sample_Count']}\", ha='center', fontsize=9)\n",
    "\n",
    "    # Highlight bin with highest break rate\n",
    "    max_idx = factor_break_rate['Break_Rate'].idxmax()\n",
    "    max_row = factor_break_rate.loc[max_idx]\n",
    "    plt.axvline(x=max_idx, color='red', linestyle='--', alpha=0.5)\n",
    "    plt.text(max_idx, max_row['Break_Rate']+0.05,\n",
    "             f\"Highest Break Rate: {max_row['Break_Rate']:.2%}\\nRange: {max_row['Bin_Range']}\",\n",
    "             ha='center', color='red')\n",
    "\n",
    "    # Enhanced axis labels with bold font and larger size\n",
    "    plt.title(f'Effect of {factor_name or factor_col} on Yarn Break Rate', fontweight='bold', pad=20)\n",
    "    plt.xlabel(factor_name or factor_col, fontweight='bold', fontsize=12)\n",
    "    plt.ylabel('Break Rate', fontweight='bold', fontsize=12)\n",
    "\n",
    "    # Make tick labels more visible\n",
    "    plt.xticks(rotation=45, fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "\n",
    "    # Make axis spines bolder\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return factor_break_rate\n",
    "\n",
    "# 3. Interaction Analysis\n",
    "def interaction_analysis(df, factor1_col, factor2_col, target_col, factor1_name=None, factor2_name=None):\n",
    "    \"\"\"Two-factor interaction analysis function\"\"\"\n",
    "    # Calculate break rate for each combination\n",
    "    interaction_break_rate = df.groupby([factor1_col, factor2_col])[target_col].agg(['mean', 'count']).reset_index()\n",
    "    interaction_break_rate.rename(columns={'mean': 'Break_Rate', 'count': 'Sample_Count'}, inplace=True)\n",
    "\n",
    "    # Create bin range labels\n",
    "    interaction_break_rate['factor1_range'] = interaction_break_rate[factor1_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\")\n",
    "    interaction_break_rate['factor2_range'] = interaction_break_rate[factor2_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\")\n",
    "\n",
    "    # Create heatmap with enhanced formatting for the last plot\n",
    "    pivot_table = interaction_break_rate.pivot(index='factor1_range', columns='factor2_range', values='Break_Rate')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.heatmap(pivot_table, annot=True, fmt=\".2%\", cmap=\"YlGnBu\",\n",
    "                     annot_kws={\"size\": 9, \"weight\": 'bold'},\n",
    "                     cbar_kws={'label': 'Break Rate'})\n",
    "\n",
    "    # Highlight combination with highest break rate\n",
    "    max_idx = interaction_break_rate['Break_Rate'].idxmax()\n",
    "    max_row = interaction_break_rate.loc[max_idx]\n",
    "    plt.title(f'Interaction Effect of {factor1_name or factor1_col} × {factor2_name or factor2_col} on Yarn Break Rate\\n'\n",
    "              f\"Highest Break Rate: {max_row['factor1_range']} × {max_row['factor2_range']} = {max_row['Break_Rate']:.2%}\",\n",
    "              fontweight='bold', pad=20)\n",
    "\n",
    "    # Enhanced axis labels with bold font and larger size\n",
    "    plt.xlabel(factor2_name or factor2_col, fontweight='bold', fontsize=12)\n",
    "    plt.ylabel(factor1_name or factor1_col, fontweight='bold', fontsize=12)\n",
    "\n",
    "    # Make tick labels more visible\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=10, rotation=45)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=10, rotation=0)\n",
    "\n",
    "    # Make colorbar label bold\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.set_ylabel('Break Rate', fontweight='bold', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Statistical significance test (Chi-square test)\n",
    "    # Create contingency table using raw data\n",
    "    contingency_table = pd.crosstab(\n",
    "        df[factor1_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\"),\n",
    "        df[factor2_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\"),\n",
    "        values=df[target_col],\n",
    "        aggfunc='sum'  # Use sum instead of mean\n",
    "    )\n",
    "\n",
    "    # Check for zero frequencies\n",
    "    if (contingency_table == 0).any().any():\n",
    "        print(\"Warning: Zero frequencies detected in contingency table, chi-square test may be inaccurate\")\n",
    "        # Apply continuity correction\n",
    "        contingency_table = contingency_table + 0.5\n",
    "\n",
    "    try:\n",
    "        chi2, p, _, _ = chi2_contingency(contingency_table.fillna(0))\n",
    "        print(f\"Chi-square test result: p-value = {p:.4f}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Unable to perform chi-square test: {str(e)}\")\n",
    "        p = None\n",
    "\n",
    "    return interaction_break_rate, p\n",
    "\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data (replace with your actual data path)\n",
    "    df = pd.read_csv('D:/code/junma/600000/0424/2.csv')\n",
    "\n",
    "    # Configuration parameters (modify according to actual column names)\n",
    "    dw_number = 1  # Spindle number\n",
    "\n",
    "    # 1. Data Preprocessing\n",
    "    print(\"Performing data preprocessing...\")\n",
    "    df_processed, full_bins, speed_bins = preprocess_data(df, dw_number=dw_number)\n",
    "\n",
    "    # Print bin boundaries\n",
    "    print(\"\\nFullness rate bin boundaries:\", full_bins)\n",
    "    print(\"Speed deviation rate bin boundaries:\", speed_bins)\n",
    "\n",
    "    # 2. Single Factor Analysis\n",
    "    print(\"\\nSingle factor analysis...\")\n",
    "    fullness_break_rate = single_factor_analysis(\n",
    "        df_processed,\n",
    "        'Fullness_Bin',\n",
    "        f'is_break_dw{dw_number}',\n",
    "        factor_name='Fullness Rate'\n",
    "    )\n",
    "\n",
    "    speed_deviation_break_rate = single_factor_analysis(\n",
    "        df_processed,\n",
    "        'Speed_Deviation_Bin',\n",
    "        f'is_break_dw{dw_number}',\n",
    "        factor_name='Speed Deviation(%)'\n",
    "    )\n",
    "\n",
    "    # 3. Interaction Analysis\n",
    "    print(\"\\nTwo-factor interaction analysis...\")\n",
    "    interaction_break_rate, p_value = interaction_analysis(\n",
    "        df_processed,\n",
    "        'Fullness_Bin',\n",
    "        'Speed_Deviation_Bin',\n",
    "        f'is_break_dw{dw_number}',\n",
    "        factor1_name='Fullness Rate',\n",
    "        factor2_name='Speed Deviation(%)'\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    fullness_break_rate.to_csv('fullness_break_rate.csv', index=False)\n",
    "    speed_deviation_break_rate.to_csv('speed_deviation_break_rate.csv', index=False)\n",
    "    interaction_break_rate.to_csv('interaction_break_rate.csv', index=False)"
   ],
   "id": "4a8fd5500388e403",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# 设置专业学术风格参数\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.weight'] = 'bold'  # 使用bold保持专业感\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['axes.linewidth'] = 2.5  # 适中轴线宽度\n",
    "plt.rcParams['font.size'] = 18  # 适度字体大小\n",
    "plt.rcParams['axes.titlesize'] = 16  # 标题大小\n",
    "plt.rcParams['axes.labelsize'] = 20  # 轴标签大小\n",
    "plt.rcParams['xtick.labelsize'] = 18  # 刻度标签\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.fontsize'] = 13\n",
    "plt.rcParams['grid.linewidth'] = 1.2  # 细网格线\n",
    "plt.rcParams['lines.linewidth'] = 3.5  # 数据线宽度\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['font.family'] = 'Arial'  # 使用更专业的英文字体\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "def interaction_analysis(df, factor1_col, factor2_col, target_col, factor1_name=None, factor2_name=None):\n",
    "    \"\"\"Two-factor interaction analysis function with professional academic style\"\"\"\n",
    "    # Calculate break rate for each combination\n",
    "    interaction_break_rate = df.groupby([factor1_col, factor2_col])[target_col].agg(['mean', 'count']).reset_index()\n",
    "    interaction_break_rate.rename(columns={'mean': 'Break_Rate', 'count': 'Sample_Count'}, inplace=True)\n",
    "\n",
    "    # Create bin range labels\n",
    "    interaction_break_rate['factor1_range'] = interaction_break_rate[factor1_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\")\n",
    "    interaction_break_rate['factor2_range'] = interaction_break_rate[factor2_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\")\n",
    "\n",
    "    # Create heatmap with enhanced formatting\n",
    "    pivot_table = interaction_break_rate.pivot(index='factor1_range', columns='factor2_range', values='Break_Rate')\n",
    "\n",
    "    # Create figure with professional academic style\n",
    "    fig, ax = plt.subplots(figsize=(10, 7), facecolor='white')\n",
    "\n",
    "    # Draw heatmap with grid lines\n",
    "    sns.heatmap(pivot_table, annot=True, fmt=\".2%\", cmap=\"YlGnBu\",\n",
    "                annot_kws={\"size\": 13, \"weight\": 'bold'},\n",
    "                cbar_kws={'label': 'Break Rate', 'shrink': 0.8}, ax=ax,\n",
    "                linewidths=0.5, linecolor='black')  # 添加格子线\n",
    "\n",
    "    # Highlight combination with highest break rate\n",
    "    max_idx = interaction_break_rate['Break_Rate'].idxmax()\n",
    "    max_row = interaction_break_rate.loc[max_idx]\n",
    "    # plt.title(f'Interaction Effect of {factor1_name or factor1_col} × {factor2_name or factor2_col} on Yarn Break Rate\\n',\n",
    "    #           # f\"Highest Break Rate: {max_row['factor1_range']} × {max_row['factor2_range']} = {max_row['Break_Rate']:.2%}\",\n",
    "    #           fontweight='bold', pad=15, fontsize=14)\n",
    "\n",
    "    # Enhanced axis labels with bold font and larger size\n",
    "    plt.xlabel(factor2_name or factor2_col, fontweight='bold', fontsize=19, labelpad=10)\n",
    "    plt.ylabel(factor1_name or factor1_col, fontweight='bold', fontsize=19, labelpad=10)\n",
    "\n",
    "    # Make tick labels more visible\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=18, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=18, rotation=0)\n",
    "\n",
    "    # Make colorbar label bold\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.set_ylabel('Break Rate', fontweight='bold', fontsize=15)\n",
    "\n",
    "    # 设置边框线\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(5)  # 加粗边框线\n",
    "        spine.set_color('black')  # 设置边框颜色为黑色\n",
    "\n",
    "    # 调整刻度标签颜色和粗细\n",
    "    ax.tick_params(axis='x', colors='black', width=2, length=6, labelsize=18)\n",
    "    ax.tick_params(axis='y', colors='black', width=2, length=6, labelsize=18)\n",
    "\n",
    "    # 设置刻度标签加粗\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_fontweight('bold')\n",
    "    for tick in ax.get_yticklabels():\n",
    "        tick.set_fontweight('bold')\n",
    "\n",
    "    # 优化刻度设置\n",
    "    ax.tick_params(axis='both', which='major',\n",
    "                   width=1.5, length=6,\n",
    "                   labelsize=12, colors='black', pad=5)\n",
    "\n",
    "    # 添加XY轴格子线\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.8, color='gray', alpha=0.6)\n",
    "\n",
    "    # 确保白色背景\n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    # 最终调整元素间距\n",
    "    plt.subplots_adjust(left=0.12, right=0.95, top=0.88, bottom=0.15)\n",
    "\n",
    "    # 保存高质量图片\n",
    "    plt.savefig('interaction_heatmap.png', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Statistical significance test (Chi-square test)\n",
    "    # Create contingency table using raw data\n",
    "    contingency_table = pd.crosstab(\n",
    "        df[factor1_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\"),\n",
    "        df[factor2_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\"),\n",
    "        values=df[target_col],\n",
    "        aggfunc='sum'  # Use sum instead of mean\n",
    "    )\n",
    "\n",
    "    # Check for zero frequencies\n",
    "    if (contingency_table == 0).any().any():\n",
    "        print(\"Warning: Zero frequencies detected in contingency table, chi-square test may be inaccurate\")\n",
    "        # Apply continuity correction\n",
    "        contingency_table = contingency_table + 0.5\n",
    "\n",
    "    try:\n",
    "        chi2, p, _, _ = chi2_contingency(contingency_table.fillna(0))\n",
    "        print(f\"Chi-square test result: p-value = {p:.4f}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Unable to perform chi-square test: {str(e)}\")\n",
    "        p = None\n",
    "\n",
    "    return interaction_break_rate, p\n",
    "\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data (replace with your actual data path)\n",
    "    df = pd.read_csv('D:/code/junma/600000/0424/2.csv')\n",
    "\n",
    "    # Configuration parameters (modify according to actual column names)\n",
    "    dw_number = 1  # Spindle number\n",
    "\n",
    "    # 1. Data Preprocessing\n",
    "    print(\"Performing data preprocessing...\")\n",
    "    df_processed, full_bins, speed_bins = preprocess_data(df, dw_number=dw_number)\n",
    "\n",
    "    # 3. Interaction Analysis\n",
    "    print(\"\\nTwo-factor interaction analysis...\")\n",
    "    interaction_break_rate, p_value = interaction_analysis(\n",
    "        df_processed,\n",
    "        'Fullness_Bin',\n",
    "        'Speed_Deviation_Bin',\n",
    "        f'is_break_dw{dw_number}',\n",
    "        factor1_name='Fullness Rate',\n",
    "        factor2_name='Speed Deviation(%)'\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    interaction_break_rate.to_csv('interaction_break_rate.csv', index=False)"
   ],
   "id": "b53bd638c6bbac46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# 设置专业学术风格参数\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.dpi'] = 1200\n",
    "plt.rcParams['savefig.dpi'] = 1200\n",
    "plt.rcParams['font.weight'] = 'bold'  # 使用bold保持专业感\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['axes.linewidth'] = 2.5  # 适中轴线宽度\n",
    "plt.rcParams['font.size'] = 18  # 适度字体大小\n",
    "plt.rcParams['axes.titlesize'] = 16  # 标题大小\n",
    "plt.rcParams['axes.labelsize'] = 20  # 轴标签大小\n",
    "plt.rcParams['xtick.labelsize'] = 18  # 刻度标签\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.fontsize'] = 13\n",
    "plt.rcParams['grid.linewidth'] = 1.2  # 细网格线\n",
    "plt.rcParams['lines.linewidth'] = 3.5  # 数据线宽度\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['font.family'] = 'Arial'  # 使用更专业的英文字体\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "def interaction_analysis(df, factor1_col, factor2_col, target_col, factor1_name=None, factor2_name=None):\n",
    "    \"\"\"Two-factor interaction analysis function with professional academic style\"\"\"\n",
    "    # Calculate break rate for each combination\n",
    "    interaction_break_rate = df.groupby([factor1_col, factor2_col])[target_col].agg(['mean', 'count']).reset_index()\n",
    "    interaction_break_rate.rename(columns={'mean': 'Break_Rate', 'count': 'Sample_Count'}, inplace=True)\n",
    "\n",
    "    # Create bin range labels\n",
    "    interaction_break_rate['factor1_range'] = interaction_break_rate[factor1_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\")\n",
    "    interaction_break_rate['factor2_range'] = interaction_break_rate[factor2_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\")\n",
    "\n",
    "    # Create heatmap with enhanced formatting\n",
    "    pivot_table = interaction_break_rate.pivot(index='factor1_range', columns='factor2_range', values='Break_Rate')\n",
    "\n",
    "    # Create figure with professional academic style\n",
    "    fig, ax = plt.subplots(figsize=(10, 7), facecolor='white')\n",
    "\n",
    "    # Draw heatmap with grid lines\n",
    "    sns.heatmap(pivot_table, annot=True, fmt=\".2%\", cmap=\"YlGnBu\",\n",
    "                annot_kws={\"size\": 13, \"weight\": 'bold'},\n",
    "                cbar_kws={'label': 'Break Rate', 'shrink': 0.8}, ax=ax,\n",
    "                linewidths=0.5, linecolor='black')  # 添加格子线\n",
    "\n",
    "    # Highlight combination with highest break rate\n",
    "    max_idx = interaction_break_rate['Break_Rate'].idxmax()\n",
    "    max_row = interaction_break_rate.loc[max_idx]\n",
    "    # plt.title(f'Interaction Effect of {factor1_name or factor1_col} × {factor2_name or factor2_col} on Yarn Break Rate\\n',\n",
    "    #           # f\"Highest Break Rate: {max_row['factor1_range']} × {max_row['factor2_range']} = {max_row['Break_Rate']:.2%}\",\n",
    "    #           fontweight='bold', pad=15, fontsize=14)\n",
    "\n",
    "    # Enhanced axis labels with bold font and larger size\n",
    "    plt.xlabel(factor2_name or factor2_col, fontweight='bold', fontsize=19, labelpad=10)\n",
    "    plt.ylabel(factor1_name or factor1_col, fontweight='bold', fontsize=19, labelpad=10)\n",
    "\n",
    "    # Make tick labels more visible\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=18, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=18, rotation=0)\n",
    "\n",
    "    # Make colorbar label bold\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.set_ylabel('Break Rate', fontweight='bold', fontsize=15)\n",
    "\n",
    "    # 设置边框线\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(2)  # 加粗边框线\n",
    "        spine.set_color('black')  # 设置边框颜色为黑色\n",
    "\n",
    "    # 确保所有边框都显示（包括顶部和右侧）\n",
    "    ax.spines['top'].set_visible(True)\n",
    "    ax.spines['right'].set_visible(True)\n",
    "    ax.spines['bottom'].set_visible(True)\n",
    "    ax.spines['left'].set_visible(True)\n",
    "\n",
    "    # 调整刻度标签颜色和粗细\n",
    "    ax.tick_params(axis='x', colors='black', width=2, length=6, labelsize=18)\n",
    "    ax.tick_params(axis='y', colors='black', width=2, length=6, labelsize=18)\n",
    "\n",
    "    # 设置刻度标签加粗\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_fontweight('bold')\n",
    "    for tick in ax.get_yticklabels():\n",
    "        tick.set_fontweight('bold')\n",
    "\n",
    "    # 优化刻度设置\n",
    "    ax.tick_params(axis='both', which='major',\n",
    "                   width=1.5, length=6,\n",
    "                   labelsize=12, colors='black', pad=5)\n",
    "\n",
    "    # 添加XY轴格子线\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.8, color='gray', alpha=0.6)\n",
    "\n",
    "    # 确保白色背景\n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    # 最终调整元素间距\n",
    "    plt.subplots_adjust(left=0.12, right=0.95, top=0.88, bottom=0.15)\n",
    "\n",
    "    # 保存高质量图片\n",
    "    plt.savefig('interaction_heatmap.png', bbox_inches='tight', dpi=1200)\n",
    "    plt.show()\n",
    "\n",
    "    # Statistical significance test (Chi-square test)\n",
    "    # Create contingency table using raw data\n",
    "    contingency_table = pd.crosstab(\n",
    "        df[factor1_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\"),\n",
    "        df[factor2_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\"),\n",
    "        values=df[target_col],\n",
    "        aggfunc='sum'  # Use sum instead of mean\n",
    "    )\n",
    "\n",
    "    # Check for zero frequencies\n",
    "    if (contingency_table == 0).any().any():\n",
    "        print(\"Warning: Zero frequencies detected in contingency table, chi-square test may be inaccurate\")\n",
    "        # Apply continuity correction\n",
    "        contingency_table = contingency_table + 0.5\n",
    "\n",
    "    try:\n",
    "        chi2, p, _, _ = chi2_contingency(contingency_table.fillna(0))\n",
    "        print(f\"Chi-square test result: p-value = {p:.4f}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Unable to perform chi-square test: {str(e)}\")\n",
    "        p = None\n",
    "\n",
    "    return interaction_break_rate, p\n",
    "\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data (replace with your actual data path)\n",
    "    df = pd.read_csv('D:/code/junma/600000/0424/2.csv')\n",
    "\n",
    "    # Configuration parameters (modify according to actual column names)\n",
    "    dw_number = 1  # Spindle number\n",
    "\n",
    "    # 1. Data Preprocessing\n",
    "    print(\"Performing data preprocessing...\")\n",
    "    df_processed, full_bins, speed_bins = preprocess_data(df, dw_number=dw_number)\n",
    "\n",
    "    # 3. Interaction Analysis\n",
    "    print(\"\\nTwo-factor interaction analysis...\")\n",
    "    interaction_break_rate, p_value = interaction_analysis(\n",
    "        df_processed,\n",
    "        'Fullness_Bin',\n",
    "        'Speed_Deviation_Bin',\n",
    "        f'is_break_dw{dw_number}',\n",
    "        factor1_name='Fullness Rate',\n",
    "        factor2_name='Speed Deviation(%)'\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    interaction_break_rate.to_csv('interaction_break_rate.csv', index=False)"
   ],
   "id": "1e2690dfaa9f5dc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import ticker\n",
    "\n",
    "# 设置全局样式参数\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 1200,\n",
    "    'savefig.dpi': 1200,\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.edgecolor': 'black',\n",
    "    'axes.linewidth': 4,\n",
    "    'xtick.color': 'black',\n",
    "    'ytick.color': 'black',\n",
    "    'xtick.major.width': 4,\n",
    "    'ytick.major.width': 4,\n",
    "    'xtick.major.size': 16,\n",
    "    'ytick.major.size': 16,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'savefig.facecolor': 'white',\n",
    "    'axes.labelsize': 30,\n",
    "    'axes.titlesize': 28,\n",
    "    'xtick.labelsize': 22,\n",
    "    'ytick.labelsize': 22,\n",
    "    'legend.fontsize': 20,\n",
    "    'legend.title_fontsize': 22,\n",
    "    'font.family': 'Arial',\n",
    "    'axes.unicode_minus': False\n",
    "})\n",
    "\n",
    "# 1. 数据预处理函数\n",
    "def preprocess_data(df, dw_number=1):\n",
    "    \"\"\"数据预处理函数\"\"\"\n",
    "    target_col = f'is_break_dw{dw_number}'\n",
    "    df[target_col] = df[f'D_dw{dw_number}'].apply(lambda x: 1 if x == 3 else 0)\n",
    "\n",
    "    speed_col = f'V_dw{dw_number}'\n",
    "    reference_speed = df['R_dw22'].median()\n",
    "    df['Speed_Deviation(%)'] = ((df[speed_col] - reference_speed) / reference_speed) * 100\n",
    "\n",
    "    df['Fullness_Bin'], full_bins = pd.cut(df[f'P_dw{dw_number}'], bins=10, retbins=True)\n",
    "\n",
    "    return df, full_bins\n",
    "\n",
    "# 2. 单因素分析函数（使用更明亮的配色）\n",
    "def single_factor_analysis(df, factor_col, target_col, bins=None, factor_name=None):\n",
    "    \"\"\"专业学术风格的单因素分析可视化\"\"\"\n",
    "    factor_break_rate = df.groupby(factor_col)[target_col].agg(['mean', 'count']).reset_index()\n",
    "    factor_break_rate.rename(columns={'mean': 'Break_Rate', 'count': 'Sample_Count'}, inplace=True)\n",
    "\n",
    "    factor_break_rate['Bin_Range'] = factor_break_rate[factor_col].apply(\n",
    "        lambda x: f\"{x.left:.2f}-{x.right:.2f}\" if pd.notna(x.left) else \"NA\"\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    sns.set_style(\"whitegrid\", {\n",
    "        'axes.edgecolor': '0.2',\n",
    "        'grid.color': '0.85',\n",
    "        'grid.linestyle': ':',\n",
    "        'axes.linewidth': 2.5\n",
    "    })\n",
    "    sns.set_context(\"paper\", font_scale=1.0, rc={\n",
    "        \"font.size\": 16,\n",
    "        \"axes.titlesize\": 18,\n",
    "        \"axes.labelsize\": 24,\n",
    "        \"xtick.labelsize\": 18,\n",
    "        \"ytick.labelsize\": 18,\n",
    "        \"legend.fontsize\": 18,\n",
    "        \"font.family\": \"Arial\",\n",
    "        \"mathtext.default\": \"regular\",\n",
    "        \"mathtext.fontset\": \"custom\",\n",
    "        \"mathtext.it\": \"Arial:italic\",\n",
    "        \"mathtext.rm\": \"Arial\",\n",
    "        \"mathtext.sf\": \"Arial\",\n",
    "        \"mathtext.tt\": \"Arial\",\n",
    "    })\n",
    "\n",
    "    # 使用更明亮的调色板（例如 \"viridis\" 或 \"plasma\"）\n",
    "    custom_palette = sns.color_palette(\"viridis\", n_colors=len(factor_break_rate))\n",
    "\n",
    "    ax = sns.barplot(data=factor_break_rate, x='Bin_Range', y='Break_Rate',\n",
    "                     palette=custom_palette, saturation=0.9, width=0.85,\n",
    "                     edgecolor='black', linewidth=2.5)\n",
    "\n",
    "    max_break_rate = factor_break_rate['Break_Rate'].max()\n",
    "    y_max = max_break_rate * 1.15 if max_break_rate > 0 else 0.15\n",
    "    plt.ylim(0, y_max)\n",
    "\n",
    "    for i, row in factor_break_rate.iterrows():\n",
    "        text_y_pos = max(row['Break_Rate'] * 0.25, y_max * 0.05)\n",
    "        # ax.text(i, text_y_pos, f\"n={row['Sample_Count']}\",\n",
    "        #         ha='center', fontsize=15, fontweight='bold', color='white')\n",
    "\n",
    "    max_idx = factor_break_rate['Break_Rate'].idxmax()\n",
    "    max_row = factor_break_rate.loc[max_idx]\n",
    "\n",
    "    plt.axvline(x=max_idx, color='#E63946', linestyle='--', alpha=1, linewidth=3, ymax=1)\n",
    "\n",
    "    plt.text(max_idx, y_max * 0.8,\n",
    "             f\"Highest: {max_row['Break_Rate']:.2%}\\n({max_row['Bin_Range']})\",\n",
    "             ha='center', color='#E63946', fontsize=15, fontweight='bold',\n",
    "             bbox=dict(facecolor='white', alpha=1, edgecolor='#E63946',\n",
    "                      boxstyle='round,pad=1.0', linewidth=2.5))\n",
    "\n",
    "    plt.xlabel(factor_name or factor_col,\n",
    "               fontweight='bold',\n",
    "               fontsize=22,\n",
    "               labelpad=12)\n",
    "    plt.ylabel('Break Rate',\n",
    "               fontweight='bold',\n",
    "               fontsize=22,\n",
    "               labelpad=12)\n",
    "\n",
    "    ax.yaxis.set_major_formatter(ticker.PercentFormatter(1.0))\n",
    "\n",
    "    ax.tick_params(axis='both', which='major',\n",
    "                   width=2, length=8,\n",
    "                   labelsize=20, pad=8)\n",
    "    ax.tick_params(axis='both', which='minor', width=2, length=5)\n",
    "\n",
    "    plt.xticks(rotation=45, fontsize=18, fontweight='bold',\n",
    "               ha='right', rotation_mode='anchor')\n",
    "    plt.yticks(fontsize=18, fontweight='bold')\n",
    "\n",
    "    plt.margins(x=0.12)\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(2.5)\n",
    "        spine.set_color('black')\n",
    "\n",
    "    ax.yaxis.grid(True, linestyle=':', alpha=0.5, linewidth=1.2)\n",
    "    ax.xaxis.grid(False)\n",
    "\n",
    "    ax.set_facecolor('#f8f9fa')\n",
    "\n",
    "    plt.gca().add_patch(plt.Rectangle((0, 0), 1, 1, transform=ax.transAxes,\n",
    "                                     fill=False, edgecolor='gray', linewidth=2.5,\n",
    "                                     alpha=0.3, zorder=-1))\n",
    "\n",
    "    plt.tight_layout(pad=2.5)\n",
    "    plt.subplots_adjust(left=0.12, right=0.95, top=0.9, bottom=0.15)\n",
    "\n",
    "    plt.savefig('yarn_break_rate_analysis_professional.png',\n",
    "                dpi=1200,\n",
    "                bbox_inches='tight',\n",
    "                pad_inches=0.7,\n",
    "                transparent=False,\n",
    "                facecolor='white')\n",
    "    plt.show()\n",
    "\n",
    "    return factor_break_rate\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('D:/code/junma/600000/0424/2.csv')\n",
    "\n",
    "    dw_number = 1\n",
    "\n",
    "    print(\"Performing data preprocessing...\")\n",
    "    df_processed, full_bins = preprocess_data(df, dw_number=dw_number)\n",
    "\n",
    "    print(\"\\nFullness rate bin boundaries:\", full_bins)\n",
    "\n",
    "    print(\"\\nSingle factor analysis...\")\n",
    "    fullness_break_rate = single_factor_analysis(\n",
    "        df_processed,\n",
    "        'Fullness_Bin',\n",
    "        f'is_break_dw{dw_number}',\n",
    "        factor_name='Fullness Rate'\n",
    "    )\n",
    "\n",
    "    fullness_break_rate.to_csv('fullness_break_rate.csv', index=False)"
   ],
   "id": "24327bb13bcacfde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 设置中文字体和负号显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "# 1. 数据预处理\n",
    "def preprocess_data(df, dw_number=1):\n",
    "    \"\"\"数据预处理函数\"\"\"\n",
    "    # 创建断纱标志列\n",
    "    target_col = f'is_break_dw{dw_number}'\n",
    "    df[target_col] = df[f'D_dw{dw_number}'].apply(lambda x: 1 if x == 3 else 0)\n",
    "\n",
    "    # 计算速度偏移率（当前锭子速度与所有锭子速度中位数的偏差百分比）\n",
    "    speed_col = f'V_dw{dw_number}'\n",
    "    reference_speed = df['R_dw22'].median()  # 使用 R_dw22 的中位数作为参考速度\n",
    "    df['速度偏移率(%)'] = ((df[speed_col] - reference_speed) / reference_speed) * 100\n",
    "\n",
    "    # 将满卷率和速度偏移率分箱（binning），并保留区间标签\n",
    "    df['满卷率分箱'], full_bins = pd.cut(df[f'P_dw{dw_number}'], bins=10, retbins=True)\n",
    "    df['速度偏移率分箱'], speed_bins = pd.cut(df['速度偏移率(%)'], bins=10, retbins=True)\n",
    "\n",
    "    # 处理设备名称和对应面\n",
    "    df['设备名称'] = df['name'].apply(lambda x: x.split('-')[0])  # 提取设备名称（如 NX78）\n",
    "    df['设备面'] = df['subsystem'].apply(lambda x: x.split('-')[1])  # 提取设备面（如 R103）\n",
    "\n",
    "    return df, full_bins, speed_bins\n",
    "\n",
    "# 2. 单因素分析\n",
    "def single_factor_analysis(df, factor_col, target_col, bins=None, factor_name=None):\n",
    "    \"\"\"单因素分析函数\"\"\"\n",
    "    # 计算每个区间的断纱率\n",
    "    factor_break_rate = df.groupby(factor_col)[target_col].agg(['mean', 'count']).reset_index()\n",
    "    factor_break_rate.rename(columns={'mean': '断纱率', 'count': '样本数'}, inplace=True)\n",
    "\n",
    "    # 获取区间范围\n",
    "    factor_break_rate['区间范围'] = factor_break_rate[factor_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\")\n",
    "\n",
    "    # 绘制柱状图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(data=factor_break_rate, x='区间范围', y='断纱率', palette='viridis')\n",
    "\n",
    "    # 添加样本数标注\n",
    "    for i, row in factor_break_rate.iterrows():\n",
    "        ax.text(i, row['断纱率']+0.01, f\"n={row['样本数']}\", ha='center', fontsize=9)\n",
    "\n",
    "    # 标注断纱率最高的区间\n",
    "    max_idx = factor_break_rate['断纱率'].idxmax()\n",
    "    max_row = factor_break_rate.loc[max_idx]\n",
    "    plt.axvline(x=max_idx, color='red', linestyle='--', alpha=0.5)\n",
    "    plt.text(max_idx, max_row['断纱率']+0.05,\n",
    "             f\"最高断纱率: {max_row['断纱率']:.2%}\\n区间: {max_row['区间范围']}\",\n",
    "             ha='center', color='red')\n",
    "\n",
    "    plt.title(f'{factor_name or factor_col} 对断纱率的影响')\n",
    "    plt.xlabel(factor_name or factor_col)\n",
    "    plt.ylabel('断纱率')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return factor_break_rate\n",
    "\n",
    "# 3. 双因素交互分析\n",
    "def interaction_analysis(df, factor1_col, factor2_col, target_col, factor1_name=None, factor2_name=None):\n",
    "    \"\"\"双因素交互分析函数\"\"\"\n",
    "    # 计算每个组合区间的断纱率\n",
    "    interaction_break_rate = df.groupby([factor1_col, factor2_col])[target_col].agg(['mean', 'count']).reset_index()\n",
    "    interaction_break_rate.rename(columns={'mean': '断纱率', 'count': '样本数'}, inplace=True)\n",
    "\n",
    "    # 创建区间范围标签\n",
    "    interaction_break_rate['factor1_range'] = interaction_break_rate[factor1_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\")\n",
    "    interaction_break_rate['factor2_range'] = interaction_break_rate[factor2_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\")\n",
    "\n",
    "    # 创建热力图\n",
    "    pivot_table = interaction_break_rate.pivot(index='factor1_range', columns='factor2_range', values='断纱率')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.heatmap(pivot_table, annot=True, fmt=\".2%\", cmap=\"YlGnBu\",\n",
    "                     annot_kws={\"size\": 9}, cbar_kws={'label': '断纱率'})\n",
    "\n",
    "    # 标注断纱率最高的组合区间\n",
    "    max_idx = interaction_break_rate['断纱率'].idxmax()\n",
    "    max_row = interaction_break_rate.loc[max_idx]\n",
    "    plt.title(f'{factor1_name or factor1_col} × {factor2_name or factor2_col} 对断纱率的影响\\n'\n",
    "              f\"断纱率最高的组合: {max_row['factor1_range']} × {max_row['factor2_range']} = {max_row['断纱率']:.2%}\",\n",
    "              pad=20)\n",
    "\n",
    "    plt.xlabel(factor2_name or factor2_col)\n",
    "    plt.ylabel(factor1_name or factor1_col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 统计显著性检验（卡方检验）\n",
    "    contingency_table = pd.crosstab(\n",
    "        df[factor1_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\"),\n",
    "        df[factor2_col].apply(lambda x: f\"{x.left:.2f}-{x.right:.2f}\"),\n",
    "        values=df[target_col],\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    chi2, p, _, _ = chi2_contingency(contingency_table.fillna(0))\n",
    "    print(f\"卡方检验结果: p值 = {p:.4f}\")\n",
    "\n",
    "    return interaction_break_rate, p\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载数据（替换为你的实际数据路径）\n",
    "    df = pd.read_csv('D:/code/junma/600000/0424/2.csv')\n",
    "\n",
    "    # 配置参数（根据实际列名修改）\n",
    "    dw_number = 1  # 锭号\n",
    "\n",
    "    # 1. 数据预处理\n",
    "    print(\"正在进行数据预处理...\")\n",
    "    df_processed, full_bins, speed_bins = preprocess_data(df, dw_number=dw_number)\n",
    "\n",
    "    # 打印分箱边界\n",
    "    print(\"\\n满卷率分箱边界:\", full_bins)\n",
    "    print(\"速度偏移率分箱边界:\", speed_bins)\n",
    "\n",
    "    # 2. 单因素分析\n",
    "    print(\"\\n单因素分析...\")\n",
    "    fullness_break_rate = single_factor_analysis(\n",
    "        df_processed,\n",
    "        '满卷率分箱',\n",
    "        f'is_break_dw{dw_number}',\n",
    "        factor_name='满卷率'\n",
    "    )\n",
    "\n",
    "    speed_deviation_break_rate = single_factor_analysis(\n",
    "        df_processed,\n",
    "        '速度偏移率分箱',\n",
    "        f'is_break_dw{dw_number}',\n",
    "        factor_name='速度偏移率(%)'\n",
    "    )\n",
    "\n",
    "    # 3. 双因素交互分析\n",
    "    print(\"\\n双因素交互分析...\")\n",
    "    interaction_break_rate, p_value = interaction_analysis(\n",
    "        df_processed,\n",
    "        '满卷率分箱',\n",
    "        '速度偏移率分箱',\n",
    "        f'is_break_dw{dw_number}',\n",
    "        factor1_name='满卷率',\n",
    "        factor2_name='速度偏移率(%)'\n",
    "    )\n",
    "\n",
    "    # 保存结果\n",
    "    fullness_break_rate.to_csv('fullness_break_rate.csv', index=False)\n",
    "    speed_deviation_break_rate.to_csv('speed_deviation_break_rate.csv', index=False)\n",
    "    interaction_break_rate.to_csv('interaction_break_rate.csv', index=False)"
   ],
   "id": "c7e61fb33eb4c207",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "from IPython.display import display\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "def analyze_yarn_breakage(df, target_spindle='NX16-L102'):\n",
    "    \"\"\"\n",
    "    分析锭子断纱率与满卷率、速度偏移率的交互作用\n",
    "\n",
    "    Parameters:\n",
    "        df: 原始数据\n",
    "        target_spindle: 目标锭子编号（如 'NX16-L102'）\n",
    "    \"\"\"\n",
    "    # 1. 数据预处理：计算速度偏移率\n",
    "    target_spindle_num = int(target_spindle.split('-')[-1][3:])  # 提取锭子编号（如 L102 → 102）\n",
    "    speed_col = f'V_dw{target_spindle_num}'\n",
    "    fullness_col = f'P_dw{target_spindle_num}'\n",
    "    break_col = f'D_dw{target_spindle_num}'\n",
    "\n",
    "    # 计算参考速度（所有锭子的中位数）\n",
    "    all_speed_cols = [f'V_dw{i}' for i in range(1, 25)]\n",
    "    df['reference_speed'] = df[all_speed_cols].median(axis=1)\n",
    "    df['speed_deviation_pct'] = ((df[speed_col] - df['reference_speed']) / df['reference_speed']) * 100\n",
    "\n",
    "    # 提取目标锭子数据\n",
    "    target_data = df[df['subsystem'] == target_spindle].copy()\n",
    "    analysis_df = target_data[[break_col, fullness_col, 'speed_deviation_pct']].copy()\n",
    "    analysis_df.columns = ['break_status', 'fullness', 'speed_deviation']\n",
    "\n",
    "    # 2. 单因素分析：满卷率 vs 断纱率\n",
    "    analysis_df['fullness_bin'] = pd.cut(\n",
    "        analysis_df['fullness'],\n",
    "        bins=np.linspace(0, 100, 11),\n",
    "        labels=[f\"{i}%-{i+10}%\" for i in range(0, 100, 10)]\n",
    "    )\n",
    "    fullness_break_rate = analysis_df.groupby('fullness_bin')['break_status'].mean()\n",
    "\n",
    "    # 3. 单因素分析：速度偏移率 vs 断纱率\n",
    "    analysis_df['speed_deviation_bin'] = pd.cut(\n",
    "        analysis_df['speed_deviation'],\n",
    "        bins=np.linspace(-20, 20, 9),\n",
    "        labels=[f\"{i}% to {i+5}%\" for i in range(-20, 20, 5)]\n",
    "    )\n",
    "    speed_break_rate = analysis_df.groupby('speed_deviation_bin')['break_status'].mean()\n",
    "\n",
    "    # 4. 双因素交互分析：热力图\n",
    "    joint_analysis = analysis_df.groupby(['fullness_bin', 'speed_deviation_bin']).agg(\n",
    "        total_samples=('break_status', 'count'),\n",
    "        break_count=('break_status', lambda x: (x == 3).sum()),\n",
    "        break_rate=('break_status', lambda x: (x == 3).mean())\n",
    "    ).reset_index()\n",
    "\n",
    "    # 过滤样本量不足的区间（避免噪声）\n",
    "    valid_groups = joint_analysis[joint_analysis['total_samples'] >= 5]\n",
    "    heatmap_data = valid_groups.pivot(\n",
    "        index='fullness_bin',\n",
    "        columns='speed_deviation_bin',\n",
    "        values='break_rate'\n",
    "    )\n",
    "\n",
    "    # 5. 统计检验（卡方检验）\n",
    "    contingency_table = pd.crosstab(\n",
    "        index=analysis_df['fullness_bin'],\n",
    "        columns=analysis_df['speed_deviation_bin'],\n",
    "        values=analysis_df['break_status'],\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    chi2, p_value, _, _ = chi2_contingency(contingency_table.fillna(0))\n",
    "    print(f\"卡方检验 p-value = {p_value:.4f} (交互作用{'显著' if p_value < 0.05 else '不显著'})\")\n",
    "\n",
    "    # 6. 可视化\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    # 单因素：满卷率 vs 断纱率\n",
    "    plt.subplot(1, 2, 1)\n",
    "    fullness_break_rate.plot(kind='bar', color='skyblue')\n",
    "    plt.title(\"满卷率 vs 断纱率\", fontsize=12)\n",
    "    plt.xlabel(\"满卷率区间\")\n",
    "    plt.ylabel(\"断纱率\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # 单因素：速度偏移率 vs 断纱率\n",
    "    plt.subplot(1, 2, 2)\n",
    "    speed_break_rate.plot(kind='bar', color='salmon')\n",
    "    plt.title(\"速度偏移率 vs 断纱率\", fontsize=12)\n",
    "    plt.xlabel(\"速度偏移率区间 (%)\")\n",
    "    plt.ylabel(\"断纱率\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 双因素：热力图\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        heatmap_data * 100,\n",
    "        annot=True,\n",
    "        fmt=\".1f\",\n",
    "        cmap=\"YlOrRd\",\n",
    "        cbar_kws={'label': '断纱率 (%)'},\n",
    "        annot_kws={\"size\": 10}\n",
    "    )\n",
    "    plt.title(\n",
    "        f\"锭子 {target_spindle} 断纱率分析\\n满卷率 × 速度偏移率交互作用\",\n",
    "        fontsize=14\n",
    "    )\n",
    "    plt.xlabel(\"速度偏移率区间 (%)\", fontsize=12)\n",
    "    plt.ylabel(\"满卷率区间\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return joint_analysis\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('D:/code/junma/600000/0424/second_final_processed.csv')\n",
    "\n",
    "    # 检查必要列是否存在\n",
    "    required_cols = ['subsystem'] + [f'D_dw{i}' for i in range(1, 25)]\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"缺少必要列: {missing_cols}\")\n",
    "\n",
    "    print(\"开始分析断纱率...\")\n",
    "    breakage_stats = analyze_yarn_breakage(df, target_spindle='NX16-L102')\n",
    "\n",
    "    # 保存结果\n",
    "    breakage_stats.to_csv('breakage_analysis_results.csv', index=False)\n",
    "    print(\"\\n分析完成！结果已保存至 breakage_analysis_results.csv\")"
   ],
   "id": "643a49b79efc4ee4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f0eb43e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:58:03.050737Z",
     "iopub.status.busy": "2023-09-10T22:58:03.049476Z",
     "iopub.status.idle": "2023-09-10T22:58:03.058033Z",
     "shell.execute_reply": "2023-09-10T22:58:03.057016Z"
    },
    "papermill": {
     "duration": 0.092915,
     "end_time": "2023-09-10T22:58:03.060203",
     "exception": false,
     "start_time": "2023-09-10T22:58:02.967288",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final Prediction Model\n",
    "final_model = best_global_model\n",
    "\n",
    "# Make predictions on the test set using the final model\n",
    "y_final_pred = final_model.predict(X_test)\n",
    "final_y_pred = (y_final_pred)\n",
    "final_y_test =(y_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f488843e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:58:03.225058Z",
     "iopub.status.busy": "2023-09-10T22:58:03.224305Z",
     "iopub.status.idle": "2023-09-10T22:58:03.236034Z",
     "shell.execute_reply": "2023-09-10T22:58:03.234652Z"
    },
    "papermill": {
     "duration": 0.09709,
     "end_time": "2023-09-10T22:58:03.238485",
     "exception": false,
     "start_time": "2023-09-10T22:58:03.141395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create a DataFrame with the predicted prices and true prices\n",
    "results = pd.DataFrame({'Predicted Value': final_y_pred, 'True Value': final_y_test})\n",
    "\n",
    "# Calculate the difference between the true prices and predicted prices and add a new column\n",
    "results['Difference'] = results['True Value'] - results['Predicted Value']\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(\"First 5 rows:\")\n",
    "print(results.head())\n",
    "\n",
    "# Display the last 5 rows\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(results.tail())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Final Prediction Model\n",
    "final_model = best_global_model\n",
    "\n",
    "# Make predictions on the test set using the final model\n",
    "y_final_pred = final_model.predict(X_test)\n",
    "final_y_pred = (y_final_pred)\n",
    "final_y_test = (y_test)\n",
    "\n",
    "# Create a DataFrame with the predicted values and true values\n",
    "results = pd.DataFrame({'Predicted Value': final_y_pred, 'True Value': final_y_test})\n",
    "\n",
    "# Filter the results to only include rows where the predicted value is 1 (断纱)\n",
    "results_break = results[results['Predicted Value'] == 1]\n",
    "\n",
    "# Calculate the difference between the true values and predicted values and add a new column\n",
    "results_break['Difference'] = results_break['True Value'] - results_break['Predicted Value']\n",
    "\n",
    "# Display the first 5 rows of the filtered results\n",
    "print(\"First 5 rows (Predicted Break = 1):\")\n",
    "print(results_break.head())\n",
    "\n",
    "# Display the last 5 rows of the filtered results\n",
    "print(\"\\nLast 5 rows (Predicted Break = 1):\")\n",
    "print(results_break.tail())\n",
    "\n",
    "# Optionally, save the filtered results to a CSV file\n",
    "results_break.to_csv('predicted_break_results.csv', index=False)"
   ],
   "id": "75a66c04becce86f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 加载优化后的模型进行最终预测\n",
    "try:\n",
    "    print(\"加载优化后的模型进行最终预测...\")\n",
    "\n",
    "    # 尝试加载优化后的模型\n",
    "    optimized_model_info = joblib.load('rf_balanced_optimized_latest.pkl')\n",
    "    final_model = optimized_model_info['model']\n",
    "    best_params = optimized_model_info['best_params']\n",
    "\n",
    "    print(\"优化模型加载成功！\")\n",
    "    print(f\"使用的最佳参数: {best_params}\")\n",
    "\n",
    "    # 使用优化后的模型进行预测\n",
    "    y_final_pred = final_model.predict(X_test)\n",
    "    y_final_pred_proba = final_model.predict_proba(X_test)[:, 1]  # 预测概率\n",
    "\n",
    "    # 确保数据类型一致\n",
    "    final_y_pred = y_final_pred.astype(int)\n",
    "    final_y_test = y_test.astype(int)\n",
    "\n",
    "    # 创建完整的预测结果DataFrame\n",
    "    results_full = pd.DataFrame({\n",
    "        'Predicted_Value': final_y_pred,\n",
    "        'True_Value': final_y_test,\n",
    "        'Prediction_Probability': y_final_pred_proba,\n",
    "        'Prediction_Correct': final_y_pred == final_y_test\n",
    "    })\n",
    "\n",
    "    # 添加预测状态描述\n",
    "    results_full['Prediction_Status'] = results_full['Prediction_Correct'].map({\n",
    "        True: '正确预测',\n",
    "        False: '错误预测'\n",
    "    })\n",
    "\n",
    "    # 添加类别描述\n",
    "    results_full['True_Label'] = results_full['True_Value'].map({0: '正常', 1: '断纱'})\n",
    "    results_full['Predicted_Label'] = results_full['Predicted_Value'].map({0: '正常', 1: '断纱'})\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"优化模型预测结果总览\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 总体统计\n",
    "    total_samples = len(results_full)\n",
    "    correct_predictions = results_full['Prediction_Correct'].sum()\n",
    "    overall_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f\"总样本数: {total_samples}\")\n",
    "    print(f\"正确预测数: {correct_predictions}\")\n",
    "    print(f\"总体准确率: {overall_accuracy:.4f} ({overall_accuracy:.2%})\")\n",
    "\n",
    "    # 1. 断纱预测结果分析 (预测为1的样本)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"断纱预测结果分析 (预测值 = 1)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    results_break = results_full[results_full['Predicted_Value'] == 1].copy()\n",
    "    results_break['Difference'] = results_break['True_Value'] - results_break['Predicted_Value']\n",
    "\n",
    "    break_total = len(results_break)\n",
    "    break_correct = (results_break['True_Value'] == 1).sum()\n",
    "    break_incorrect = (results_break['True_Value'] == 0).sum()\n",
    "    break_accuracy = break_correct / break_total if break_total > 0 else 0\n",
    "\n",
    "    print(f\"预测为断纱的样本总数: {break_total}\")\n",
    "    print(f\"其中实际为断纱(正确预测): {break_correct}\")\n",
    "    print(f\"其中实际为正常(错误预测 - 误报): {break_incorrect}\")\n",
    "    print(f\"断纱预测准确率: {break_accuracy:.4f} ({break_accuracy:.2%})\")\n",
    "\n",
    "    # 显示断纱预测的前后各5行\n",
    "    print(f\"\\n前5个断纱预测样本:\")\n",
    "    break_display_cols = ['Predicted_Label', 'True_Label', 'Prediction_Probability', 'Prediction_Status']\n",
    "    display(results_break[break_display_cols].head())\n",
    "\n",
    "    print(f\"\\n后5个断纱预测样本:\")\n",
    "    display(results_break[break_display_cols].tail())\n",
    "\n",
    "    # 2. 正常预测结果分析 (预测为0的样本)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"正常预测结果分析 (预测值 = 0)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    results_normal = results_full[results_full['Predicted_Value'] == 0].copy()\n",
    "    results_normal['Difference'] = results_normal['True_Value'] - results_normal['Predicted_Value']\n",
    "\n",
    "    normal_total = len(results_normal)\n",
    "    normal_correct = (results_normal['True_Value'] == 0).sum()\n",
    "    normal_incorrect = (results_normal['True_Value'] == 1).sum()\n",
    "    normal_accuracy = normal_correct / normal_total if normal_total > 0 else 0\n",
    "\n",
    "    print(f\"预测为正常的样本总数: {normal_total}\")\n",
    "    print(f\"其中实际为正常(正确预测): {normal_correct}\")\n",
    "    print(f\"其中实际为断纱(错误预测 - 漏报): {normal_incorrect}\")\n",
    "    print(f\"正常预测准确率: {normal_accuracy:.4f} ({normal_accuracy:.2%})\")\n",
    "\n",
    "    # 显示正常预测的前后各5行\n",
    "    print(f\"\\n前5个正常预测样本:\")\n",
    "    normal_display_cols = ['Predicted_Label', 'True_Label', 'Prediction_Probability', 'Prediction_Status']\n",
    "    display(results_normal[normal_display_cols].head())\n",
    "\n",
    "    print(f\"\\n后5个正常预测样本:\")\n",
    "    display(results_normal[normal_display_cols].tail())\n",
    "\n",
    "    # 3. 详细错误分析\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"详细错误分析\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 误报分析 (预测为1但实际为0)\n",
    "    false_positives = results_break[results_break['True_Value'] == 0]\n",
    "    print(f\"误报数量 (预测断纱但实际正常): {len(false_positives)}\")\n",
    "    if len(false_positives) > 0:\n",
    "        print(\"误报样本详情:\")\n",
    "        display(false_positives[['Prediction_Probability', 'True_Label', 'Predicted_Label']].head(10))\n",
    "\n",
    "    # 漏报分析 (预测为0但实际为1)\n",
    "    false_negatives = results_normal[results_normal['True_Value'] == 1]\n",
    "    print(f\"\\n漏报数量 (预测正常但实际断纱): {len(false_negatives)}\")\n",
    "    if len(false_negatives) > 0:\n",
    "        print(\"漏报样本详情:\")\n",
    "        display(false_negatives[['Prediction_Probability', 'True_Label', 'Predicted_Label']].head(10))\n",
    "\n",
    "    # 4. 预测概率分布分析\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"预测概率分布分析\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 正确预测的概率分布\n",
    "    correct_probabilities = results_full[results_full['Prediction_Correct'] == True]['Prediction_Probability']\n",
    "    incorrect_probabilities = results_full[results_full['Prediction_Correct'] == False]['Prediction_Probability']\n",
    "\n",
    "    print(f\"正确预测的平均概率: {correct_probabilities.mean():.4f}\")\n",
    "    print(f\"错误预测的平均概率: {incorrect_probabilities.mean():.4f}\")\n",
    "    print(f\"正确预测的概率标准差: {correct_probabilities.std():.4f}\")\n",
    "    print(f\"错误预测的概率标准差: {incorrect_probabilities.std():.4f}\")\n",
    "\n",
    "    # 按类别统计概率\n",
    "    for true_class in [0, 1]:\n",
    "        class_data = results_full[results_full['True_Value'] == true_class]\n",
    "        class_name = '正常' if true_class == 0 else '断纱'\n",
    "        print(f\"\\n{class_name}样本的预测概率统计:\")\n",
    "        print(f\"  平均概率: {class_data['Prediction_Probability'].mean():.4f}\")\n",
    "        print(f\"  概率中位数: {class_data['Prediction_Probability'].median():.4f}\")\n",
    "        print(f\"  概率标准差: {class_data['Prediction_Probability'].std():.4f}\")\n",
    "\n",
    "    # 5. 保存详细结果\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"保存预测结果\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 保存完整结果\n",
    "    results_full.to_csv('optimized_model_complete_predictions.csv', index=False)\n",
    "    print(\"完整预测结果已保存至: optimized_model_complete_predictions.csv\")\n",
    "\n",
    "    # 保存断纱预测结果\n",
    "    results_break.to_csv('optimized_model_break_predictions.csv', index=False)\n",
    "    print(\"断纱预测结果已保存至: optimized_model_break_predictions.csv\")\n",
    "\n",
    "    # 保存正常预测结果\n",
    "    results_normal.to_csv('optimized_model_normal_predictions.csv', index=False)\n",
    "    print(\"正常预测结果已保存至: optimized_model_normal_predictions.csv\")\n",
    "\n",
    "    # 保存错误分析结果\n",
    "    error_analysis = pd.concat([false_positives, false_negatives])\n",
    "    if len(error_analysis) > 0:\n",
    "        error_analysis.to_csv('optimized_model_error_analysis.csv', index=False)\n",
    "        print(\"错误分析结果已保存至: optimized_model_error_analysis.csv\")\n",
    "\n",
    "    # 6. 生成预测结果汇总报告\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"预测结果汇总报告\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    summary_report = {\n",
    "        '总样本数': total_samples,\n",
    "        '正确预测数': correct_predictions,\n",
    "        '总体准确率': f\"{overall_accuracy:.4f} ({overall_accuracy:.2%})\",\n",
    "        '断纱预测总数': break_total,\n",
    "        '断纱正确预测数': break_correct,\n",
    "        '断纱误报数': break_incorrect,\n",
    "        '断纱预测准确率': f\"{break_accuracy:.4f} ({break_accuracy:.2%})\",\n",
    "        '正常预测总数': normal_total,\n",
    "        '正常正确预测数': normal_correct,\n",
    "        '正常漏报数': normal_incorrect,\n",
    "        '正常预测准确率': f\"{normal_accuracy:.4f} ({normal_accuracy:.2%})\",\n",
    "        '误报率 (False Positive Rate)': f\"{break_incorrect/break_total:.4f} ({break_incorrect/break_total:.2%})\" if break_total > 0 else \"N/A\",\n",
    "        '漏报率 (False Negative Rate)': f\"{normal_incorrect/normal_total:.4f} ({normal_incorrect/normal_total:.2%})\" if normal_total > 0 else \"N/A\",\n",
    "        '使用模型': 'RF_Optimized',\n",
    "        '最佳参数': str(best_params)\n",
    "    }\n",
    "\n",
    "    summary_df = pd.DataFrame(list(summary_report.items()), columns=['指标', '值'])\n",
    "    print(\"预测结果汇总:\")\n",
    "    display(summary_df)\n",
    "\n",
    "    # 保存汇总报告\n",
    "    summary_df.to_csv('optimized_model_prediction_summary.csv', index=False)\n",
    "    print(\"\\n预测汇总报告已保存至: optimized_model_prediction_summary.csv\")\n",
    "\n",
    "    # 7. 可视化预测结果 (可选)\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "\n",
    "        print(\"\\n生成预测结果可视化...\")\n",
    "\n",
    "        # 设置绘图样式\n",
    "        plt.style.use('default')\n",
    "        plt.rcParams.update({\n",
    "            'font.family': 'Arial',\n",
    "            'font.weight': 'bold',\n",
    "            'axes.labelweight': 'bold',\n",
    "            'axes.titleweight': 'bold',\n",
    "            'font.size': 10\n",
    "        })\n",
    "\n",
    "        # 创建预测结果分布图\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "        # 1. 预测概率分布\n",
    "        axes[0,0].hist(results_full['Prediction_Probability'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0,0].set_xlabel('预测概率')\n",
    "        axes[0,0].set_ylabel('频数')\n",
    "        axes[0,0].set_title('预测概率分布')\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "        # 2. 正确与错误预测的概率分布\n",
    "        axes[0,1].hist(correct_probabilities, bins=30, alpha=0.7, label='正确预测', color='green')\n",
    "        axes[0,1].hist(incorrect_probabilities, bins=30, alpha=0.7, label='错误预测', color='red')\n",
    "        axes[0,1].set_xlabel('预测概率')\n",
    "        axes[0,1].set_ylabel('频数')\n",
    "        axes[0,1].set_title('正确vs错误预测的概率分布')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "        # 3. 类别分布饼图\n",
    "        prediction_counts = results_full['Predicted_Label'].value_counts()\n",
    "        axes[1,0].pie(prediction_counts.values, labels=prediction_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "        axes[1,0].set_title('预测类别分布')\n",
    "\n",
    "        # 4. 准确率条形图\n",
    "        accuracy_data = [overall_accuracy, break_accuracy, normal_accuracy]\n",
    "        accuracy_labels = ['总体准确率', '断纱预测准确率', '正常预测准确率']\n",
    "        bars = axes[1,1].bar(accuracy_labels, accuracy_data, color=['blue', 'red', 'green'])\n",
    "        axes[1,1].set_ylabel('准确率')\n",
    "        axes[1,1].set_title('各类别预测准确率')\n",
    "        axes[1,1].set_ylim(0, 1)\n",
    "\n",
    "        # 在条形图上添加数值标签\n",
    "        for bar, acc in zip(bars, accuracy_data):\n",
    "            height = bar.get_height()\n",
    "            axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                          f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('optimized_model_prediction_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"预测分析图已保存至: optimized_model_prediction_analysis.png\")\n",
    "        plt.show()\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Matplotlib/Seaborn 不可用，跳过可视化部分\")\n",
    "\n",
    "    print(\"\\n优化模型预测分析完成！\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"错误: 优化模型文件 'rf_balanced_optimized_latest.pkl' 未找到\")\n",
    "    print(\"请先运行模型优化代码\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"预测过程中发生错误: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ],
   "id": "5903f778be152481",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9c9a320e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T22:58:03.402683Z",
     "iopub.status.busy": "2023-09-10T22:58:03.402280Z",
     "iopub.status.idle": "2023-09-10T22:58:03.414985Z",
     "shell.execute_reply": "2023-09-10T22:58:03.413797Z"
    },
    "papermill": {
     "duration": 0.096977,
     "end_time": "2023-09-10T22:58:03.417368",
     "exception": false,
     "start_time": "2023-09-10T22:58:03.320391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#Saving the Final CatBoost Model to Disk\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "dump(final_model, 'catboost_model.joblib')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # 创建目标变量\n",
    "    y = df['broken_status']  # 目标变量\n",
    "\n",
    "    # 1. 明确区分数值列和分类列\n",
    "    # 数值列（确保只包含数值型数据）\n",
    "    num_cols = [col for col in df.select_dtypes(include=['int64', 'float64']).columns\n",
    "               if col != 'broken_status' and col in df.columns]\n",
    "\n",
    "    # 分类列（明确指定或自动检测字符串列）\n",
    "    cat_cols = [col for col in df.select_dtypes(include=['object', 'category']).columns\n",
    "               if col in df.columns]\n",
    "\n",
    "    # 2. 动态生成锭位相关列（确保只包含实际存在的列）\n",
    "    spindle_cols = []\n",
    "    for prefix in ['d_dw', 'v_dw', 'p_dw']:\n",
    "        for i in range(1, 101):\n",
    "            col_name = f\"{prefix}{i}\"\n",
    "            if col_name in df.columns:\n",
    "                spindle_cols.append(col_name)\n",
    "\n",
    "    # 3. 合并所有特征列（确保分类列正确识别）\n",
    "    feature_cols = num_cols + [col for col in cat_cols if col in df.columns] + spindle_cols\n",
    "\n",
    "    # 4. 明确分类特征（锭位状态列通常是分类的）\n",
    "    categorical_features = [col for col in feature_cols\n",
    "                           if col in cat_cols or col.startswith('d_dw')]  # d_dw前缀的通常是分类变量\n",
    "    numerical_features = [col for col in feature_cols\n",
    "                         if col not in categorical_features and col in df.columns]\n",
    "\n",
    "    # 确保所有特征列都存在于DataFrame中\n",
    "    categorical_features = [col for col in categorical_features if col in df.columns]\n",
    "    numerical_features = [col for col in numerical_features if col in df.columns]\n",
    "\n",
    "    # 创建特征DataFrame\n",
    "    X = df[feature_cols]\n",
    "\n",
    "    return X, y, categorical_features, numerical_features\n",
    "\n",
    "def build_preprocessor(categorical_features, numerical_features):\n",
    "    # 数值型处理管道\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # 分类型处理管道（特别处理字符串数据）\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # 确保只传递实际存在的列\n",
    "    transformers = []\n",
    "    if numerical_features:\n",
    "        transformers.append(('num', numeric_transformer, numerical_features))\n",
    "    if categorical_features:\n",
    "        transformers.append(('cat', categorical_transformer, categorical_features))\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder='drop'  # 忽略不在转换器中的列\n",
    "    )\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "def grid_search_tuning(X, y, preprocessor):\n",
    "    # 定义管道\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', RandomForestRegressor(random_state=42))\n",
    "    ])\n",
    "\n",
    "    # 定义参数网格 - 20种组合\n",
    "    param_grid = {\n",
    "        'regressor__n_estimators': [50, 100, 200],\n",
    "        'regressor__max_depth': [None, 10, 20, 30],\n",
    "        'regressor__min_samples_split': [2, 5, 10],\n",
    "        'regressor__min_samples_leaf': [1, 2, 4],\n",
    "        'regressor__max_features': ['sqrt', 'log2', 0.5],\n",
    "        'regressor__bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # 创建网格搜索对象\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,  # 5折交叉验证\n",
    "        n_jobs=-1,  # 使用所有CPU核心\n",
    "        verbose=2,\n",
    "        scoring='neg_mean_squared_error'\n",
    "    )\n",
    "\n",
    "    print(\"开始网格搜索调优...\")\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    return grid_search\n",
    "\n",
    "def main():\n",
    "    # 加载数据 - 替换为你的实际数据路径\n",
    "    df = pd.read_csv(\"D:/code/junma/600000/0424/1.csv\")\n",
    "\n",
    "    # 数据预处理\n",
    "    X, y, categorical_features, numerical_features = preprocess_data(df)\n",
    "\n",
    "    # 检查特征列\n",
    "    print(\"数值特征:\", numerical_features)\n",
    "    print(\"分类特征:\", categorical_features)\n",
    "\n",
    "    # 构建预处理管道\n",
    "    preprocessor = build_preprocessor(categorical_features, numerical_features)\n",
    "\n",
    "    # 划分训练测试集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # 网格搜索调优\n",
    "    grid_search = grid_search_tuning(X_train, y_train, preprocessor)\n",
    "\n",
    "    # 输出最佳参数\n",
    "    print(\"\\n最佳参数组合:\")\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # 评估最佳模型\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\n测试集MSE: {mse:.4f}\")\n",
    "    print(f\"测试集R²: {r2:.4f}\")\n",
    "\n",
    "    # 特征重要性\n",
    "    try:\n",
    "        # 获取特征名称\n",
    "        feature_names = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "        # 获取特征重要性\n",
    "        importances = best_model.named_steps['regressor'].feature_importances_\n",
    "\n",
    "        # 创建重要性DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False).head(20)\n",
    "\n",
    "        print(\"\\n最重要的20个特征:\")\n",
    "        print(importance_df)\n",
    "\n",
    "        # 可视化特征重要性\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title('Top 20 Feature Importances')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n无法获取特征重要性: {str(e)}\")\n",
    "\n",
    "    # 保存模型\n",
    "    joblib.dump(best_model, 'random_forest_regressor.pkl')\n",
    "    print(\"\\n模型已保存为 'random_forest_regressor.pkl'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "bc20fd5e5433dd0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8375a14",
   "metadata": {
    "papermill": {
     "duration": 0.082677,
     "end_time": "2023-09-10T22:58:03.580628",
     "exception": false,
     "start_time": "2023-09-10T22:58:03.497951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 30px; color:white; margin:10; font-size:150%; text-align:left; display:fill; border-radius:10px; background-color:#3b3745\"><b><span style='color:#F1A424'>21 |</span></b> <b>Conclusion</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90add18e",
   "metadata": {
    "papermill": {
     "duration": 0.081054,
     "end_time": "2023-09-10T22:58:03.743928",
     "exception": false,
     "start_time": "2023-09-10T22:58:03.662874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: 'Verdana', sans-serif;\n",
    "            background-color: #e6f7ff; /* Light blue background */\n",
    "            color: #333333; /* Dark grey text */\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "            overflow-x: hidden; /* Prevent horizontal scrolling */\n",
    "        }\n",
    "        .container {\n",
    "            max-width: 100%; /* Full width */\n",
    "            margin: 40px auto;\n",
    "            padding: 30px;\n",
    "            border: 2px solid #008080; /* Teal border */\n",
    "            border-radius: 15px;\n",
    "            background-color: rgba(230, 247, 255, 0.6); /* Slightly transparent light blue background */\n",
    "        }\n",
    "        p {\n",
    "            font-size: 18px;\n",
    "            line-height: 1.6;\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <p>In this study, we delved deep into optimizing biogas production in U.S. livestock farms using various machine learning models. The dataset encompassed various livestock types such as cattle, dairy cows, poultry, and swine.</p>\n",
    "        <p>💡 <strong>Key Findings:</strong><br>\n",
    "        Among all models, the LightGBM model emerged as the most proficient, boasting an R^2 score of 0.893, indicating that it can explain 89.3% of the variability in biogas production. Notably, it achieved the lowest RMSE of 114,838.761, underscoring its accuracy. The CatBoost model also showcased a commendable performance with an R^2 score of 0.852, although its execution time was relatively higher at 58.845 seconds.</p>\n",
    "        <p>🚀 <strong>Implications:</strong><br>\n",
    "        These findings pave the way for:\n",
    "        <ul>\n",
    "            <li>Optimized Farming Strategies: Farmers can optimize livestock types for maximum biogas production, increasing profitability.</li>\n",
    "            <li>Investment Decisions: The insights serve as a tool for energy companies and investors to allocate resources efficiently for biogas production.</li>\n",
    "            <li>Sustainable Energy Goals: Policymakers can use this approach for scalable, sustainable energy solutions, contributing to environmental conservation.</li>\n",
    "        </ul>\n",
    "        </p>\n",
    "        <p>⏱ <strong>Future Work:</strong><br>\n",
    "        While the models, especially LightGBM, displayed promising outcomes, there's scope for further refinement. Future studies might involve more advanced machine learning techniques or additional features to enhance accuracy and reliability.</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb637d4",
   "metadata": {
    "papermill": {
     "duration": 0.080808,
     "end_time": "2023-09-10T22:58:03.905482",
     "exception": false,
     "start_time": "2023-09-10T22:58:03.824674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"text-align: center; padding: 60px; background: url('https://source.unsplash.com/800x600?energy') no-repeat center/cover; border: 5px solid #FFEB3B; border-radius: 35px; box-shadow: 0 15px 25px rgba(0, 0, 0, 0.2);\">\n",
    "    <p style=\"font-size: 26px; margin-bottom: 35px; color: #000000; font-family: 'Arial', sans-serif; font-weight: bold; text-transform: uppercase; letter-spacing: 3px; text-shadow: 4px 4px 8px rgba(0, 0, 0, 0.5); animation: slide 3s infinite alternate;\">\n",
    "        Click for the Next Analysis => U.S. Farm Biogas ML Prediction (Dairy Cow Farms)\n",
    "    </p>\n",
    "    <a href=\"https://www.kaggle.com/code/mehmetisik/u-s-farm-biogas-ml-prediction-dairy-cow-farms\" target=\"_blank\" style=\"text-decoration: none; display: inline-block; padding: 15px 30px; font-size: 24px; color: #673AB7; background-color: #FFFFFF; border-radius: 50px; transition: transform 0.3s ease; box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2);\">👉</a>\n",
    "    <style>\n",
    "        @keyframes pulsate {\n",
    "            0% { transform: scale(1); }\n",
    "            50% { transform: scale(1.08); }\n",
    "            100% { transform: scale(1); }\n",
    "        }\n",
    "        @keyframes slide {\n",
    "            0% { transform: translateX(-10px); }\n",
    "            100% { transform: translateX(10px); }\n",
    "        }\n",
    "        a:hover {\n",
    "            transform: translateY(-5px);\n",
    "            box-shadow: 0 8px 16px rgba(0, 0, 0, 0.3);\n",
    "        }\n",
    "    </style>\n",
    "</div>"
   ]
  }
 ],
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": 3
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.6.4"
 },
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 192.965752,
   "end_time": "2023-09-10T22:58:05.111024",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-09-10T22:54:52.145272",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
